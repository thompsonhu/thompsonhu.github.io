<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bonbon Blog on Bonbon Blog</title>
    <link>/</link>
    <description>Recent content in Bonbon Blog on Bonbon Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0800</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>TensorFlow实现YOLOv1目标检测</title>
      <link>/post/tensorflow%E5%AE%9E%E7%8E%B0yolov1%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</link>
      <pubDate>Wed, 06 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tensorflow%E5%AE%9E%E7%8E%B0yolov1%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</guid>
      <description>


&lt;p&gt;目标检测是计算机视觉一个重要的领域，希望让计算机可以自己自动找出某张图片（某个视频的某一帧画面）中的物体，并认出它们是什么，这是一个具有挑战又有趣的任务。本文主要对YOLO(You Only Look Once)模型进行探讨并在TensorFlow上实现，YOLOv1是YOLO的第一个版本（YOLO version1），虽然它的目标检测效果不咋滴，但是现在YOLO进化到了第三个版本了，是一个极其强大的工具。为了更好的学习YOLOV3，那也需要领会它前辈YOLOv1的精髓所在。YOLOv1的作者有四个大佬，&lt;a href=&#34;https://github.com/pjreddie?tab=repositories&#34;&gt;Joseph Redmon&lt;/a&gt;大佬喜欢用C语言，他们也用C语言搭建Darknet并实现了YOLOv1，这也让众多YOLO信徒不知如何用TensorFlow来领悟其中的奥妙。github找到零零散散几个有关于YOLOv1的，但是大多数只有检测过程，而没有训练过程。几番周折，找到了既有训练又能检测的代码。本文参照&lt;a href=&#34;https://github.com/hizhangp/yolo_tensorflow&#34;&gt;hizhangp&lt;/a&gt;的Github仓库代码，对模型进行介绍和探讨，同时对代码进行解读并实际完成目标检测。&lt;/p&gt;
&lt;div id=&#34;you-only-look-once-&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;You Only Look Once, 你不要看我第二次（//▽//）&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://pjreddie.com/darknet/yolov1/&#34;&gt;YOLOv1&lt;/a&gt;采用一个单独的CNN模型实现end-to-end的目标检测，它的思想正如它paper的标题一般，你只看一次，就能看出图片中有什么东西，达到跟人类基本一样的探索功能。就像你看到下面这只可爱的黄色电气鼠，一瞄就知道是皮卡丘！&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;1.PNG&#34; width = &#34;400&#34; height = &#34;400&#34; alt=&#34;Figure1&#34; /&gt;
&lt;p&gt;
图1. 我是谁？皮卡丘！
&lt;/div&gt;
&lt;p&gt;YOLO(本文指YOLO version1)的大体检测框架是：首先通过转换图像为&lt;span class=&#34;math inline&#34;&gt;\(448\times 448\)&lt;/span&gt;大小的输入，在图像输入之后YOLO会将其分为&lt;span class=&#34;math inline&#34;&gt;\(S\times S\)&lt;/span&gt;的grid cell（小格子）。每一个grid cell会产生B个边界框（Bounding Box），每个Bounding Box会附带一个置信度值。原文中设定了每张图像分为&lt;span class=&#34;math inline&#34;&gt;\(7\times 7\)&lt;/span&gt;的grid cell，每个grid cell产生2个Bounding Boxes，那所有的grid cell总共会生成98($772)个边界框；另外原文设定了检测物体的类别数为20。需要注意的是，在原文中多次出现exist和appear的词，表示grid cell中包含了object，但其实更准确的是表示object的中心点出现在这个grid cell中。在完成训练阶段之后，通过非极大值抑制来去除多余的边界框并完成检测（后面会补充介绍非极大值抑制的内容）~&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;2.PNG&#34; width = &#34;600&#34; height = &#34;600&#34; alt=&#34;Figure2&#34; /&gt;
&lt;p&gt;
图2. YOLO检测系统细节图
&lt;/div&gt;
&lt;div id=&#34;yolotraning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;YOLO模型训练阶段（Traning）解析&lt;/h3&gt;
&lt;p&gt;YOLO检测网络包含了24个卷积层和2个全连接层，其中也运用了最大池化层，如图3所示：&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;3.PNG&#34; width = &#34;1000&#34; height = &#34;900&#34; alt=&#34;Figure3&#34; /&gt;
&lt;p&gt;
图3. YOLO检测网络图
&lt;/div&gt;
&lt;p&gt;其中激活函数使用了leaky relu函数： &lt;span class=&#34;math display&#34;&gt;\[
\phi(x) = \left\{ \begin{array}{l}
x,\quad\ \ \text{if}\ \ x&amp;gt;0\\
0.1x,\ \text{otherwise}
\end{array} \right.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;最终模型网络输出的是一个&lt;span class=&#34;math inline&#34;&gt;\(7\times 7\times 30\)&lt;/span&gt;的张量（Tensor），这里我们可以看作49(&lt;span class=&#34;math inline&#34;&gt;\(7\times 7\)&lt;/span&gt;)个grid cells，其中每个grid cell中涉及30个通道（channel）。这30个channels中有2组对应2个Bounding Boxes的信息（总共占用10(&lt;span class=&#34;math inline&#34;&gt;\(2\times 5\)&lt;/span&gt;)个channels），剩下的20个channels对应的是20个检测类的概率。&lt;/p&gt;
&lt;p&gt;前5个channels（张量的第1-5d的位置）中的参数分别是第一个Bounding Box（&lt;font color=#FFD700&gt;&lt;strong&gt;黄色边界框&lt;/strong&gt;&lt;/font&gt;）的四个坐标值以及相应的置信度。这四个坐标(&lt;span class=&#34;math inline&#34;&gt;\(x,y,w,h\)&lt;/span&gt;)中(&lt;span class=&#34;math inline&#34;&gt;\(x,y\)&lt;/span&gt;)是物体中心点相对于左上角的坐标值，(&lt;span class=&#34;math inline&#34;&gt;\(w,h\)&lt;/span&gt;)是Bounding Box的宽和高，以中心点坐标就可以确定Bounding Box的大小。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;4.PNG&#34; width = &#34;1000&#34; height = &#34;900&#34; alt=&#34;Figure4&#34; /&gt;
&lt;p&gt;
图4. 第一个Bounding Box对应的值
&lt;/div&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;5.PNG&#34; width = &#34;400&#34; height = &#34;400&#34; alt=&#34;Figure5&#34; /&gt;
&lt;p&gt;
图5. (&lt;span class=&#34;math inline&#34;&gt;\(x,y,w,h\)&lt;/span&gt;)坐标值的含义
&lt;/div&gt;
&lt;p&gt;同理，第二个Bounding Box（&lt;font color=#FFD700&gt;&lt;strong&gt;黄色边界框&lt;/strong&gt;&lt;/font&gt;）的参数对应的是张量的第6-10的位置（如图6所示），同样是(&lt;span class=&#34;math inline&#34;&gt;\(x,y,w,h\)&lt;/span&gt;)这四个坐标值以置信度值。&lt;/p&gt;
&lt;p&gt;说了好几次置信度(confidence)这个词，具体是如何定义的呢？如何计算的呢？其实它只是一个概率和交并比（IoU）的乘积： &lt;span class=&#34;math display&#34;&gt;\[
\text{confidence} = Pr(\text{Object})\times \text{IOU}^{\text{truth}}_{\text{pred}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果中心点存在的，那么&lt;span class=&#34;math inline&#34;&gt;\(Pr(\text{Object})=1\)&lt;/span&gt;，则置信度可以表示为：&lt;/p&gt;
&lt;center&gt;
&lt;p&gt;confidence = &lt;font color=   #9370DB&gt;&lt;strong&gt;紫色边界框(truth)&lt;/strong&gt;&lt;/font&gt;和&lt;font color=#FFD700&gt;&lt;strong&gt;黄色边界框(pred)&lt;/strong&gt;&lt;/font&gt;的IOU&lt;/p&gt;
&lt;/center&gt;
&lt;p&gt;如果物体不存在，即中心点不存在，那么&lt;span class=&#34;math inline&#34;&gt;\(Pr(\text{Object})=0\)&lt;/span&gt;，则置信度为0。&lt;/p&gt;
&lt;p&gt;原文是这样描述的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Formally we define confidence as &lt;span class=&#34;math inline&#34;&gt;\(Pr(\text{Object}) * \text{IOU}^{\text{truth}}_{\text{pred}}\)&lt;/span&gt;. If no object exists in that cell, the confidence scores should be zero. Otherwise we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;6.PNG&#34; width = &#34;1000&#34; height = &#34;900&#34; alt=&#34;Figure6&#34; /&gt;
&lt;p&gt;
图6. 第二个Bounding Box对应的值
&lt;/div&gt;
&lt;div id=&#34;iou&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;IOU的定义&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;这里又多次提及了交并比IOU，下面就IOU定义进行介绍：&lt;/p&gt;
&lt;p&gt;假设存在两个矩形框，它们的交集部分叫做Intersection，它们的并集部分叫做Union，那么它们的比就是交并比IOU。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;7.PNG&#34; width = &#34;800&#34; height = &#34;800&#34; alt=&#34;Figure7&#34; /&gt;
&lt;p&gt;
图7. IOU的计算定义
&lt;/div&gt;
&lt;p&gt;最后的20个通道为20类物体的对应概率，如果第一个预测的是猫（cat）的概率的话，我们可以表示为&lt;span class=&#34;math inline&#34;&gt;\(Pr(\text{Cat|Object})\)&lt;/span&gt;，即存在Object情况下物体时Cat的条件概率。&lt;/p&gt;
&lt;p&gt;原文中这样说到：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We only predict one set of class probabilities per grid cell, regardless of the number of boxes B.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The predictions are encoded as an &lt;span class=&#34;math inline&#34;&gt;\(S\times S\times(B*5+C)\)&lt;/span&gt; tensor.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;无论有多少个Bounding Boxes，都只会计算一次概率，最终预测结果会被编码为&lt;span class=&#34;math inline&#34;&gt;\(S\times S\times(B*5+C)\)&lt;/span&gt;的张量。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;8.PNG&#34; width = &#34;1000&#34; height = &#34;900&#34; alt=&#34;Figure8&#34; /&gt;
&lt;p&gt;
图8. 后20个通道对应20类物体概率
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part---loss-function&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;最重要的Part - Loss function&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;整体Loss function可以分为三个部分：坐标误差，IOU误差和分类误差；误差均用平方和来衡量。整体上看，可以发现一些特殊的记号：&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{1}_{ij}^{obj}\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{1}_{ij}^{noobj}\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{1}_{i}^{obj}\)&lt;/span&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{1}_{i}^{obj}\)&lt;/span&gt; denotes if object appears in cell &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{1}_{ij}^{obj}\)&lt;/span&gt; denotes that the &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; bounding box predictor in cell &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is “responsible” for that prediction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;它们的定义是这样的： &lt;span class=&#34;math display&#34;&gt;\[
\begin{array}{l}
\mathbb{1}_{ij}^{obj} = \left\{ \begin{array}{l}
{1，}{\text{存在Object且若第j个BBox对第i个grid cell负责}}\\
{0，}{\text{否则}}
\end{array} \right.\\
\mathbb{1}_{ij}^{noobj} = \left\{ \begin{array}{l}
{1，}{\text{如果grid cell不包含Object}}\\
{0，}{\text{否则}}
\end{array} \right.\\
\mathbb{1}_{i}^{obj} = \left\{ \begin{array}{l}
{1，}{\text{如果grid cell存在Object}}\\
{0，}{\text{否则}}
\end{array} \right.
\end{array}
\]&lt;/span&gt; 这里的负责可以假设其拥有最高IOU。&lt;/p&gt;
&lt;p&gt;首先，坐标误差很明显的就是Bounding Box的坐标(&lt;span class=&#34;math inline&#34;&gt;\(x,y,w,h\)&lt;/span&gt;)和真实的Bounding Box的坐标(&lt;span class=&#34;math inline&#34;&gt;\(\hat{x},\hat{y},\hat{w},\hat{h}\)&lt;/span&gt;)的误差，其中(&lt;span class=&#34;math inline&#34;&gt;\(w,h\)&lt;/span&gt;)用根号来表示的目的是为了缩小大的Object相比小的Object的坐标带来的误差影响；关于平方根原文是这么解释的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Sum-squared error also equally weights errors in large boxes and small boxes. Our error metric should reflect that small deviations in large boxes matter less than in small boxes. To partially address this we predict the square root of the bounding box width and height instead of the width and height directly.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第二部分的IOU误差其实也是前面提到的confidence的误差，这里计算了包含Object的Bounding Box的confidence误差，也计算量不包含Object的Bounding Box的confidence误差。由于一张图像中大部分grid cell是不包含Object的，它们的confidence也就趋于0，这就会变相放大包含Object的grid cell的损失函数在计算梯度时的影响，使得模型不稳定，训练时会趋于发散。因此，通过调节参数&lt;span class=&#34;math inline&#34;&gt;\(\lambda_{coord}\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(\lambda{noobj}\)&lt;/span&gt;来改善问题；作者设定&lt;span class=&#34;math inline&#34;&gt;\(\lambda_{coord}=5\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(\lambda{noobj}=0.5\)&lt;/span&gt;。原文这么阐述：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Also, in every image many grid cells do not contain any object. This pushes the “confidence” scores of those cells towards zero, often overpowering the gradient from cells that do contain objects. This can lead to model instability, causing training to diverge early on. To remedy this, we increase the loss from bounding box coordinate predictions and decrease the loss from confidence predictions for boxes that don’t contain objects. We use two parameters, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{coord}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lambda{noobj}\)&lt;/span&gt; to accomplish this. We set &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{coord}=5\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lambda{noobj}=0.5\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第三部分分类误差是衡量了各种检测物体类别的条件概率的误差。上笔记图！&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;9.PNG&#34; width = &#34;800&#34; height = &#34;800&#34; alt=&#34;Figure9&#34; /&gt;
&lt;p&gt;
图9. Loss function各部分的含义
&lt;/div&gt;
&lt;p&gt;训练过程的思路就这么多了！开始Detection部分！&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;yolodetection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;YOLO模型测试阶段（Detection）解析&lt;/h3&gt;
&lt;p&gt;在Detection阶段主要用了条件概率公式： &lt;span class=&#34;math display&#34;&gt;\[
Pr(\text{Class}_i|\text{Object})*Pr(Object)*\text{IOU}^{\text{truth}}_{\text{pred}}=Pr(\text{Class}_i)*\text{IOU}^{\text{truth}}_{\text{pred}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;显然，第一部分&lt;span class=&#34;math inline&#34;&gt;\(Pr(\text{Class}_i|\text{Object})\)&lt;/span&gt;是后面20个channels中存储的条件概率，第二部分&lt;span class=&#34;math inline&#34;&gt;\(Pr(Object)*\text{IOU}^{\text{truth}}_{\text{pred}}\)&lt;/span&gt;则是每个Bounding Box对应的置信度，如图10所示。对于图像中某个grid cell，其中张量进行乘法运算（图10中蓝色部分相乘），可以得到一列类的得分向量，图中黄色矩形条表示第一个Bounding Box的类得分向量。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;10.PNG&#34; width = &#34;800&#34; height = &#34;800&#34; alt=&#34;Figure10&#34; /&gt;
&lt;p&gt;
图10. 第一个Bounding Box中条件概率公式各部分对应张量的不同通道位置相乘
&lt;/div&gt;
&lt;p&gt;同理，第二个Bounding Box也执行相同的乘法操作。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;11.PNG&#34; width = &#34;800&#34; height = &#34;800&#34; alt=&#34;Figure11&#34; /&gt;
&lt;p&gt;
图11. 第二个Bounding Box中条件概率公式各部分对应张量的不同通道位置相乘
&lt;/div&gt;
&lt;p&gt;对每个grid cell重复上述操作，每个grid cell若有两个Bounding Boxes，则得到两列类得分向量，如图12所示。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;12.PNG&#34; width = &#34;800&#34; height = &#34;800&#34; alt=&#34;Figure12&#34; /&gt;
&lt;p&gt;
图12. 每个grid cell的两个Bounding Boxes中执行乘法操作
&lt;/div&gt;
&lt;p&gt;如果图像分为&lt;span class=&#34;math inline&#34;&gt;\(7\times 7\)&lt;/span&gt;个grid cell，每个grid cell产生2个Bounding Boxes，那总共会生成98列得分向量。我们设定一定的阈值，比如0.2；小于0.2的得分我们设置得分为0。再通过降序排列，进一步执行NMS（非极大值抑制，Non-Maximum Suppression）操作，去除多余的Bounding Boxes（如图13所示）。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;13.PNG&#34; width = &#34;800&#34; height = &#34;800&#34; alt=&#34;Figure13&#34; /&gt;
&lt;p&gt;
图13. 得分矩阵的处理
&lt;/div&gt;
&lt;div id=&#34;non-max-suppression&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;非极大值抑制（Non-max suppression）&lt;/strong&gt;&lt;/h4&gt;
我们以第一类dog为例，假设实际只有四个框（&lt;font color=  #ffa500&gt;&lt;strong&gt;橙色边界框&lt;/strong&gt;&lt;/font&gt;/&lt;font color= #adff2f&gt;&lt;strong&gt;青色边界框&lt;/strong&gt;&lt;/font&gt;/&lt;font color= #0000cd&gt;&lt;strong&gt;蓝色边界框&lt;/strong&gt;&lt;/font&gt;/&lt;font color=   #ff00ff&gt;&lt;strong&gt;紫色边界框&lt;/strong&gt;&lt;/font&gt;）圈到了图像中的物体狗，它们对应的得分为(0.8 / 0.5 / 0.3 / 0.2)。非极大值抑制实际上是通过设定一定IOU阈值，从第一个框开始遍历，如果两个框的IOU大于设定的阈值，那么我们可以认为这两个框相似度很大，检测的是同一个物体，但是得分较大的框更好的圈出了检测的物体；所以保留得分大的框，把另外一个去除。如果两个框的IOU小于设定的阈值，则保留继续后面的判断。对于得分为0的框则不判断。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;14.PNG&#34; width = &#34;800&#34; height = &#34;800&#34; alt=&#34;Figure14&#34; /&gt;
&lt;p&gt;
图14. NMS过程（一）
&lt;/div&gt;
由于排序后排后的边界框对应的得分都为0，所以不再拿第一个边界框与其他比较；下一步取第二个不为零的框作为最大得分框，与后面的边界框进行IOU判断，与上面的步骤一致。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;15.PNG&#34; width = &#34;800&#34; height = &#34;800&#34; alt=&#34;Figure15&#34; /&gt;
&lt;p&gt;
图15. NMS过程（二）
&lt;/div&gt;
我们假定只有四个边界框的得分非零，现在只剩下两个边界框得分不为零了，这样第一类的边界框得分判定结束；同理，对其他类进行NMS操作。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;16.PNG&#34; width = &#34;800&#34; height = &#34;800&#34; alt=&#34;Figure16&#34; /&gt;
&lt;p&gt;
图16. NMS过程（三）
&lt;/div&gt;
完成上述的NMS操作后，取出边界框得分向量中的最大得分以及其对应的类别，如果这个得分是大于0的，就在图像上画下边界框以及标出对应的类别；否则丢弃当前边界框。对所有的得分向量重复上述操作。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;17.PNG&#34; width = &#34;800&#34; height = &#34;800&#34; alt=&#34;Figure17&#34; /&gt;
&lt;p&gt;
图17. 提取边界框（Bounding Box）
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;yolo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;YOLO代码解析与实践&lt;/h2&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;训练&lt;/h3&gt;
&lt;p&gt;首先，从&lt;a href=&#34;https://github.com/hizhangp/yolo_tensorflow&#34;&gt;hizhangp&lt;/a&gt;的Github上下载YOLO version1在TensorFlow复现的代码。由于我们用Pascal VOC数据集进行训练，所以我们需要先对数据集的图像进行处理。&lt;/p&gt;
&lt;div id=&#34;pascal_voc.py&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;pascal_voc.py文件&lt;/strong&gt;&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;import os
import xml.etree.ElementTree as ET
import numpy as np
import cv2
import pickle
import copy
import yolo.config as cfg

class pascal_voc(object):
  def _init_(self, phase, rebuild=False):
      # config.py中参数已经设置完毕并通过cfg.调用
      # devkil_path存放VOCdevkit的路径
      self.devkil_path = os.path.join(cfg.PASCAL_PATH, &amp;#39;VOCdevkit&amp;#39;)
      # data_path是数据存放在VOC2007中的路径
      self.data_path = os.path.join(self.devkil_path, &amp;#39;VOC2007&amp;#39;)
      # 缓存路径
      self.cache_path = cfg.CACHE_PATH
      # 批量大小
      self.batch_size = cfg.BATCH_SIZE
      # 图像大小
      self.image_size = cfg.IMAGE_SIZE
      # 图像分割为Grid Cell的数量，图像分割为多少块
      self.cell_size = cfg.CELL_SIZE
      # 识别object中可能出现的类
      self.classes = cfg.CLASSES
      # 将可能出现的类转换为字典(dictionary)
      # range()产生从0到self.classes的长度减1的序列;
      # 若len(self.classes)=20,
      # 则产生[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]
      # zip()打包为元组的列表
      # dict()构造字典
      self.class_to_ind = dict(zip(self.classes, range(len(self.classes))))
      # config中FLIPPED设置为True
      self.flipped = cfg.FLIPPED
      # _init_函数的输入变量
      self.phase = phase
      self.rebuild = rebuild
      # 其他参数
      self.cursor = 0
      self.epoch = 1
      self.gt_labels = None
      self.prepare()

  def image_read(self, imname, flipped=False):
      # 读取图像
      image = cv2.imread(imname)
      # resize图像为image_size大小
      image = cv2.resize(image, (self.image_size, self.image_size))
      # opencv读取的是bgr格式,转换为rgb格式,并字段类型转换为float32
      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)
      # 归一化到[-1,1]
      image = (image / 255.0) * 2.0 - 1.0
      # flipped为翻转
      # ::n代表序列中每第n项; -1指从倒数开始截取
      # ::-1会将序列从头到尾颠倒
      if flipped:
        image = image[:, ::-1, :]
      return image

  def load_pascal_annotation(self, index):
      # 加载图片数据
      # Load image and bounding boxes info from XML file in the PASCAL VOC format
      imname = os.path.join(self.data_path, &amp;#39;JPEGImages&amp;#39;, index + &amp;#39;.jpg&amp;#39;)
      im = cv2.imread(imname)
      # im = cv2.resize(im, [self.image_size, self.image_size])
      h_ratio = 1.0 * self.image_size / im.shape[0]
      w_ratio = 1.0 * self.image_size / im.shape[1]
      # 置信度(1) + Bounding Box坐标(2-5) + 20类class(6-25)
      label = np.zeros((self.cell_size, self.cell_size, 25))
      filename = os.path.join(self.data_path, &amp;#39;Annotations&amp;#39;, index + &amp;#39;.xml&amp;#39;)
      tree = ET.parse(filename)
      objs = tree.findall(&amp;#39;object&amp;#39;)
      for obj in objs:
        bbox = obj.find(&amp;#39;bndbox&amp;#39;)
        # Make pixel indexes 0-based
        x1 = max(min((float(bbox.find(&amp;#39;xmin&amp;#39;).text) - 1) * w_ratio, self.image_size - 1), 0)
        y1 = max(min((float(bbox.find(&amp;#39;ymin&amp;#39;).text) - 1) * h_ratio, self.image_size - 1), 0)
        x2 = max(min((float(bbox.find(&amp;#39;xmax&amp;#39;).text) - 1) * w_ratio, self.image_size - 1), 0)
        y2 = max(min((float(bbox.find(&amp;#39;ymax&amp;#39;).text) - 1) * h_ratio, self.image_size - 1), 0)
        cls_ind = self.class_to_ind[obj.find(&amp;#39;name&amp;#39;).text.lower().strip()]
        boxes = [(x2 + x1) / 2.0, (y2 + y1) / 2.0, x2 - x1, y2 - y1]
        x_ind = int(boxes[0] * self.cell_size / self.image_size)
        y_ind = int(boxes[1] * self.cell_size / self.image_size)
        if label[y_ind, x_ind, 0] == 1:
          continue
        label[y_ind, x_ind, 0] = 1
        label[y_ind, x_ind, 1:5] = boxes
        label[y_ind, x_ind, 5 + cls_ind] = 1
      return label, len(objs)

  def load_labels(self):
      cache_file = os.path.join(self.cache_path, &amp;#39;pascal_&amp;#39; + self.phase + &amp;#39;_gt_labels.pkl&amp;#39;)
      if os.path.isfile(cache_file) and not self.rebuild:
        print(&amp;#39;Loading gt_labels from: &amp;#39; + cache_file)
        with open(cache_file, &amp;#39;rb&amp;#39;) as f:
          gt_labels = pickle.load(f)
        return gt_labels
      print(&amp;#39;Processing gt_labels from: &amp;#39; + self.data_path)
      if not os.path.exists(self.cache_path):
        os.makedirs(self.cache_path)
      if self.phase == &amp;#39;train&amp;#39;:
        txtname = os.path.join(self.data_path, &amp;#39;ImageSets&amp;#39;, &amp;#39;Main&amp;#39;, &amp;#39;trainval.txt&amp;#39;)
      else:
        txtname = os.path.join(self.data_path, &amp;#39;ImageSets&amp;#39;, &amp;#39;Main&amp;#39;, &amp;#39;test.txt&amp;#39;)
      with open(txtname, &amp;#39;r&amp;#39;) as f:
        self.image_index = [x.strip() for x in f.readlines()]
      gt_labels = []
      # 通过load_pascal_annotation()函数将所有图像读取出来
      # 一个一个做成label并存放在gt_labels里面,同时保存在pickle里面
      for index in self.image_index:
        label, num = self.load_pascal_annotation(index)
        if num == 0:
          continue
        imname = os.path.join(self.data_path, &amp;#39;JPEGImages&amp;#39;, index + &amp;#39;.jpg&amp;#39;)
        gt_labels.append({&amp;#39;imname&amp;#39;: imname, &amp;#39;label&amp;#39;: label, &amp;#39;flipped&amp;#39;: False})
      print(&amp;#39;Saving gt_labels to: &amp;#39; + cache_file)
      with open(cache_file, &amp;#39;wb&amp;#39;) as f:
        pickle.dump(gt_labels, f)
      return gt_labels

  def prepare(self):
      gt_labels = self.load_labels()
      if self.flipped:
        print(&amp;#39;Appending horizontally-flipped training examples ...&amp;#39;)
        gt_labels_cp = copy.deepcopy(gt_labels)
        for idx in range(len(gt_labels_cp)):
          gt_labels_cp[idx][&amp;#39;flipped&amp;#39;] = True
          gt_labels_cp[idx][&amp;#39;label&amp;#39;] = gt_labels_cp[idx][&amp;#39;label&amp;#39;][:, ::-1, :]
          for i in range(self.cell_size):
            for j in range(self.cell_size):
              if gt_labels_cp[idx][&amp;#39;label&amp;#39;][i, j, 0] == 1:
                gt_labels_cp[idx][&amp;#39;label&amp;#39;][i, j, 1] = self.image_size - 1 - gt_labels_cp[idx][&amp;#39;label&amp;#39;][i, j, 1]
        gt_labels += gt_labels_cp
      np.random.shuffle(gt_labels)
      self.gt_labels = gt_labels
      return gt_labels

  def get(self):
      # np.zeros()创建指定大小的数组，数组元素以 0 来填充
      images = np.zeros((self.batch_size, self.image_size, self.image_size, 3))
      labels = np.zeros((self.batch_size, self.cell_size, self.cell_size, 25))
      count = 0
      while count &amp;lt; self.batch_size:
        # 调用self类中gt_labels,cursor和函数image_read()
        imname = self.gt_labels[self.cursor][&amp;#39;imname&amp;#39;]
        flipped = self.gt_labels[self.cursor][&amp;#39;flipped&amp;#39;]
        images[count, :, :, :] = self.image_read(imname, flipped)
        labels[count, :, :, :] = self.gt_labels[self.cursor][&amp;#39;label&amp;#39;]
        count += 1
        self.cursor += 1
        if self.cursor &amp;gt;= len(self.gt_labels):
          np.random.shuffle(self.gt_labels)
          self.cursor = 0
          self.epoch += 1
      return images, labels&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;yolo_net.py&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;再看看网络搭建的yolo_net.py文件&lt;/strong&gt;&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;import numpy as np
import tensorflow as tf
import yolo.config as cfg
import tensorflow.contrib.slim as slim
# slim = tf.contrib.slim

class YOLONet(object):
  def __init__(self, is_training=True):
    # 从config中获取预测的类
    self.classes = cfg.CLASSES
    # 预测的类的数量(类别数)
    self.num_class = len(self.classes)
    # 图像尺寸
    self.image_size = cfg.IMAGE_SIZE
    # Grid cell的数量
    self.cell_size = cfg.CELL_SIZE
    # 每个Grid cell生成Bounding Box的数量
    self.boxes_per_cell = cfg.BOXES_PER_CELL
    # 输出尺寸: S * S * (C + B * 5)
    self.output_size = (self.cell_size * self.cell_size) * (self.num_class + self.boxes_per_cell * 5)
    self.scale = 1.0 * self.image_size / self.cell_size
    self.boundary1 = self.cell_size * self.cell_size * self.num_class
    self.boundary2 = self.boundary1 + self.cell_size * self.cell_size * self.boxes_per_cell
    self.object_scale = cfg.OBJECT_SCALE
    self.noobject_scale = cfg.NOOBJECT_SCALE
    self.class_scale = cfg.CLASS_SCALE
    self.coord_scale = cfg.COORD_SCALE
    # 学习率
    self.learning_rate = cfg.LEARNING_RATE
    # 批量尺寸
    self.batch_size = cfg.BATCH_SIZE
    # 激活函数leaky_relu的参数alpha(取0.1)
    self.alpha = cfg.ALPHA
    # Bounding Box的中心点(center point)坐标 -- x, y -- 相对于左上角点的偏移量
    # (self.B, self.S, self.S)为三维数组的维度，有self.B个array，每个array的维度为(self.S, self.S)
    # (1,2,0)改变了数组的构造，将会有self.S个array，每个array的维度为(self.S, self.B)
    self.offset = np.transpose(np.reshape(np.array([np.arange(self.cell_size)] * self.cell_size * self.boxes_per_cell), (self.boxes_per_cell, self.cell_size, self.cell_size)), (1, 2, 0))
    self.images = tf.placeholder(tf.float32, [None, self.image_size, self.image_size, 3], name=&amp;#39;images&amp;#39;)
    # 调用函数build_network()
    self.logits = self.build_network(self.images, num_outputs=self.output_size, alpha=self.alpha, is_training=is_training)

    if is_training:
        self.labels = tf.placeholder(tf.float32, [None, self.cell_size, self.cell_size, 5 + self.num_class])
        self.loss_layer(self.logits, self.labels)
        self.total_loss = tf.losses.get_total_loss()
        tf.summary.scalar(&amp;#39;total_loss&amp;#39;, self.total_loss)
  
  def build_network(self, images, num_outputs, alpha, keep_prob=0.5, is_training=True, scope=&amp;#39;yolo&amp;#39;):
    # 网络结构可对照paper的Figure 3: The Architecture验证
    # 
    # 导入tf.contrib.slim为slim (import tf.contrib.slim as slim)
    # 
    # slim.conv2d(inputs, num_outputs, kernel_size, stride, padding, ..., scope)
    # inputs指需要做卷积的输入图像
    # num_outputs指定卷积核的个数
    # kernel_size指定卷积核的维度
    # stride为卷积时在图像每一维的步长
    # padding可选VALID或SAME
    # scope为共享变量所指的variable_scope
    # 
    # slim.max_pool2d(inputs, kernel_size)函数表示最大池化层, 第二个参数表示池化层核的维度
    # 
    # slim.flatten(tensor)表示将tensor扁平化
    # 假设tensor为四维张量, flatten函数会转化tensor为三维, 三维数组中有多个二维数组, 二维数组均为1*k的数组
    # 
    # slim.fully_connected()表示全连接层, 前两个参数分别为网络输入、输出的神经元数量
    # slim.fully_connected(net, 512, scope=&amp;#39;fc_33&amp;#39;)中输出的神经元数量为512
    # 
    # slim.dropout(inputs, keep_prob, is_training, ...)
    # inputs: The tensor to pass to the nn.dropout op.
    # keep_prob: A scalar `Tensor` with the same type as x. The probability that each element is kept.
    # is_training: A bool `Tensor` indicating whether or not the model is in training mode. 
    #              If so, dropout is applied and values scaled. Otherwise, inputs is returned.
    # dropout函数的使用目的是防止或减轻过拟合, 它一般用在全连接层
    # 在不同的训练过程中随机扔掉一部分神经元, 让某个神经元的激活值以一定的概率p, 让其停止工作
    # 这次训练过程中不更新权值, 也不参加神经网络的计算
    # 但它的权重得保留下来(只是暂时不更新而已), 因为下次样本输入时它可能又得工作了
    # 
    # tf.pad(tensor, paddings, mode=&amp;#39;CONSTANT&amp;#39;, name=None)函数:
    # 第二个参数paddings也是一个张量(tensor)
    # 代表每一维填充多少行/列，但是有一个要求它的rank一定要和tensor的rank是一样的
    # tf.pad(tensor, [[1,2],[3,4]])会将二维数组左边加1个0, 右边加2个0, 上边加3个0, 下边加4个0
    # 第三个参数中&amp;#39;CONSTANT&amp;#39;表示填充的元素为0
    # 本例中, 对于四维张量中四个维度的不同两个方向进行添加0元素
    # pad目的是填充补数,保证最后卷积层输出特征图大小为7*7
    # 
    # tf.transpose(inputs, perm, name=None)函数:
    # 第二个参数perm=[0,1,2]中
    # 0代表三维数组的高(即为二维数组的个数), 1代表二维数组的行, 2代表二维数组的列
    # 若在四维空间中, 0代表四维空间的高(即为三维数组的个数)
    # 1代表三维数组的高(即为二维数组的个数), 2代表二维数组的行, 3代表二维数组的列
    # tf.transpose(net, [0, 3, 1, 2], name=&amp;#39;trans_31&amp;#39;)
    # [0,3,1,2]表示每个三维数组中, 三维数组的高变为每个二维数组的行
    # 二维数组的行变为二维数组的列, 二维数组的列变为三维数组的高
    with tf.variable_scope(scope):
      with slim.arg_scope([slim.conv2d, slim.fully_connected], activation_fn=leaky_relu(alpha), weights_regularizer=slim.l2_regularizer(0.0005), weights_initializer=tf.truncated_normal_initializer(0.0, 0.01)):
        net = tf.pad(images, np.array([[0, 0], [3, 3], [3, 3], [0, 0]]), name=&amp;#39;pad_1&amp;#39;)
        net = slim.conv2d(net, 64, 7, 2, padding=&amp;#39;VALID&amp;#39;, scope=&amp;#39;conv_2&amp;#39;)
        net = slim.max_pool2d(net, 2, padding=&amp;#39;SAME&amp;#39;, scope=&amp;#39;pool_3&amp;#39;)
        net = slim.conv2d(net, 192, 3, scope=&amp;#39;conv_4&amp;#39;)
        net = slim.max_pool2d(net, 2, padding=&amp;#39;SAME&amp;#39;, scope=&amp;#39;pool_5&amp;#39;)
        net = slim.conv2d(net, 128, 1, scope=&amp;#39;conv_6&amp;#39;)
        net = slim.conv2d(net, 256, 3, scope=&amp;#39;conv_7&amp;#39;)
        net = slim.conv2d(net, 256, 1, scope=&amp;#39;conv_8&amp;#39;)
        net = slim.conv2d(net, 512, 3, scope=&amp;#39;conv_9&amp;#39;)
        net = slim.max_pool2d(net, 2, padding=&amp;#39;SAME&amp;#39;, scope=&amp;#39;pool_10&amp;#39;)
        net = slim.conv2d(net, 256, 1, scope=&amp;#39;conv_11&amp;#39;)
        net = slim.conv2d(net, 512, 3, scope=&amp;#39;conv_12&amp;#39;)
        net = slim.conv2d(net, 256, 1, scope=&amp;#39;conv_13&amp;#39;)
        net = slim.conv2d(net, 512, 3, scope=&amp;#39;conv_14&amp;#39;)
        net = slim.conv2d(net, 256, 1, scope=&amp;#39;conv_15&amp;#39;)
        net = slim.conv2d(net, 512, 3, scope=&amp;#39;conv_16&amp;#39;)
        net = slim.conv2d(net, 256, 1, scope=&amp;#39;conv_17&amp;#39;)
        net = slim.conv2d(net, 512, 3, scope=&amp;#39;conv_18&amp;#39;)
        net = slim.conv2d(net, 512, 1, scope=&amp;#39;conv_19&amp;#39;)
        net = slim.conv2d(net, 1024, 3, scope=&amp;#39;conv_20&amp;#39;)
        net = slim.max_pool2d(net, 2, padding=&amp;#39;SAME&amp;#39;, scope=&amp;#39;pool_21&amp;#39;)
        net = slim.conv2d(net, 512, 1, scope=&amp;#39;conv_22&amp;#39;)
        net = slim.conv2d(net, 1024, 3, scope=&amp;#39;conv_23&amp;#39;)
        net = slim.conv2d(net, 512, 1, scope=&amp;#39;conv_24&amp;#39;)
        net = slim.conv2d(net, 1024, 3, scope=&amp;#39;conv_25&amp;#39;)
        net = slim.conv2d(net, 1024, 3, scope=&amp;#39;conv_26&amp;#39;)
        net = tf.pad(net, np.array([[0, 0], [1, 1], [1, 1], [0, 0]]), name=&amp;#39;pad_27&amp;#39;)
        net = slim.conv2d(net, 1024, 3, 2, padding=&amp;#39;VALID&amp;#39;, scope=&amp;#39;conv_28&amp;#39;)
        net = slim.conv2d(net, 1024, 3, scope=&amp;#39;conv_29&amp;#39;)
        net = slim.conv2d(net, 1024, 3, scope=&amp;#39;conv_30&amp;#39;)
        net = tf.transpose(net, [0, 3, 1, 2], name=&amp;#39;trans_31&amp;#39;)
        net = slim.flatten(net, scope=&amp;#39;flat_32&amp;#39;)
        # First fully connected layer
        net = slim.fully_connected(net, 512, scope=&amp;#39;fc_33&amp;#39;)
        net = slim.fully_connected(net, 4096, scope=&amp;#39;fc_34&amp;#39;)
        # From section 2.2 in paper, &amp;quot;After first fully connected layer
        # A dropout layer with rate = .5 after the first connected layer prevents co-adaptation between layers&amp;quot;
        net = slim.dropout(net, keep_prob=keep_prob, is_training=is_training, scope=&amp;#39;dropout_35&amp;#39;)
        net = slim.fully_connected(net, num_outputs, activation_fn=None, scope=&amp;#39;fc_36&amp;#39;)
    return net

  def calc_iou(self, boxes1, boxes2, scope=&amp;#39;iou&amp;#39;):
    # Calculate IOUs
    # Args:
    # boxes1: 5-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4]  ====&amp;gt; (x_center, y_center, w, h)
    # boxes2: 5-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4] ===&amp;gt; (x_center, y_center, w, h)
    # Return:
    # iou: 4-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]
    with tf.variable_scope(scope):
      # Transform (x_center, y_center, w, h) to (x1, y1, x2, y2)
      # 将(x_center, y_center, w, h)转换为Bounding Box的左上角和右下角坐标(x1, y1, x2, y2)
      # Bounding Box左上角坐标: (x_center - w / 2, y_center - h / 2)
      # Bounding Box右下角坐标: (x_center + w / 2, y_center + h / 2)
      boxes1_t = tf.stack([boxes1[..., 0] - boxes1[..., 2] / 2.0, boxes1[..., 1] - boxes1[..., 3] / 2.0, boxes1[..., 0] + boxes1[..., 2] / 2.0, boxes1[..., 1] + boxes1[..., 3] / 2.0], axis=-1)
      boxes2_t = tf.stack([boxes2[..., 0] - boxes2[..., 2] / 2.0, boxes2[..., 1] - boxes2[..., 3] / 2.0, boxes2[..., 0] + boxes2[..., 2] / 2.0, boxes2[..., 1] + boxes2[..., 3] / 2.0], axis=-1)
      # 计算两个Bounding Boxes的相交点坐标
      # Calculate the left up point &amp;amp; right down point
      # 相交框(intersection)左上角坐标(lu)为两个Bounding Boxes的左上角较大点坐标
      # 相交框(intersection)右下角坐标(rd)为两个Bounding Boxes的右下角较小点坐标
      lu = tf.maximum(boxes1_t[..., :2], boxes2_t[..., :2])
      rd = tf.minimum(boxes1_t[..., 2:], boxes2_t[..., 2:])
      # Intersection
      # rd-lu可以得到相交部分框的长和宽
      # 与0取最大是为了删除不合理的框，比如两个框无交集
      # inter_square为面积(长*宽)
      intersection = tf.maximum(0.0, rd - lu)
      inter_square = intersection[..., 0] * intersection[..., 1]
      # 计算两个Bounding Boxes的面积
      # Calculate the boxs1 square and boxs2 square
      square1 = boxes1[..., 2] * boxes1[..., 3]
      square2 = boxes2[..., 2] * boxes2[..., 3]
      # union_square为两个Bounding Boxes的总面积减去相交面积
      # tf.maximum保证相交面积不为0,由于下一步计算作为分母
      union_square = tf.maximum(square1 + square2 - inter_square, 1e-10)
      # tf.clip_by_value()函数: 若交并比大于1则为1; 若交并比小于0则为0
    return tf.clip_by_value(inter_square / union_square, 0.0, 1.0)

  def loss_layer(self, predicts, labels, scope=&amp;#39;loss_layer&amp;#39;):
    # tf.reshape(tensor, shape)函数:
    # 将张量tensor重塑为shape设定的维度下的张量
    # 
    # tf.tile(inputs, multiples, name=None)函数: 
    # tf.tile(boxes, [1, 1, 1, self.boxes_per_cell, 1])
    # 第二个参数表示每个维度复制的次数，这个地方只是对轴3复制两次，因为我们知道一个cell_size负责两个框的预测
    # 
    # tf.stack(tensor, axis)函数:
    # 在新的维度上拼接, 拼接后维度加1; 若axis=0, 指第一个维度; 若axis=-1, 指最后一个维度
    # t1 = [[1, 2, 3], [4, 5, 6]]
    # t2 = [[7, 8, 9], [10, 11, 12]]
    # t3 = tf.stack([t1,t2],axis=0)
    # t4 = tf.stack([t1,t2],axis=1)
    # t5 = tf.stack([t1,t2],axis=-1)
    # with tf.Session() as sess: print(sess.run(t3))
    # [[[ 1  2  3]
    #   [ 4  5  6]]
    # 
    #  [[ 7  8  9]
    #   [10 11 12]]]
    # with tf.Session() as sess: print(sess.run(t4))
    # [[[ 1  2  3]
    #   [ 7  8  9]]
    # 
    #  [[ 4  5  6]
    #   [10 11 12]]]
    # with tf.Session() as sess: print(sess.run(t5))
    # [[[ 1  7]
    #   [ 2  8]
    #   [ 3  9]]
    # 
    #  [[ 4 10]
    #   [ 5 11]
    #   [ 6 12]]]
    #
    # tf.ones_like(tensor, dtype, ...)函数：生成一个和tensor相同形状的, 数据类型为dtype, 所有元素都被设置为1
    # 
    # tf.cast(x, dtype, name=None)函数: 将x转换为dtype
    # tensor a is [1.8, 2.2], dtype = tf.float
    # tf.cast(a, tf.int32) ==&amp;gt; [1, 2], where dtype = tf.int32
    # 
    # tf.reduce_mean(tensor, axis=None, keep_dims=False, name=None)函数: 求tensor中平均值, axis参数指定轴方向
    # 
    # tf.reduce_max(tensor, axis=None, keep_dims=False, name=None)函数: 求tensor中最大值, axis参数指定轴方向
    # 
    # tf.reduce_sum(tensor, axis=None, keep_dims=False, name=None)函数:求tensor中元素和, axis参数指定轴方向
    # 
    # tf.square(x, name=None)函数: 计算平方x^2
    #
    # tf.sqrt(x, name=None)函数: 计算开根号x^{1/2}
    # 
    # tf.expand_dims(tensor, dim, name=None)函数: tensor维度加1; 第二个参数dim表示维度扩增的方向
    # t6 = tf.expand_dims(t1,0)
    # t7 = tf.expand_dims(t1,1)
    # t8 = tf.expand_dims(t1,2) 和 tf.expand_dims(t1,-1) 相同; -1指最后一个维度
    #
    # with tf.Session() as sess: print(sess.run(t6))
    # [[[1 2 3]
    #   [4 5 6]]]
    # with tf.Session() as sess: print(sess.run(t7))
    # [[[1 2 3]]
    # 
    #  [[4 5 6]]]
    # with tf.Session() as sess: print(sess.run(t8))
    # [[[1]
    #   [2]
    #   [3]]
    # 
    #  [[4]
    #   [5]
    #   [6]]] 
    #
    # tf.losses.add_loss()函数: 将外部定义的loss添加到losses的集合中
    # 
    # tf.summary.scalar()函数: 查看learning rate和目标函数如何变化
    # 
    # tf.summary.histogram()函数: 查看activations, gradients或者weights的分布
    # 
    with tf.variable_scope(scope):
      # 预测类别
      predict_classes = tf.reshape(predicts[:, :self.boundary1], [self.batch_size, self.cell_size, self.cell_size, self.num_class])
      # 预测置信度
      predict_scales = tf.reshape(predicts[:, self.boundary1:self.boundary2], [self.batch_size, self.cell_size, self.cell_size, self.boxes_per_cell])
      # 预测Bounding Box
      predict_boxes = tf.reshape(predicts[:, self.boundary2:], [self.batch_size, self.cell_size, self.cell_size, self.boxes_per_cell, 4])
      # response是提取label中的置信度，表示这个地方是否有框
      response = tf.reshape(labels[..., 0], [self.batch_size, self.cell_size, self.cell_size, 1])
      # 提取框
      boxes = tf.reshape(labels[..., 1:5],[self.batch_size, self.cell_size, self.cell_size, 1, 4])
      boxes = tf.tile(boxes, [1, 1, 1, self.boxes_per_cell, 1]) / self.image_size
      # 提取类(one-hot编码)
      classes = labels[..., 5:]
      offset = tf.reshape(tf.constant(self.offset, dtype=tf.float32), [1, self.cell_size, self.cell_size, self.boxes_per_cell])
      offset = tf.tile(offset, [self.batch_size, 1, 1, 1])
      offset_tran = tf.transpose(offset, (0, 2, 1, 3))
      predict_boxes_tran = tf.stack([(predict_boxes[..., 0] + offset) / self.cell_size, (predict_boxes[..., 1] + offset_tran) / self.cell_size, tf.square(predict_boxes[..., 2]), tf.square(predict_boxes[..., 3])], axis=-1)
      iou_predict_truth = self.calc_iou(predict_boxes_tran, boxes)
      # Calculate I tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]
      object_mask = tf.reduce_max(iou_predict_truth, 3, keep_dims=True)
      object_mask = tf.cast((iou_predict_truth &amp;gt;= object_mask), tf.float32) * response
      # Calculate no_I tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]
      noobject_mask = tf.ones_like(object_mask, dtype=tf.float32) - object_mask
      boxes_tran = tf.stack([boxes[..., 0] * self.cell_size - offset, boxes[..., 1] * self.cell_size - offset_tran, tf.sqrt(boxes[..., 2]), tf.sqrt(boxes[..., 3])], axis=-1)
      # Calculate each part in loss function
      # class_loss
      class_delta = response * (predict_classes - classes)
      class_loss = tf.reduce_mean(tf.reduce_sum(tf.square(class_delta), axis=[1, 2, 3]), name=&amp;#39;class_loss&amp;#39;) * self.class_scale
      # object_loss
      object_delta = object_mask * (predict_scales - iou_predict_truth)
      object_loss = tf.reduce_mean(tf.reduce_sum(tf.square(object_delta), axis=[1, 2, 3]), name=&amp;#39;object_loss&amp;#39;) * self.object_scale
      # noobject_loss
      noobject_delta = noobject_mask * predict_scales
      noobject_loss = tf.reduce_mean(tf.reduce_sum(tf.square(noobject_delta), axis=[1, 2, 3]), name=&amp;#39;noobject_loss&amp;#39;) * self.noobject_scale
      # coord_loss
      coord_mask = tf.expand_dims(object_mask, 4)
      boxes_delta = coord_mask * (predict_boxes - boxes_tran)
      coord_loss = tf.reduce_mean(tf.reduce_sum(tf.square(boxes_delta), axis=[1, 2, 3, 4]), name=&amp;#39;coord_loss&amp;#39;) * self.coord_scale
      # Add each part of loss function together
      tf.losses.add_loss(class_loss)
      tf.losses.add_loss(object_loss)
      tf.losses.add_loss(noobject_loss)
      tf.losses.add_loss(coord_loss)
      tf.summary.scalar(&amp;#39;class_loss&amp;#39;, class_loss)
      tf.summary.scalar(&amp;#39;object_loss&amp;#39;, object_loss)
      tf.summary.scalar(&amp;#39;noobject_loss&amp;#39;, noobject_loss)
      tf.summary.scalar(&amp;#39;coord_loss&amp;#39;, coord_loss)
      tf.summary.histogram(&amp;#39;boxes_delta_x&amp;#39;, boxes_delta[..., 0])
      tf.summary.histogram(&amp;#39;boxes_delta_y&amp;#39;, boxes_delta[..., 1])
      tf.summary.histogram(&amp;#39;boxes_delta_w&amp;#39;, boxes_delta[..., 2])
      tf.summary.histogram(&amp;#39;boxes_delta_h&amp;#39;, boxes_delta[..., 3])
      tf.summary.histogram(&amp;#39;iou&amp;#39;, iou_predict_truth)

  def leaky_relu(alpha):
    # 激活函数leaky_relu
    # if(x&amp;gt;0) then \phi(x)=x
    # otherwise \phi(x)=alpha*x
    # In this case,alpha=0.1
    def op(inputs):
      return tf.nn.leaky_relu(inputs, alpha=alpha, name=&amp;#39;leaky_relu&amp;#39;)
  return op&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;train.py&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;网络搭建后用train.py文件进行训练&lt;/strong&gt;&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;import os
import argparse
import datetime
import tensorflow as tf
import yolo.config as cfg
from yolo.yolo_net import YOLONet
from utils.timer import Timer
from utils.pascal_voc import pascal_voc
os.environ[&amp;#39;TF_CPP_MIN_LOG_LEVEL&amp;#39;] = &amp;#39;2&amp;#39;
import tensorflow.contrib.slim as slim
# slim = tf.contrib.slim

class Solver(object):
    def __init__(self, net, data):
      # tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate, staircase)函数:
      # initial_learning_rate表示初始设定学习率; global_step表示当前的学习步数; decay_rate表示衰减速率;
      # staircase若为True表明每decay_steps次计算学习率变化并更新原始学习率; False则每一步都更新
      # learning_rate = initial_learning_rate * (decay_rate) ^ (global_step / decay_steps)
      # 
      # tf.train.GradientDescentOptimizer(learning_rate)函数: 梯度下降优化器
      # 
      # slim.learning.create_train_op(loss, optimizer, ...)通过loss, optimizer等创建train_op, 用于训练
      # 
      # tf.GPUOptions(per_process_gpu_memory_fraction)作为可选配置参数的一部分来显示地指定需要分配的显存比例
      # per_process_gpu_memory_fraction指定了每个GPU进程中使用显存的上限, 但它只能均匀地作用于所有GPU, 无法对不同GPU设置不同的上限
      # 
      # sess.run(a)函数: 找到与a有数据依赖的节点, 然后顺序执行
      # 
      # 参数初始化
      self.net = net
      self.data = data
      self.weights_file = cfg.WEIGHTS_FILE
      self.max_iter = cfg.MAX_ITER
      self.initial_learning_rate = cfg.LEARNING_RATE
      self.decay_steps = cfg.DECAY_STEPS
      self.decay_rate = cfg.DECAY_RATE
      self.staircase = cfg.STAIRCASE
      self.summary_iter = cfg.SUMMARY_ITER
      self.save_iter = cfg.SAVE_ITER
      self.output_dir = os.path.join(cfg.OUTPUT_DIR, datetime.datetime.now().strftime(&amp;#39;%Y_%m_%d_%H_%M&amp;#39;))
      if not os.path.exists(self.output_dir):
        os.makedirs(self.output_dir)
      self.save_cfg()
      # tf.global_variables表示获取程序中的变量, 返回的值是变量的一个列表
      self.variable_to_restore = tf.global_variables()
      # tf.train.Saver表示Save and restore all the variables
      self.saver = tf.train.Saver(self.variable_to_restore, max_to_keep=None)
      self.ckpt_file = os.path.join(self.output_dir, &amp;#39;yolo&amp;#39;)
      # merge_all可以将所有summary全部保存到磁盘, 以便tensorboard显示
      self.summary_op = tf.summary.merge_all()
      # 定义一个写入summary的目标文件，dir为写入文件地址
      self.writer = tf.summary.FileWriter(self.output_dir, flush_secs=60)
      # tf.train.create_global_step()表示在图中创建全局步长张量
      self.global_step = tf.train.create_global_step()
      # 学习率的设定: 一开始使用较大学习率以快速得到较优解; 再通过较小学习率使模型训练后期更稳定
      self.learning_rate = tf.train.exponential_decay(self.initial_learning_rate, self.global_step, self.decay_steps, self.decay_rate, self.staircase, name=&amp;#39;learning_rate&amp;#39;)
      # 梯度下降
      self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)
      # 创建训练
      self.train_op = slim.learning.create_train_op(self.net.total_loss, self.optimizer, global_step=self.global_step)
      # 显存GPU设置
      gpu_options = tf.GPUOptions()
      config = tf.ConfigProto(gpu_options=gpu_options)
      # tf.Session()创建一个会话
      self.sess = tf.Session(config=config)
      self.sess.run(tf.global_variables_initializer())
      # 导入weights文件
      if self.weights_file is not None:
        print(&amp;#39;Restoring weights from: &amp;#39; + self.weights_file)
        # saver.restore(sess, ckpt.model_checkpoint_path)恢复变量
        self.saver.restore(self.sess, self.weights_file)
      self.writer.add_graph(self.sess.graph)

  def train(self):
    train_timer = Timer()
    load_timer = Timer()
    for step in range(1, self.max_iter + 1):
      # 记录加载数据时间 &amp;amp; 加载数据
      load_timer.tic()
      images, labels = self.data.get()
      load_timer.toc()
      feed_dict = {self.net.images: images, self.net.labels: labels}
      if step % self.summary_iter == 0:
        if step % (self.summary_iter * 10) == 0:
          # 记录训练时间
          train_timer.tic()
          summary_str, loss, _ = self.sess.run([self.summary_op, self.net.total_loss, self.train_op], feed_dict=feed_dict)
          train_timer.toc()
          # 打印输出相关数据
          log_str = &amp;quot;{} Epoch: {}, Step: {}, Learning rate: {}, Loss: {:5.3f}\nSpeed: {:.3f}s/iter, Load: {:.3f}s/iter, Remain: {}&amp;quot;.format(
            datetime.datetime.now().strftime(&amp;#39;%m-%d %H:%M:%S&amp;#39;),
            self.data.epoch,
            int(step),
            round(self.learning_rate.eval(session=self.sess), 6),
            loss,
            train_timer.average_time,
            load_timer.average_time,
            train_timer.remain(step, self.max_iter))
          print(log_str)
        else:
          train_timer.tic()
          summary_str, _ = self.sess.run([self.summary_op, self.train_op], feed_dict=feed_dict)
          train_timer.toc()
        self.writer.add_summary(summary_str, step)
      else:
        train_timer.tic()
        self.sess.run(self.train_op, feed_dict=feed_dict)
        train_timer.toc()
      if step % self.save_iter == 0:
        print(&amp;#39;{} Saving checkpoint file to: {}&amp;#39;.format(datetime.datetime.now().strftime(&amp;#39;%m-%d %H:%M:%S&amp;#39;), self.output_dir))
        # 保存ckpt模型文件
        self.saver.save(self.sess, self.ckpt_file, global_step=self.global_step)

  def save_cfg(self):
    with open(os.path.join(self.output_dir, &amp;#39;config.txt&amp;#39;), &amp;#39;w&amp;#39;) as f:
      cfg_dict = cfg.__dict__
      for key in sorted(cfg_dict.keys()):
        if key[0].isupper():
          cfg_str = &amp;#39;{}: {}\n&amp;#39;.format(key, cfg_dict[key])
          f.write(cfg_str)

  def update_config_paths(data_dir, weights_file):
    cfg.DATA_PATH = data_dir
    cfg.PASCAL_PATH = os.path.join(data_dir, &amp;#39;pascal_voc&amp;#39;)
    cfg.CACHE_PATH = os.path.join(cfg.PASCAL_PATH, &amp;#39;cache&amp;#39;)
    cfg.OUTPUT_DIR = os.path.join(cfg.PASCAL_PATH, &amp;#39;output&amp;#39;)
    cfg.WEIGHTS_DIR = os.path.join(cfg.PASCAL_PATH, &amp;#39;weights&amp;#39;)
    cfg.WEIGHTS_FILE = os.path.join(cfg.WEIGHTS_DIR, weights_file)

  def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(&amp;#39;--weights&amp;#39;, default=&amp;quot;YOLO_small.ckpt&amp;quot;, type=str)
    parser.add_argument(&amp;#39;--data_dir&amp;#39;, default=&amp;quot;data&amp;quot;, type=str)
    parser.add_argument(&amp;#39;--threshold&amp;#39;, default=0.2, type=float)
    parser.add_argument(&amp;#39;--iou_threshold&amp;#39;, default=0.5, type=float)
    parser.add_argument(&amp;#39;--gpu&amp;#39;, default=&amp;#39;&amp;#39;, type=str)
    args = parser.parse_args()
    if args.gpu is not None:
      cfg.GPU = args.gpu
    if args.data_dir != cfg.DATA_PATH:
        update_config_paths(args.data_dir, args.weights)
    os.environ[&amp;#39;CUDA_VISIBLE_DEVICES&amp;#39;] = cfg.GPU

    yolo = YOLONet()
    pascal = pascal_voc(&amp;#39;train&amp;#39;)

    solver = Solver(yolo, pascal)

    print(&amp;#39;Start training ...&amp;#39;)
    solver.train()
    print(&amp;#39;Done training.&amp;#39;)

if __name__ == &amp;#39;__main__&amp;#39;:
    # python train.py --weights YOLO_small.ckpt --gpu 0
    main()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;参照&lt;a href=&#34;https://github.com/hizhangp/yolo_tensorflow&#34;&gt;hizhangp&lt;/a&gt;的指引就可以完成训练操作~&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;检测&lt;/h3&gt;
&lt;p&gt;通过cmd或Anaconda Prompt运行&lt;code&gt;python test.py&lt;/code&gt;可以检测test文件夹里面的图片，这里我们直接用到了&lt;a href=&#34;https://github.com/hizhangp/yolo_tensorflow&#34;&gt;hizhangp&lt;/a&gt;训练完成的模型文件ckpt。假设我们对test文件中的cat进行检测，检测结果如图18所示。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;18.PNG&#34; width = &#34;800&#34; height = &#34;800&#34; alt=&#34;Figure18&#34; /&gt;
&lt;p&gt;
图18. cat图像检测结果
&lt;/div&gt;
&lt;div id=&#34;test.py&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;strong&gt;检测阶段的test.py文件&lt;/strong&gt;&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;import os
import cv2
import argparse
import numpy as np
import tensorflow as tf
import yolo.config as cfg
from yolo.yolo_net import YOLONet
from utils.timer import Timer
os.environ[&amp;#39;TF_CPP_MIN_LOG_LEVEL&amp;#39;] = &amp;#39;2&amp;#39;

class Detector(object):
  def __init__(self, net, weight_file):
    # 参数初始化
    self.net = net
    self.weights_file = weight_file
    self.classes = cfg.CLASSES
    self.num_class = len(self.classes)
    self.image_size = cfg.IMAGE_SIZE
    self.cell_size = cfg.CELL_SIZE
    self.boxes_per_cell = cfg.BOXES_PER_CELL
    self.threshold = cfg.THRESHOLD
    self.iou_threshold = cfg.IOU_THRESHOLD
    self.boundary1 = self.cell_size * self.cell_size * self.num_class
    self.boundary2 = self.boundary1 + self.cell_size * self.cell_size * self.boxes_per_cell
    # 创建一个会话
    self.sess = tf.Session()
    self.sess.run(tf.global_variables_initializer())
    print(&amp;#39;Restoring weights from: &amp;#39; + self.weights_file)
    self.saver = tf.train.Saver()
    self.saver.restore(self.sess, self.weights_file)

  def draw_result(self, img, result):
    # 将框和原图放在一起并绘制出来
    for i in range(len(result)):
      x = int(result[i][1])
      y = int(result[i][2])
      w = int(result[i][3] / 2)
      h = int(result[i][4] / 2)
      # cv2.rectangle是矩形框左上角和右下角的坐标
      # cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)绘制矩形
      # img表示原图; 第二个参数(x,y)表示矩形左上点坐标; 第三个参数(x+w,y+h)表示矩形右下点坐标;
      # (0,255,0)表示画线对应的rgb颜色(0 255 0 为青色); 2表示画线的宽度
      # (125,125,125)为灰色; -1表示填充图形
      cv2.rectangle(img, (x - w, y - h), (x + w, y + h), (0, 255, 0), 2)
      cv2.rectangle(img, (x - w, y - h - 20), (x + w, y - h), (125, 125, 125), -1)
      lineType = cv2.LINE_AA if cv2.__version__ &amp;gt; &amp;#39;3&amp;#39; else cv2.CV_AA
      cv2.putText(img, result[i][0] + &amp;#39; : %.2f&amp;#39; % result[i][5], (x - w + 5, y - h - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, lineType)

  def detect(self, img):
    img_h, img_w, _ = img.shape
    inputs = cv2.resize(img, (self.image_size, self.image_size))
    inputs = cv2.cvtColor(inputs, cv2.COLOR_BGR2RGB).astype(np.float32)
    inputs = (inputs / 255.0) * 2.0 - 1.0
    inputs = np.reshape(inputs, (1, self.image_size, self.image_size, 3))
    # 预测测试集
    result = self.detect_from_cvmat(inputs)[0]
    # 预测结果转换变换回原始图像中
    for i in range(len(result)):
      result[i][1] *= (1.0 * img_w / self.image_size)
      result[i][2] *= (1.0 * img_h / self.image_size)
      result[i][3] *= (1.0 * img_w / self.image_size)
      result[i][4] *= (1.0 * img_h / self.image_size)
    return result

  def detect_from_cvmat(self, inputs):
    # 预测测试集的结果
    net_output = self.sess.run(self.net.logits, feed_dict={self.net.images: inputs})
    results = []
    for i in range(net_output.shape[0]):
      results.append(self.interpret_output(net_output[i]))
    return results

  def interpret_output(self, output):
    probs = np.zeros((self.cell_size, self.cell_size, self.boxes_per_cell, self.num_class))
    class_probs = np.reshape(output[0:self.boundary1], (self.cell_size, self.cell_size, self.num_class))
    scales = np.reshape(output[self.boundary1:self.boundary2], (self.cell_size, self.cell_size, self.boxes_per_cell))
    boxes = np.reshape(output[self.boundary2:], (self.cell_size, self.cell_size, self.boxes_per_cell, 4))
    offset = np.array([np.arange(self.cell_size)] * self.cell_size * self.boxes_per_cell)
    offset = np.transpose(np.reshape(offset, [self.boxes_per_cell, self.cell_size, self.cell_size]), (1, 2, 0))

    boxes[:, :, :, 0] += offset
    boxes[:, :, :, 1] += np.transpose(offset, (1, 0, 2))
    boxes[:, :, :, :2] = 1.0 * boxes[:, :, :, 0:2] / self.cell_size
    boxes[:, :, :, 2:] = np.square(boxes[:, :, :, 2:])

    boxes *= self.image_size

    for i in range(self.boxes_per_cell):
      for j in range(self.num_class):
        probs[:, :, i, j] = np.multiply(class_probs[:, :, j], scales[:, :, i])

    filter_mat_probs = np.array(probs &amp;gt;= self.threshold, dtype=&amp;#39;bool&amp;#39;)
    filter_mat_boxes = np.nonzero(filter_mat_probs)
    boxes_filtered = boxes[filter_mat_boxes[0], filter_mat_boxes[1], filter_mat_boxes[2]]
    probs_filtered = probs[filter_mat_probs]
    classes_num_filtered = np.argmax(filter_mat_probs, axis=3)[filter_mat_boxes[0], filter_mat_boxes[1], filter_mat_boxes[2]]

    argsort = np.array(np.argsort(probs_filtered))[::-1]
    boxes_filtered = boxes_filtered[argsort]
    probs_filtered = probs_filtered[argsort]
    classes_num_filtered = classes_num_filtered[argsort]

    for i in range(len(boxes_filtered)):
      if probs_filtered[i] == 0:
        continue
      # 通过iou阈值去除多余Bounding Boxes
      for j in range(i + 1, len(boxes_filtered)):
        if self.iou(boxes_filtered[i], boxes_filtered[j]) &amp;gt; self.iou_threshold:
          probs_filtered[j] = 0.0

    filter_iou = np.array(probs_filtered &amp;gt; 0.0, dtype=&amp;#39;bool&amp;#39;)
    boxes_filtered = boxes_filtered[filter_iou]
    probs_filtered = probs_filtered[filter_iou]
    classes_num_filtered = classes_num_filtered[filter_iou]

    result = []
    for i in range(len(boxes_filtered)):
      result.append([self.classes[classes_num_filtered[i]], boxes_filtered[i][0], boxes_filtered[i][1], boxes_filtered[i][2], boxes_filtered[i][3], probs_filtered[i]])

    return result

  def iou(self, box1, box2):
    tb = min(box1[0] + 0.5 * box1[2], box2[0] + 0.5 * box2[2]) - max(box1[0] - 0.5 * box1[2], box2[0] - 0.5 * box2[2])
    lr = min(box1[1] + 0.5 * box1[3], box2[1] + 0.5 * box2[3]) - max(box1[1] - 0.5 * box1[3], box2[1] - 0.5 * box2[3])
    inter = 0 if tb &amp;lt; 0 or lr &amp;lt; 0 else tb * lr
    return inter / (box1[2] * box1[3] + box2[2] * box2[3] - inter)

  def camera_detector(self, cap, wait=10):
    # 摄像机检测器
    detect_timer = Timer()
    ret, _ = cap.read()

    while ret:
      ret, frame = cap.read()
      detect_timer.tic()
      result = self.detect(frame)
      detect_timer.toc()
      print(&amp;#39;Average detecting time: {:.3f}s&amp;#39;.format(detect_timer.average_time))

      self.draw_result(frame, result)
      cv2.imshow(&amp;#39;Camera&amp;#39;, frame)
      cv2.waitKey(wait)

      ret, frame = cap.read()

  def image_detector(self, imname, wait=0):
    # test文件夹中图像检测
    detect_timer = Timer()
    image = cv2.imread(imname)

    detect_timer.tic()
    result = self.detect(image)
    detect_timer.toc()
    print(&amp;#39;Average detecting time: {:.3f}s&amp;#39;.format(detect_timer.average_time))

    self.draw_result(image, result)
    cv2.imshow(&amp;#39;Image&amp;#39;, image)
    cv2.waitKey(wait)

  def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(&amp;#39;--weights&amp;#39;, default=&amp;quot;YOLO_small.ckpt&amp;quot;, type=str)
    parser.add_argument(&amp;#39;--weight_dir&amp;#39;, default=&amp;#39;weights&amp;#39;, type=str)
    parser.add_argument(&amp;#39;--data_dir&amp;#39;, default=&amp;quot;data&amp;quot;, type=str)
    parser.add_argument(&amp;#39;--gpu&amp;#39;, default=&amp;#39;&amp;#39;, type=str)
    args = parser.parse_args()
    os.environ[&amp;#39;CUDA_VISIBLE_DEVICES&amp;#39;] = args.gpu
    # Detection
    yolo = YOLONet(False)
    weight_file = os.path.join(args.data_dir, args.weight_dir, args.weights)
    detector = Detector(yolo, weight_file)
    # Detect from camera
    # cap = cv2.VideoCapture(-1)
    # detector.camera_detector(cap)
    # 
    # Detect from image file
    imname = &amp;#39;test/person.jpg&amp;#39;
    detector.image_detector(imname)

if __name__ == &amp;#39;__main__&amp;#39;:
    main()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;J. Redmon, S. Divvala, R. Girshick, A. Farhadi, “You Only Look Once: Unified Real-Time Object Detection”, IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788, 2016.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/av23354360?from=search&amp;amp;seid=13332955749650173893&#34;&gt;尤鱼哥. Yolo v1全面深度解读 目标检测论文&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/hizhangp/yolo_tensorflow&#34;&gt;hizhangp. Github yolo_tensorflow&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>TensorFlow&#43;Keras实现YOLOv3目标检测</title>
      <link>/post/tensorflow-keras%E5%AE%9E%E7%8E%B0yolov3%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</link>
      <pubDate>Wed, 20 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/tensorflow-keras%E5%AE%9E%E7%8E%B0yolov3%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</guid>
      <description>


&lt;div id=&#34;gitkeras-yolo3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;用Git下载keras-yolo3库&lt;/h2&gt;
&lt;p&gt;首先你电脑如果安装了Git，那在你希望存储的文件夹目录中打开Git Bash，接着输入一行命令就可以了~&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/qqwweee/keras-yolo3.git&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Git会很快帮你下载整个仓库(Repository)~&lt;/p&gt;
&lt;p&gt;那如果你没有安装Git也没关系，打开Github的&lt;a href=&#34;https://github.com/qqwweee/keras-yolo3&#34;&gt;keras-yolo3&lt;/a&gt;仓库，点击Clone or download后再点击Download ZIP，也是一样的。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pippython&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;用pip下Python库&lt;/h2&gt;
&lt;p&gt;Github上作者qqwweee已经把代码都完成了，所以我们只需要确保所需的库也安装了，就万事俱备，只欠东风！&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TensorFlow&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Keras&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;h5py&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;OpenCV&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通常安装库的方法会选用pip来下载安装，打开Anaconda prompt之后输入命令：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;activate python35 # 激活python3.5环境
pip install tensorflow
pip install keras
pip install h5py
pip install opencv-python&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;完成安装后就开始正题了！&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;yolo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;YOLO的实现&lt;/h2&gt;
&lt;div id=&#34;yolov3.weight&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;下载yolov3.weight和格式变换&lt;/h3&gt;
&lt;p&gt;我们可以直接从&lt;a href=&#34;https://pjreddie.com/darknet/yolo/&#34;&gt;YOLO官网&lt;/a&gt;可以下载预训练的权重yolov3.weight，也可以通过命令行进行下载；&lt;/p&gt;
&lt;p&gt;先打开Anaconda Prompt（或者cmd）并路径更新到当前keras-yolo3所在文件夹路径，通过下面命令行可完成对yolov3.weight的下载，并转成TensorFlow所支持的h5文件。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wget https://pjreddie.com/media/files/yolov3.weights
python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;demo&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;测试demo图像文件&lt;/h3&gt;
&lt;p&gt;如果想对某张图片进行检测，可以将这张图片放在当前keras-yolo3所在文件夹，再通过Anaconda Prompt运行下面命令&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python yolo_vedio.py --image&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;命令行运行后会弹出“Input image filename:”，输入指定图片文件即可。&lt;/p&gt;
&lt;p&gt;举个栗子~&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;kite.jpg&#34; width = &#34;600&#34; height = &#34;600&#34; alt=&#34;Figure1&#34; /&gt;
&lt;p&gt;
大家一起放风筝~
&lt;/div&gt;
&lt;p&gt;YOLOv3的Output效果不错！！&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;kiteoutput.PNG&#34; width = &#34;600&#34; height = &#34;600&#34; alt=&#34;Figure1&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Enjoy your YOLO now !!!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Recurrent Nerual Network 循环神经网络</title>
      <link>/post/recurrent-nerual-network-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/recurrent-nerual-network-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid>
      <description>


&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;循环神经网络（Recurrent Nerual Network, RNN）在Deep Learning领域中是一个经典有很重要的神经网络模型。RNN是在&lt;strong&gt;自然语言处理（Natural Language Processing, NLP）&lt;/strong&gt;领域最先被使用发展起来的。在NLP中通常会处理一些文字句子，比如我们想做一个机器翻译，把中文转换成英文。初中生可能在翻译的时候只会逐个中文词翻译成英文，但高中生就可能会对翻译中的词进行调整，结合前后的词汇，那我们也希望机器这样做。&lt;/p&gt;
&lt;p&gt;假设有一段对话：&lt;/p&gt;
&lt;p&gt;A：你好吗？&lt;br /&gt;
B：我很好。&lt;/p&gt;
&lt;p&gt;我们想让机器翻译成英文，对于A而言，机器可能在一些训练后很容易翻译成“How are you?”，但对于B可能就翻译成了“I’m ok.”。那就相当尴尬了~（想起某军的Are you ok?╭(●｀∀´●)╯）机器其实需要参考A问了什么，再来对B的回答进行翻译，才能获得比较好的回答翻译“I’m fine.”&lt;/p&gt;
&lt;p&gt;RNN就是专门解决了处理序列化数据的问题，像上面这样的小例子。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;简单循环神经网络&lt;/h2&gt;
对于简单RNN，它由输入层，一个隐藏层和一个输出层构成的，像这样子：
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;RNN1.PNG&#34; width = &#34;700&#34; height = &#34;600&#34; alt=&#34;Figure1&#34; /&gt;
&lt;p&gt;
图1. 简单RNN示意图
&lt;/div&gt;
&lt;p&gt;先看左边的图时会觉得不能理解，但是如果将其展开得到右边的图，就比较好理解了。在&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;时刻，&lt;span class=&#34;math inline&#34;&gt;\(x_t\)&lt;/span&gt;作为输入的同时，还有上一个时刻隐藏层的&lt;span class=&#34;math inline&#34;&gt;\(h_{t-1}\)&lt;/span&gt;，以&lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;为权重作为第&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;时刻的输入。我们用计算公式来表示，可以表示为： &lt;span class=&#34;math display&#34;&gt;\[
h_t = f (U x_t + V h_{t-1})\\
o_t = \sigma (W h_t)
\]&lt;/span&gt; 其中&lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;是激活函数（Activation function）。&lt;/p&gt;
&lt;p&gt;如果将上面两个式子联合可以得到： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
o_t &amp;amp;= \sigma (W h_t)\\
&amp;amp;= \sigma (W f (U x_t + V h_{t-1}))\\
&amp;amp;= \sigma (W f (U x_t + V f (U x_{t-1} + V h_{t-2})))\\
&amp;amp;=\ ...
\end{align}
\]&lt;/span&gt; 可以看到我们会得到一个循环向前的公式，循环神经网络的输出值&lt;span class=&#34;math inline&#34;&gt;\(o_t\)&lt;/span&gt;是受到前面历次输入值&lt;span class=&#34;math inline&#34;&gt;\(x_t, x_{t-1}, x_{t-2}, \dots\)&lt;/span&gt;的影响的。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rnn-&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;深度RNN &amp;amp; 梯度消失与爆炸&lt;/h2&gt;
在介绍完简单RNN后，我们会考虑添加更多的隐藏层，那就可以得到深度RNN（如图2）；在这样多个隐藏层的神经网络结构中，我们通过&lt;strong&gt;反向传播&lt;/strong&gt;就可以计算更新网络的参数。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;RNN2.PNG&#34; width = &#34;400&#34; height = &#34;500&#34; alt=&#34;Figure2&#34; /&gt;
&lt;p&gt;
图2. 深度RNN示意图
&lt;/div&gt;
&lt;p&gt;当神经网络中层数太多的话，可能就会出现&lt;strong&gt;梯度消失&lt;/strong&gt;。举个例子：&lt;/p&gt;
&lt;p&gt;假设我们用了激活函数sigmoid函数 &lt;span class=&#34;math display&#34;&gt;\[
f(x) = \frac{1}{1+e^{-x}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;它的导数为 &lt;span class=&#34;math display&#34;&gt;\[
f&amp;#39;(x) = f(x)(1-f(x))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;我们知道&lt;span class=&#34;math inline&#34;&gt;\(1 + e^{-x} &amp;gt; 1\)&lt;/span&gt;，所以可以得到&lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; f(x) &amp;lt; 1\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;对于导数&lt;span class=&#34;math inline&#34;&gt;\(f&amp;#39;(x)\)&lt;/span&gt;，我们可以通过&lt;span class=&#34;math inline&#34;&gt;\(-\frac{b}{2a}\)&lt;/span&gt;计算得到，当&lt;span class=&#34;math inline&#34;&gt;\(f(x) = 0.5\)&lt;/span&gt;时，&lt;span class=&#34;math inline&#34;&gt;\(f&amp;#39;(x)\)&lt;/span&gt;取得最大值为&lt;span class=&#34;math inline&#34;&gt;\(0.25\)&lt;/span&gt;。 &lt;img src=&#34;./post/2019-02-01-recurrent-nerual-network-循环神经网络_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;在反向传播计算过程中，每经过一个sigmoid函数，便需要乘以一个sigmoid函数的导数值，那残差就至少会被削减变为原来的0.25倍；如果整个循环神经网络的时间长度很长，那么可能残差传递到前面的某一时刻就基本为0了；这样会导致在往前的时刻中无法更新参数，这就是&lt;strong&gt;梯度消失&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;同理，如果导数的最小值是不小于1的，那么残差在传递中就会一次比一次大，如果整个循环神经网络的时间长度很长，那每次参数的更新就会非常剧烈，这就是&lt;strong&gt;梯度爆炸&lt;/strong&gt;。下面用数学推导表示梯度消失与梯度爆炸。&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;数学推导下的梯度消失与梯度爆炸&lt;/h3&gt;
&lt;p&gt;我们在前面的时候用计算公式表达了简单RNN： &lt;span class=&#34;math display&#34;&gt;\[
h_t = f (U x_t + V h_{t-1})\\
o_t = \sigma (W h_t)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果将公式写的更明细的话，可以表达为： &lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
h_1^t\\ h_2^t\\ .\\ .\\ h_n^t 
\end{bmatrix}
=
f\bigg(
\begin{bmatrix}
u_{11} &amp;amp; u_{12} &amp;amp; \cdots &amp;amp; u_{1m}\\
u_{21} &amp;amp; u_{22} &amp;amp; \cdots &amp;amp; u_{2m}\\
. &amp;amp; . &amp;amp; . &amp;amp; .\\
. &amp;amp; . &amp;amp; . &amp;amp; .\\
u_{n1} &amp;amp; u_{n2} &amp;amp; \cdots &amp;amp; u_{nm}\\
\end{bmatrix}
\begin{bmatrix}
x_1\\ x_2\\ .\\ .\\ x_m 
\end{bmatrix}
+
\begin{bmatrix}
v_{11} &amp;amp; v_{12} &amp;amp; \cdots &amp;amp; v_{1n}\\
v_{21} &amp;amp; v_{22} &amp;amp; \cdots &amp;amp; v_{2n}\\
. &amp;amp; . &amp;amp; . &amp;amp; .\\
. &amp;amp; . &amp;amp; . &amp;amp; .\\
v_{n1} &amp;amp; v_{n2} &amp;amp; \cdots &amp;amp; v_{nn}\\
\end{bmatrix}
\begin{bmatrix}
h_1^{t-1}\\ h_2^{t-1}\\ .\\ .\\ h_n^{t-1} 
\end{bmatrix}
\bigg)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;span class=&#34;math inline&#34;&gt;\(h_j^t\)&lt;/span&gt;表示向量&lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;的第&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;个元素在&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;时刻的值；&lt;span class=&#34;math inline&#34;&gt;\(u_ji\)&lt;/span&gt;表示输入层第&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;个神经元到循环层第&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;个神经元的权重；&lt;span class=&#34;math inline&#34;&gt;\(w_ij\)&lt;/span&gt;表示循环层第&lt;span class=&#34;math inline&#34;&gt;\(t-1\)&lt;/span&gt;时刻的第&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;个神经元到循环层第&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;个时刻的第&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;个神经元的权重。&lt;/p&gt;
&lt;p&gt;我们用向量&lt;span class=&#34;math inline&#34;&gt;\(net_t\)&lt;/span&gt;来表示神经元在&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;时刻的&lt;strong&gt;加权输入&lt;/strong&gt;： &lt;span class=&#34;math display&#34;&gt;\[
net_t = Ux_t + Vh_{t-1}\\
h_t = f(net_t)
\]&lt;/span&gt; 同样有： &lt;span class=&#34;math display&#34;&gt;\[
h_{t-1} = f(net_{t-1})
\]&lt;/span&gt; 因此： &lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial net_t}{\partial net_{t-1}} = \frac{\partial net_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial net_{t-1}}
\]&lt;/span&gt; 由&lt;span class=&#34;math inline&#34;&gt;\(net_t = Ux_t + Vh_{t-1}\)&lt;/span&gt;式子，右边等式的第一部分可以推导得到： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\frac{\partial net_t}{\partial h_{t-1}} &amp;amp; = \begin{bmatrix}
\frac{\partial net_1^t}{\partial h_1^{t-1}} &amp;amp; \frac{\partial net_1^t}{\partial h_2^{t-1}} &amp;amp; \cdots &amp;amp; \frac{\partial net_1^t}{\partial h_n^{t-1}}\\
\frac{\partial net_2^t}{\partial h_1^{t-1}} &amp;amp; \frac{\partial net_2^t}{\partial h_2^{t-1}} &amp;amp; \cdots &amp;amp; \frac{\partial net_2^t}{\partial h_n^{t-1}}\\
. &amp;amp; . &amp;amp; . &amp;amp; .\\
. &amp;amp; . &amp;amp; . &amp;amp; .\\
\frac{\partial net_n^t}{\partial h_1^{t-1}} &amp;amp; \frac{\partial net_n^t}{\partial h_2^{t-1}} &amp;amp; \cdots &amp;amp; \frac{\partial net_n^t}{\partial h_n^{t-1}}\\
\end{bmatrix}\\
&amp;amp;= \begin{bmatrix}
v_{11} &amp;amp; v_{12} &amp;amp; \cdots &amp;amp; v_{1n}\\
v_{21} &amp;amp; v_{22} &amp;amp; \cdots &amp;amp; v_{2n}\\
. &amp;amp; . &amp;amp; . &amp;amp; .\\
. &amp;amp; . &amp;amp; . &amp;amp; .\\
v_{n1} &amp;amp; v_{n2} &amp;amp; \cdots &amp;amp; v_{nn}\\
\end{bmatrix}\\
&amp;amp;= V
\end{align}
\]&lt;/span&gt; 同理由&lt;span class=&#34;math inline&#34;&gt;\(h_{t-1} = f(net_{t-1})\)&lt;/span&gt;式子，右边等式的第二部分可以推导得到： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\frac{\partial h_{t-1}}{\partial net_{t-1}} &amp;amp;= \begin{bmatrix}
\frac{\partial h_1^{t-1}}{\partial net_1^{t-1}} &amp;amp; \frac{\partial h_1^{t-1}}{\partial net_2^{t-1}} &amp;amp; \cdots &amp;amp; \frac{\partial h_1^{t-1}}{\partial net_n^{t-1}}\\
\frac{\partial h_2^{t-1}}{\partial net_1^{t-1}} &amp;amp; \frac{\partial h_2^{t-1}}{\partial net_2^{t-1}} &amp;amp; \cdots &amp;amp; \frac{\partial h_2^{t-1}}{\partial net_n^{t-1}}\\
. &amp;amp; . &amp;amp; . &amp;amp; .\\
. &amp;amp; . &amp;amp; . &amp;amp; .\\
\frac{\partial h_n^{t-1}}{\partial net_1^{t-1}} &amp;amp; \frac{\partial h_n^{t-1}}{\partial net_2^{t-1}} &amp;amp; \cdots &amp;amp; \frac{\partial h_n^{t-1}}{\partial net_n^{t-1}}\\
\end{bmatrix}\\
&amp;amp;= \begin{bmatrix}
f&amp;#39;(net_1^{t-1}) &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0\\
0 &amp;amp; f&amp;#39;(net_2^{t-1}) &amp;amp; \cdots &amp;amp; 0\\
. &amp;amp; . &amp;amp; . &amp;amp; .\\
. &amp;amp; . &amp;amp; . &amp;amp; .\\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; f&amp;#39;(net_n^{t-1})\\
\end{bmatrix}\\
&amp;amp;= diag[f&amp;#39;(net_{t-1})]
\end{align}
\]&lt;/span&gt; 因此我们可以得到： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\frac{\partial net_t}{\partial net_{t-1}} &amp;amp;= \frac{\partial net_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial net_{t-1}}\\
&amp;amp;= V diag[f&amp;#39;(net_{t-1})]\\
&amp;amp;= \begin{bmatrix}
v_{11}f&amp;#39;(net_1^{t-1}) &amp;amp; v_{12}f&amp;#39;(net_2^{t-1}) &amp;amp; \cdots &amp;amp; v_{1n}f&amp;#39;(net_n^{t-1})\\
v_{21}f&amp;#39;(net_1^{t-1}) &amp;amp; v_{22}f&amp;#39;(net_2^{t-1}) &amp;amp; \cdots &amp;amp; v_{2n}f&amp;#39;(net_n^{t-1})\\
. &amp;amp; . &amp;amp; . &amp;amp; .\\
. &amp;amp; . &amp;amp; . &amp;amp; .\\
v_{n1}f&amp;#39;(net_1^{t-1}) &amp;amp; v_{n2}f&amp;#39;(net_2^{t-1}) &amp;amp; \cdots &amp;amp; v_{nn}f&amp;#39;(net_n^{t-1})\\
\end{bmatrix}\\
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;我们可以求得任意时刻&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;的残差值&lt;span class=&#34;math inline&#34;&gt;\(\delta_k\)&lt;/span&gt;： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\delta_k &amp;amp;= \frac{\partial E}{\partial net_k}\\
&amp;amp;= \frac{\partial E}{\partial net_t}\frac{\partial net_t}{\partial net_k}\\
&amp;amp;= \frac{\partial E}{\partial net_t}\frac{\partial net_t}{\partial net_{t-1}}\frac{\partial net_{t-1}}{\partial net_{t-2}}\cdots\frac{\partial net_{k+1}}{\partial net_k}\\
&amp;amp;= \delta_t V diag[f&amp;#39;(net_{t-1})]V diag[f&amp;#39;(net_{t-2})]\cdots V diag[f&amp;#39;(net_k)]\\
&amp;amp;= \delta_t \prod_{i=k}^{t-1}V diag[f&amp;#39;(net_i)]
\end{align}
\]&lt;/span&gt; 从上面式子我们可以得到： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
&amp;amp; \delta_k = \delta_t \prod_{i=k}^{t-1}V diag[f&amp;#39;(net_i)]\\
\Rightarrow\ \ &amp;amp; ||\delta_k|| \leq ||\delta_t|| \prod_{i=k}^{t-1} ||V|| \cdot ||diag[f&amp;#39;(net_i)]||\\
\end{align}
\]&lt;/span&gt; 其中模可以看作对&lt;span class=&#34;math inline&#34;&gt;\(\delta_k\)&lt;/span&gt;中每一项值得大小的度量，假定&lt;span class=&#34;math inline&#34;&gt;\(\beta_f\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(\beta_W\)&lt;/span&gt;分别是&lt;span class=&#34;math inline&#34;&gt;\(||diag[f&amp;#39;(net_i)]||\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(||V||\)&lt;/span&gt;的上界，那么 &lt;span class=&#34;math display&#34;&gt;\[
||\delta_k|| \leq ||\delta_t|| (\beta_f \beta_W)^{t-k}
\]&lt;/span&gt; 显然，如果&lt;span class=&#34;math inline&#34;&gt;\(\beta_f \beta_W\)&lt;/span&gt;的乘积比1小或者比1大时，由于指数的缘故，那么残差就可能会变得很小几乎为0或者变得很大。前者就是&lt;strong&gt;梯度消失&lt;/strong&gt;，后者就是&lt;strong&gt;梯度爆炸&lt;/strong&gt;。所以，普通的RNN是无法处理长距离依赖的原因。经过科学家们的努力，终于在1997年，由Sepp Hochreiter和Jürgen Schmidhuber两位科学家提出了长短时间记忆（LSTM），解决了RNN这样的问题。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;long-short-term-memory-lstm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;长短时间记忆（Long Short Term Memory, LSTM）&lt;/h2&gt;
长短时间记忆网络在原来普通RNN的基础上再加了一个状态&lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;，用来保存长期的状态，这个状态也称为&lt;strong&gt;单元状态（cell state）&lt;/strong&gt;。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;RNN3.PNG&#34; width = &#34;700&#34; height = &#34;500&#34; alt=&#34;Figure3&#34; /&gt;
&lt;p&gt;
图3. 普通RNN和LSTM
&lt;/div&gt;
&lt;p&gt;LSTM中输入有三个：&lt;span class=&#34;math inline&#34;&gt;\(x_t\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(h_{t-1}\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(c_{t-1}\)&lt;/span&gt;；而输出有两个：&lt;span class=&#34;math inline&#34;&gt;\(h_t\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(c_t\)&lt;/span&gt;；另外，LSTM引入了“门”或者“开关”这样的概念，用来控制不同阶段的数据输入和输出，分别是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;遗忘门（Forget gate）&lt;/strong&gt;：控制单元状态&lt;span class=&#34;math inline&#34;&gt;\(c_{t-1}\)&lt;/span&gt;保留多少到&lt;span class=&#34;math inline&#34;&gt;\(c_t\)&lt;/span&gt;；它的计算公式为： &lt;span class=&#34;math display&#34;&gt;\[
f_t = \sigma(U^{(f)}x_t + V^{(f)}h_{t-1})
\]&lt;/span&gt; 其中，&lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;是激活函数（一般sigmoid函数）。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;RNN4.PNG&#34; width = &#34;700&#34; height = &#34;600&#34; alt=&#34;Figure4&#34; /&gt;
&lt;p&gt;
图4. 遗忘门示意图
&lt;/div&gt;
结合上一时刻单元状态&lt;span class=&#34;math inline&#34;&gt;\(c_{t-1}\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(f_t\)&lt;/span&gt;决定了保留多少上一时刻单元状态，所以用乘积计算，计算公式为： &lt;span class=&#34;math display&#34;&gt;\[
c_t = f_t\circ c_{t-1}
\]&lt;/span&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;RNN5.PNG&#34; width = &#34;700&#34; height = &#34;600&#34; alt=&#34;Figure5&#34; /&gt;
&lt;p&gt;
图5. 遗忘门结合上一时刻单元状态示意图
&lt;/div&gt;
&lt;p&gt;这里的&lt;span class=&#34;math inline&#34;&gt;\(c_t\)&lt;/span&gt;只是计算过程中一个值，还不是输出时的单元状态。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输入门（Input gate）&lt;/strong&gt;：控制输入&lt;span class=&#34;math inline&#34;&gt;\(x_t\)&lt;/span&gt;保存多少到&lt;span class=&#34;math inline&#34;&gt;\(c_t\)&lt;/span&gt;；它的计算公式为： &lt;span class=&#34;math display&#34;&gt;\[
i_t = \sigma(U^{(i)}x_t + V^{(i)}h_{t-1})\\
\tilde{c}_t = \tanh (U^{(c)}x_t + V^{(c)}h_{t-1})
\]&lt;/span&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;RNN6.PNG&#34; width = &#34;700&#34; height = &#34;600&#34; alt=&#34;Figure6&#34; /&gt;
&lt;p&gt;
图6. 输入门示意图&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
其中，我们把&lt;span class=&#34;math inline&#34;&gt;\(\tilde{c}_t\)&lt;/span&gt;称为“候选状态”，结合从遗忘门出来的&lt;span class=&#34;math inline&#34;&gt;\(c_t\)&lt;/span&gt;，我们将三者结合，计算公式为： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
c_t &amp;amp;= c_t + i_t\circ \tilde{c}_t\\
\Rightarrow c_t &amp;amp;= f_t\circ c_{t-1} + i_t\circ \tilde{c}_t
\end{align}
\]&lt;/span&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;RNN7.PNG&#34; width = &#34;700&#34; height = &#34;600&#34; alt=&#34;Figure7&#34; /&gt;
&lt;p&gt;
图7. 输入门结合候选状态和单元状态示意图
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输出门（Output gate）&lt;/strong&gt;：控制单元状态&lt;span class=&#34;math inline&#34;&gt;\(c_t\)&lt;/span&gt;输出多少到&lt;span class=&#34;math inline&#34;&gt;\(h_t\)&lt;/span&gt;；它的计算公式为： &lt;span class=&#34;math display&#34;&gt;\[
e_t = \sigma(U^{(e)}x_t + V^{(e)}h_{t-1})\\
h_t = \tanh (c_t)\circ e_t
\]&lt;/span&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;RNN8.PNG&#34; width = &#34;700&#34; height = &#34;600&#34; alt=&#34;Figure8&#34; /&gt;
&lt;p&gt;
图8. 输出门结合单元状态示意图
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面提及的参数在初始化设置为接近0的较小值，对于长短时间记忆网络的训练，让我们慢慢叙述~&lt;/p&gt;
&lt;div id=&#34;lstm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;LSTM的训练&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;残差值沿时间的反向传播计算&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在推导之前，我们先假定每个gate的激活函数为sigmoid函数，而其中我们还涉及到了tanh函数，它们的公式及导数分别为： &lt;span class=&#34;math display&#34;&gt;\[
\sigma(x) = \frac{1}{1+e^{-x}}\\
\sigma&amp;#39;(x) = \sigma(x)(1-\sigma(x))\\
\tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}\\
\tanh&amp;#39;(x) = 1-\tanh(x)^2
\]&lt;/span&gt; 显然，它们的导数均可以用它们本身进行表示，在计算过程中，我们只需要计算它们各自的值，就可以计算得到对应导数的值。&lt;/p&gt;
&lt;p&gt;另外，对于符号&lt;span class=&#34;math inline&#34;&gt;\(\circ\)&lt;/span&gt;，当作用于向量时，它的运算时对应元素的相乘，即： &lt;span class=&#34;math display&#34;&gt;\[
\text{a}\circ \text{b} = \begin{bmatrix}
a_1\\ a_2\\ a_3\\ \cdots\\ a_n
\end{bmatrix}
\circ
\begin{bmatrix}
b_1\\ b_2\\ b_3\\ \cdots\\ b_n
\end{bmatrix}
=
\begin{bmatrix}
a_1b_1\\ a_2b_2\\ a_3b_3\\ \cdots\\ a_nb_n
\end{bmatrix}
\]&lt;/span&gt; 当作用于向量和矩阵时，它的计算为： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\text{a}\ \circ \text{X} &amp;amp;= \begin{bmatrix}
a_1\\ a_2\\ a_3\\ \cdots\\ a_n
\end{bmatrix}
\circ
\begin{bmatrix}
x_{11} &amp;amp; x_{12} &amp;amp; x_{13} &amp;amp; \cdots &amp;amp; x_{1n}\\
x_{21} &amp;amp; x_{22} &amp;amp; x_{23} &amp;amp; \cdots &amp;amp; x_{2n}\\
x_{31} &amp;amp; x_{32} &amp;amp; x_{33} &amp;amp; \cdots &amp;amp; x_{3n}\\
 &amp;amp; &amp;amp; \cdots\\
x_{n1} &amp;amp; x_{n2} &amp;amp; x_{n3} &amp;amp; \cdots &amp;amp; x_{nn}
\end{bmatrix}\\
&amp;amp;=
\begin{bmatrix}
a_1x_{11} &amp;amp; a_1x_{12} &amp;amp; a_1x_{13} &amp;amp; \cdots &amp;amp; a_1x_{1n}\\
a_2x_{21} &amp;amp; a_2x_{22} &amp;amp; a_2x_{23} &amp;amp; \cdots &amp;amp; a_2x_{2n}\\
a_3x_{31} &amp;amp; a_3x_{32} &amp;amp; a_3x_{33} &amp;amp; \cdots &amp;amp; a_3x_{3n}\\
 &amp;amp; &amp;amp; \cdots\\
a_nx_{n1} &amp;amp; a_nx_{n2} &amp;amp; a_nx_{n3} &amp;amp; \cdots &amp;amp; a_nx_{nn}
\end{bmatrix}
\end{align}
\]&lt;/span&gt; 当作用于矩阵和矩阵时，两个矩阵的对应位置的元素相乘。&lt;/p&gt;
&lt;p&gt;同时，我们定义在&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;时刻，LSTM中的输出值为&lt;span class=&#34;math inline&#34;&gt;\(h_t\)&lt;/span&gt;，残差值&lt;span class=&#34;math inline&#34;&gt;\(\delta_t\)&lt;/span&gt;为&lt;span class=&#34;math inline&#34;&gt;\(\delta_t = \frac{\partial E}{\partial h_t}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;利用反向传播算法，我们需要得到的是前面时刻的残差值，假定现在我们先求上一时刻&lt;span class=&#34;math inline&#34;&gt;\(t-1\)&lt;/span&gt;时刻的残差值&lt;span class=&#34;math inline&#34;&gt;\(\delta_{t-1}\)&lt;/span&gt;。 &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\delta_{t-1} &amp;amp;= \frac{\partial E}{\partial h_{t-1}}\\
&amp;amp;= \frac{\partial E}{\partial h_t}\frac{\partial h_t}{\partial h_{t-1}}\\
&amp;amp;= \delta_t\frac{\partial h_t}{\partial h_{t-1}}
\end{align}
\]&lt;/span&gt; 由前面的推导，我们知道 &lt;span class=&#34;math display&#34;&gt;\[
h_t = \tanh(c_t)\circ e_t\\
c_t = f_t\circ c_{t-1} + i_t\circ \tilde{c}_t\\
f_t = \sigma(U^{(f)}x_t + V^{(f)}h_{t-1})\\
i_t = \sigma(U^{(i)}x_t + V^{(i)}h_{t-1})\\
\tilde{c}_t = \tanh (U^{(c)}x_t + V^{(c)}h_{t-1})\\
e_t = \sigma(U^{(e)}x_t + V^{(e)}h_{t-1})
\]&lt;/span&gt; 对于导数&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial h_t}{\partial h_{t-1}}\)&lt;/span&gt;，我们得到： &lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial h_t}{\partial h_{t-1}} = \frac{\partial h_t}{\partial e_t}\frac{\partial e_t}{\partial h_{t-1}} + \frac{\partial h_t}{\partial c_t}\frac{\partial c_t}{\partial f_t}\frac{\partial f_t}{\partial h_{t-1}} + \frac{\partial h_t}{\partial c_t}\frac{\partial c_t}{\partial i_t}\frac{\partial i_t}{\partial h_{t-1}} + \frac{\partial h_t}{\partial c_t}\frac{\partial c_t}{\partial \tilde{c}_t}\frac{\partial \tilde{c}_t}{\partial h_{t-1}}
\]&lt;/span&gt; 由于 &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\frac{\partial h_t}{\partial e_t} &amp;amp;= diag[\tanh(c_t)]\\
\frac{\partial e_t}{\partial h_{t-1}} &amp;amp;= diag[e_t\circ (1-e_t)]\ V^{(e)}\\
\frac{\partial h_t}{\partial c_t} &amp;amp;= diag[e_t\circ (1-\tanh(c_t)^2)]\\
\frac{\partial c_t}{\partial f_t} &amp;amp;= diag[c_{t-1}]\\
\frac{\partial c_t}{\partial i_t} &amp;amp;= diag[\tilde{c}_t]\\
\frac{\partial c_t}{\partial \tilde{c}_t} &amp;amp;= diag[i_t]\\
\frac{\partial f_t}{\partial h_{t-1}} &amp;amp;= diag[f_t\circ (1-f_t)]\ V^{(f)}\\
\frac{\partial i_t}{\partial h_{t-1}} &amp;amp;= diag[i_t\circ (1-i_t)]\ V^{(i)}\\
\frac{\partial \tilde{c}_t}{\partial h_{t-1}} &amp;amp;= diag[1-\tilde{c}_t^2]\ V^{(c)}
\end{align}
\]&lt;/span&gt; 因此， &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\frac{\partial h_t}{\partial h_{t-1}} = &amp;amp;\ diag[\tanh(c_t)] diag[e_t\circ (1-e_t)] V^{(e)}\\
&amp;amp;\ + diag[e_t\circ (1-\tanh(c_t)^2)] diag[c_{t-1}] diag[f_t\circ (1-f_t)] V^{(f)}\\
&amp;amp;\ + diag[e_t\circ (1-\tanh(c_t)^2)] diag[\tilde{c}_t] diag[i_t\circ (1-i_t)] V^{(i)}\\
&amp;amp;\ + diag[e_t\circ (1-\tanh(c_t)^2)] diag[i_t] diag[1-\tilde{c}_t^2] V^{(c)}\\
= &amp;amp;\ \tanh(c_t)\circ e_t\circ (1-e_t) V^{(e)}\\
&amp;amp;\ + e_t\circ (1-\tanh(c_t)^2)\circ c_{t-1}\circ f_t\circ (1-f_t) V^{(f)}\\
&amp;amp;\ + e_t\circ (1-\tanh(c_t)^2)\circ \tilde{c}_t\circ i_t\circ (1-i_t)\\
&amp;amp;\ + e_t\circ (1-\tanh(c_t)^2)\circ i_t\circ (1-\tilde{c}_t^2)
\end{align}
\]&lt;/span&gt; 可以求得&lt;span class=&#34;math inline&#34;&gt;\(t-1\)&lt;/span&gt;时刻的残差值： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\delta_{t-1} &amp;amp;= \delta_t\frac{\partial h_t}{\partial h_{t-1}}\\
&amp;amp;= \delta_t\circ \tanh(c_t)\circ e_t\circ (1-e_t) V^{(e)}\\
&amp;amp;\ + \delta_t\circ e_t\circ (1-\tanh(c_t)^2)\circ c_{t-1}\circ f_t\circ (1-f_t) V^{(f)}\\
&amp;amp;\ + \delta_t\circ e_t\circ (1-\tanh(c_t)^2)\circ \tilde{c}_t\circ i_t\circ (1-i_t)\\
&amp;amp;\ + \delta_t\circ e_t\circ (1-\tanh(c_t)^2)\circ i_t\circ (1-\tilde{c}_t^2)
\end{align}
\]&lt;/span&gt; 同理，利用 &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\delta_k &amp;amp;= \delta_t\frac{\partial h_t}{\partial h_k}\\
&amp;amp;= \delta_t\frac{\partial h_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial h_k}\\
&amp;amp;= \delta_t\frac{\partial h_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial h_{t-2}} \cdots \frac{\partial h_{k+1}}{\partial h_k}\\
\end{align}
\]&lt;/span&gt; 可以求得任意时刻&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;的残差值。&lt;/p&gt;
&lt;p&gt;对于残差值沿隐藏层的反向传播，其计算方法与之前提及的反向传播基本一致。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;实践出真知&lt;/h2&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import tensorflow as tf
import pandas as pd
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(&amp;quot;mnist/&amp;quot;)
# 训练参数
n_epoches = 100
batch_size = 150
Learning_rate = 0.001
# 网络参数，把28x28的图片数据拆成28行的时序数据喂进RNN
n_inputs = 28
n_steps = 28
n_hiddens = 150
n_outputs = 10  # 10分类
# 输入tensors
X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.int32, [None])
# 构建RNN结构
basic_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_hiddens, state_is_tuple=True)
# basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_hiddens)
# basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_hiddens)  # 另一种创建基本单元的方式
outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)
# 前向传播，定义损失函数、优化器
logits = tf.layers.dense(states[-1], n_outputs)  # 与states tensor连接的全连接层，LSTM时为states[-1]，即h张量
cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)
loss = tf.reduce_mean(cross_entropy)
optimizer = tf.train.AdamOptimizer(learning_rate=Learning_rate)
train_op = optimizer.minimize(loss)
prediction = tf.nn.in_top_k(logits, y, 1)
accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))  # cast函数将tensor转换为指定类型
# 从MNIST中读取数据
X_test = mnist.test.images.reshape([-1, n_steps, n_inputs])
y_test = mnist.test.labels
# 训练阶段
init = tf.global_variables_initializer()
loss_list = []
accuracy_list = []
with tf.Session() as sess:
    sess.run(init)
    n_batches = mnist.train.num_examples // batch_size  # 整除返回整数部分
    # print(&amp;quot;Batch_number: {}&amp;quot;.format(n_batches))
    for epoch in range(n_epoches):
        for iteration in range(n_batches):
            X_batch, y_batch = mnist.train.next_batch(batch_size)
            X_batch = X_batch.reshape([-1, n_steps, n_inputs])
            sess.run(train_op, feed_dict={X: X_batch, y: y_batch})
        loss_train = loss.eval(feed_dict={X: X_batch, y: y_batch})
        loss_list.append(loss_train)
        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})
        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})
        accuracy_list.append(acc_test)
        print(epoch, &amp;quot;Train accuracy: {:.3f}&amp;quot;.format(acc_train), &amp;quot;Test accuracy: {:.3f}&amp;quot;.format(acc_test))
# 导出损失和准确率，方便绘图
loss_readout = pd.DataFrame(loss_list)
loss_readout.to_csv(&amp;#39;csv/RNN_LSTM_loss.csv&amp;#39;)
acc_readout = pd.DataFrame(accuracy_list)
acc_readout.to_csv(&amp;#39;csv/RNN_LSTM_accuracy.csv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;运行结果可以绘制出loss和accuracy两图： &lt;img src=&#34;./post/2019-02-01-recurrent-nerual-network-循环神经网络_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;1.&lt;a href=&#34;https://zybuluo.com/hanbingtao/note/541458&#34;&gt;hanbingtao. 零基础入门深度学习(5) - 循环神经网络&lt;/a&gt;&lt;br /&gt;
2.&lt;a href=&#34;https://zybuluo.com/hanbingtao/note/581764&#34;&gt;hanbingtao. 零基础入门深度学习(6) - 长短时记忆网络(LSTM)&lt;/a&gt;&lt;br /&gt;
3.罗冬日. TensorFlow入门与实战&lt;br /&gt;
4.&lt;a href=&#34;https://zhuanlan.zhihu.com/p/55025354&#34;&gt;郑思座. 循环神经网络（RNN）以及简单TensorFlow实例&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>入门TeXLive&#43;TeXStudio</title>
      <link>/post/%E5%85%A5%E9%97%A8texlive-texstudio/</link>
      <pubDate>Thu, 24 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/%E5%85%A5%E9%97%A8texlive-texstudio/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://liam.page/&#34;&gt;Liam Huang&lt;/a&gt;曾说过：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;选择TeX Live，选择简单的人生；&lt;br /&gt;
选择MiKTeX，选择麻烦的人生；&lt;br /&gt;
选择CTeX套装，选择崩溃的人生。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;意外重新接触LaTeX之后，发现CTeX确实是个麻烦事。最开始接触LaTeX，就不断被LaTeX各种问题绊倒，可以说相当厌恶了╮(๑•́ ₃•̀๑)╭还好有友好的Markdown语法~&lt;/p&gt;
&lt;p&gt;删掉自己电脑中的CTeX套装，实践记录下如何开始TexLive+TexStudio的完美生活(*•̀ㅂ•́)و&lt;/p&gt;
&lt;div id=&#34;texlive-2018&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;安装TeXLive 2018&lt;/h2&gt;
&lt;p&gt;TeXLive的官网是&lt;a href=&#34;https://tug.org/texlive/&#34; class=&#34;uri&#34;&gt;https://tug.org/texlive/&lt;/a&gt;，按指引也可以完成下载；但我觉得用中国科技大学的镜像站会比较方便高效~所以从中科大这边下载： &lt;a href=&#34;https://mirrors.ustc.edu.cn/CTAN/systems/texlive/Images/texlive2018.iso&#34; class=&#34;uri&#34;&gt;https://mirrors.ustc.edu.cn/CTAN/systems/texlive/Images/texlive2018.iso&lt;/a&gt;&lt;/p&gt;
整个文件3.2GB还是相当大~完成下载后，以管理员身份运行&lt;strong&gt;install-tl-advanced.bat&lt;/strong&gt;，然后点击&lt;strong&gt;continue&lt;/strong&gt;（如图1所示），随后点击&lt;strong&gt;安装TeXLive&lt;/strong&gt;即可。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;TEX1.PNG&#34; width = &#34;600&#34; height = &#34;600&#34; alt=&#34;Figure1&#34; /&gt;
&lt;p&gt;
图1. 安装TeXLive页面
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;texstudio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;安装TexStudio&lt;/h2&gt;
&lt;p&gt;安装完TexLive后，可以说完成百分之80的工作了，TexStudio的安装相比前面的快很多（因为只有74.1MB(..•˘_˘•..)）。&lt;/p&gt;
&lt;p&gt;TexStudio的官网网址是： &lt;a href=&#34;http://texstudio.sourceforge.net/&#34; class=&#34;uri&#34;&gt;http://texstudio.sourceforge.net/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;点击网址打开后，对于Windows平台的，点击&lt;strong&gt;Download now&lt;/strong&gt;下载即可。&lt;/p&gt;
&lt;p&gt;两个安装后，就完成啦！ฅ(๑˙o˙๑)ฅ&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>卷积神经网络 Convolutional Neural Networks</title>
      <link>/post/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-convolutional-neural-networks/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-convolutional-neural-networks/</guid>
      <description>


&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;简介&lt;/h2&gt;
&lt;p&gt;Convolutional Neural Networks的中文名叫卷积神经网络，当然英文简称直接为CNN，它的创始人是著名的计算机科学家&lt;a href=&#34;http://yann.lecun.com/&#34;&gt;Yann LeCun&lt;/a&gt;，CNN和RNN（Recurrent Neural Networks）可以说是深度学习领域最常提及的两种网络模型。本篇博客参照知乎上问题&lt;a href=&#34;https://www.zhihu.com/question/52668301&#34;&gt;“CNN(卷积神经网络)是什么？有入门简介或文章吗？”&lt;/a&gt;的回答来进行介绍~&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;convolutional-neural-networks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Convolutional Neural Networks开始正文！&lt;/h2&gt;
&lt;div id=&#34;cnn&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;CNN的“卷积”介绍&lt;/h3&gt;
&lt;p&gt;我们假设神经网络的输入是一张彩色的图像，那通常我们输入的是&lt;span class=&#34;math inline&#34;&gt;\(n\times m\times 3\)&lt;/span&gt;的RGB图像，下面图中是&lt;span class=&#34;math inline&#34;&gt;\(4\times 4\times 3\)&lt;/span&gt;RGB图像的示例；其中的数字代表着图片的原始像素值。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;CNN1.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure1&#34; /&gt;
&lt;p&gt;
图1. &lt;span class=&#34;math inline&#34;&gt;\(4\times 4\times 3\)&lt;/span&gt;RGB图像
&lt;/div&gt;
&lt;p&gt;卷积核是CNN的一个重要部分，卷积则是CNN的一个重要步骤。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先，假设我们选取的卷积核为： &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
1 &amp;amp; 0 &amp;amp; 1\\
0 &amp;amp; 1 &amp;amp; 0\\
1 &amp;amp; 0 &amp;amp; 1
\end{vmatrix}
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们会从原始图像的左上角开始，选取和卷积核大小相同的区域。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过水平和垂直移动不断获得新的区域，我们假设移动的步长为1，重复上面的步骤，我们可以一个新的矩阵 &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
4 &amp;amp; 3 &amp;amp; 4\\
2 &amp;amp; 4 &amp;amp; 3\\
2 &amp;amp; 3 &amp;amp; 4
\end{vmatrix}
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;CNN2.gif&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure2&#34; /&gt;
&lt;p&gt;
图2. 卷积过程示例
&lt;/div&gt;
&lt;p&gt;由此，一个&lt;span class=&#34;math inline&#34;&gt;\(5\times 5\)&lt;/span&gt;的图像矩阵经过卷积得到了一个&lt;span class=&#34;math inline&#34;&gt;\(3\times 3\)&lt;/span&gt;的矩阵，生成的这个矩阵我们称为Convolved Feature或Feature Map。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;有时候为了不让新生成的图片缩小，可以给原始的图像进行0元素的填充（padding）。&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;CNN3.PNG&#34; width = &#34;200&#34; height = &#34;200&#34; alt=&#34;Figure3&#34; /&gt;
&lt;p&gt;
图3. 填充（padding）示意图
&lt;/div&gt;
&lt;p&gt;如图所示，原来的&lt;span class=&#34;math inline&#34;&gt;\(4\times 4\)&lt;/span&gt;矩阵如果通过&lt;span class=&#34;math inline&#34;&gt;\(3\times 3\)&lt;/span&gt;的卷积核处理会得到&lt;span class=&#34;math inline&#34;&gt;\(2\times 2\)&lt;/span&gt;的Convolved Feature，但是在周围加一圈0元素，从&lt;span class=&#34;math inline&#34;&gt;\(4\times 4\)&lt;/span&gt;矩阵扩充到&lt;span class=&#34;math inline&#34;&gt;\(5\times 5\)&lt;/span&gt;的矩阵的话，经过&lt;span class=&#34;math inline&#34;&gt;\(3\times 3\)&lt;/span&gt;的卷积核处理会得到&lt;span class=&#34;math inline&#34;&gt;\(4\times 4\)&lt;/span&gt;的Convolved Feature。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cnn&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;CNN的“池化”介绍&lt;/h3&gt;
&lt;p&gt;池化（pooling）实际上是对上个操作（卷积）得到的Convolved Feature进行降维，池化有很多方法（比如取最大值，求平均，求和等等），但常用的方法有“最大池化”（池化区域内所有值的平均值作为池化结果）和“平均池化”（池化区域内所有值的最大值作为池化结果）这两种。&lt;/p&gt;
&lt;p&gt;池化的具体操作是让一个池化窗口在图片上移动，每次取窗口内的平均值或者最大值；窗口的水平和垂直移动，它移动步长取窗口本身的大小；下图是最大池化的示意图。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;CNN4.PNG&#34; width = &#34;400&#34; height = &#34;400&#34; alt=&#34;Figure4&#34; /&gt;
&lt;p&gt;
图4. 最大池化示意图
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;为什么要池化层？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;假设输入了一张&lt;span class=&#34;math inline&#34;&gt;\(1000\times 1000\)&lt;/span&gt;的图像，我们采用100个卷积核进行提取不同的特征，其中卷积核大小为&lt;span class=&#34;math inline&#34;&gt;\(3\times 3\)&lt;/span&gt;，卷积核在原图像上移动的步长为1；若不考虑填充（padding），那么我们会得到一个&lt;span class=&#34;math inline&#34;&gt;\(998\times 998=996004\)&lt;/span&gt;的Convolved feature，那100个卷积核便会得到100个996004大小的Convolved feature；所以，一张图片最终会得到一个&lt;span class=&#34;math inline&#34;&gt;\(996004\times 100\)&lt;/span&gt;的卷积特征向量。这样上千万个特征进行图片的分类，很容易造成过拟合（overfitting）。&lt;/p&gt;
&lt;p&gt;由于图像具有空间相关性，即一个像素点和它周围的像素点在很大概率上是相似的。通过相邻的像素点进行合并（即池化），可以缩小最后得到的特征数量。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;反向传播和参数更新&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;CNN&lt;/strong&gt;的训练过程和&lt;strong&gt;全连接网络&lt;/strong&gt;的训练过程比较类似，都是先将参数随机初始化，再进行前向计算；得到最后的输出结果之后，再计算最后一层每个神经元的残差；通过&lt;strong&gt;反向传播&lt;/strong&gt;方法，可以得到所有节点的残差和损失函数对所有参数的偏导数；最后对参数进行更新。两者的架构区别主要在于卷积层和池化层。&lt;/p&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;卷积层的反向传播&lt;/h4&gt;
&lt;p&gt;我们先定义，&lt;/p&gt;
&lt;p&gt;卷积之前的矩阵： &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
x_{00} &amp;amp; x_{01} &amp;amp; x_{02}\\
x_{10} &amp;amp; x_{11} &amp;amp; x_{12}\\
x_{20} &amp;amp; x_{21} &amp;amp; x_{22}
\end{vmatrix}
\]&lt;/span&gt; 卷积核的矩阵： &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
k_{00} &amp;amp; k_{01}\\
k_{10} &amp;amp; k_{11}
\end{vmatrix}
\]&lt;/span&gt; 卷积之后的矩阵： &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
y_{00} &amp;amp; y_{01}\\
y_{10} &amp;amp; y_{11}
\end{vmatrix}
\]&lt;/span&gt; 卷积之后的残差矩阵： &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
\delta^{l+1}_{00} &amp;amp; \delta^{l+1}_{01}\\
\delta^{l+1}_{10} &amp;amp; \delta^{l+1}_{11}
\end{vmatrix}
\]&lt;/span&gt; 卷积之前的残差矩阵： &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
\delta^l_{00} &amp;amp; \delta^l_{01} &amp;amp; \delta^l_{02}\\
\delta^l_{10} &amp;amp; \delta^l_{11} &amp;amp; \delta^l_{12}\\
\delta^l_{20} &amp;amp; \delta^l_{21} &amp;amp; \delta^l_{22}
\end{vmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;我们在前向计算中，卷积操作的计算过程如下： &lt;span class=&#34;math display&#34;&gt;\[
y_{00} = x_{00}\times k_{00} + x_{01}\times k_{01} + x_{10}\times k_{10} + x_{11}\times k_{11}\\
y_{01} = x_{01}\times k_{00} + x_{02}\times k_{01} + x_{11}\times k_{10} + x_{12}\times k_{11}\\
y_{10} = x_{10}\times k_{00} + x_{11}\times k_{01} + x_{20}\times k_{10} + x_{21}\times k_{11}\\
y_{11} = x_{11}\times k_{00} + x_{12}\times k_{01} + x_{21}\times k_{10} + x_{22}\times k_{11}\\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;又卷积之后的残差可以由反向传播计算得到，我们可以得到： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\delta^l_{00} &amp;amp;= \frac{\partial L}{\partial x_{00}} = \frac{\partial L}{\partial y_{00}}\times \frac{\partial y_{00}}{\partial x_{00}}\\
&amp;amp;=\ \delta^{l+1}_{00}\times k_{00}\\
\delta^l_{01} &amp;amp;= \frac{\partial L}{\partial x_{01}} = \frac{\partial L}{\partial y_{00}}\times \frac{\partial y_{00}}{\partial x_{01}} + \frac{\partial L}{\partial y_{01}}\times \frac{\partial y_{01}}{\partial x_{01}}\\
&amp;amp;=\ \delta^{l+1}_{00}\times k_{01} + \delta^{l+1}_{01}\times k_{00}\\
\delta^l_{02} &amp;amp;= \frac{\partial L}{\partial x_{02}} = \frac{\partial L}{\partial y_{01}}\times \frac{\partial y_{01}}{\partial x_{02}}\\
&amp;amp;=\ \delta^{l+1}_{01}\times k_{01}\\
\delta^l_{10} &amp;amp;= \frac{\partial L}{\partial x_{10}} = \frac{\partial L}{\partial y_{00}}\times \frac{\partial y_{00}}{\partial x_{10}} + \frac{\partial L}{\partial y_{10}}\times \frac{\partial y_{10}}{\partial x_{10}}\\
&amp;amp;=\ \delta^{l+1}_{00}\times k_{10} + \delta^{l+1}_{10}\times k_{00}\\
\delta^l_{11} &amp;amp;= \frac{\partial L}{\partial x_{11}} = \frac{\partial L}{\partial y_{00}}\times \frac{\partial y_{00}}{\partial x_{11}} + \frac{\partial L}{\partial y_{01}}\times \frac{\partial y_{01}}{\partial x_{11}} + \frac{\partial L}{\partial y_{10}}\times \frac{\partial y_{10}}{\partial x_{11}} + \frac{\partial L}{\partial y_{11}}\times \frac{\partial y_{11}}{\partial x_{11}}\\
&amp;amp;=\ \delta^{l+1}_{00}\times k_{11} + \delta^{l+1}_{01}\times k_{10} + \delta^{l+1}_{10}\times k_{01} + \delta^{l+1}_{11}\times k_{00}\\
\delta^l_{12} &amp;amp;= \frac{\partial L}{\partial x_{12}} = \frac{\partial L}{\partial y_{01}}\times \frac{\partial y_{01}}{\partial x_{12}} + \frac{\partial L}{\partial y_{11}}\times \frac{\partial y_{11}}{\partial x_{12}}\\
&amp;amp;=\ \delta^{l+1}_{01}\times k_{11} + \delta^{l+1}_{11}\times k_{01}\\
\delta^l_{20} &amp;amp;= \frac{\partial L}{\partial x_{20}} = \frac{\partial L}{\partial y_{10}}\times \frac{\partial y_{10}}{\partial x_{20}}\\
&amp;amp;=\ \delta^{l+1}_{10}\times k_{10}\\
\delta^l_{21} &amp;amp;= \frac{\partial L}{\partial x_{21}} = \frac{\partial L}{\partial y_{10}}\times \frac{\partial y_{10}}{\partial x_{21}} + \frac{\partial L}{\partial y_{11}}\times \frac{\partial y_{11}}{\partial x_{21}}\\
&amp;amp;=\ \delta^{l+1}_{10}\times k_{11} + \delta^{l+1}_{11}\times k_{10}\\
\delta^l_{22} &amp;amp;= \frac{\partial L}{\partial x_{22}} = \frac{\partial L}{\partial y_{11}}\times \frac{\partial y_{11}}{\partial x_{22}}\\
&amp;amp;=\ \delta^{l+1}_{11}\times k_{11}
\end{align}
\]&lt;/span&gt; 通过上述等式，我们可以将残差通过卷积层反向得到上一层的残差，接下来通过残差推导损失函数对卷积核中参数的偏导数。 &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\frac{\partial L}{\partial k_{00}} &amp;amp;= \frac{\partial L}{\partial y_{00}}\times \frac{\partial y_{00}}{\partial k_{00}} + \frac{\partial L}{\partial y_{01}}\times \frac{\partial y_{01}}{\partial k_{00}} + \frac{\partial L}{\partial y_{10}}\times \frac{\partial y_{10}}{\partial k_{00}} + \frac{\partial L}{\partial y_{11}}\times \frac{\partial y_{11}}{\partial k_{00}}\\
&amp;amp;=\ \delta^{l+1}_{00}\times x_{00} + \delta^{l+1}_{01}\times x_{01} + \delta^{l+1}_{10}\times x_{10} + \delta^{l+1}_{11}\times x_{11}\\
\frac{\partial L}{\partial k_{01}} &amp;amp;= \frac{\partial L}{\partial y_{00}}\times \frac{\partial y_{00}}{\partial k_{01}} + \frac{\partial L}{\partial y_{01}}\times \frac{\partial y_{01}}{\partial k_{01}} + \frac{\partial L}{\partial y_{10}}\times \frac{\partial y_{10}}{\partial k_{01}} + \frac{\partial L}{\partial y_{11}}\times \frac{\partial y_{11}}{\partial k_{01}}\\
&amp;amp;=\ \delta^{l+1}_{00}\times x_{01} + \delta^{l+1}_{02}\times x_{01} + \delta^{l+1}_{10}\times x_{11} + \delta^{l+1}_{11}\times x_{12}\\
\frac{\partial L}{\partial k_{10}} &amp;amp;= \frac{\partial L}{\partial y_{00}}\times \frac{\partial y_{00}}{\partial k_{10}} + \frac{\partial L}{\partial y_{01}}\times \frac{\partial y_{01}}{\partial k_{10}} + \frac{\partial L}{\partial y_{10}}\times \frac{\partial y_{10}}{\partial k_{10}} + \frac{\partial L}{\partial y_{11}}\times \frac{\partial y_{11}}{\partial k_{10}}\\
&amp;amp;=\ \delta^{l+1}_{00}\times x_{010} + \delta^{l+1}_{01}\times x_{11} + \delta^{l+1}_{10}\times x_{20} + \delta^{l+1}_{11}\times x_{21}\\
\frac{\partial L}{\partial k_{11}} &amp;amp;= \frac{\partial L}{\partial y_{00}}\times \frac{\partial y_{00}}{\partial k_{11}} + \frac{\partial L}{\partial y_{01}}\times \frac{\partial y_{01}}{\partial k_{11}} + \frac{\partial L}{\partial y_{10}}\times \frac{\partial y_{10}}{\partial k_{11}} + \frac{\partial L}{\partial y_{11}}\times \frac{\partial y_{11}}{\partial k_{11}}\\
&amp;amp;=\ \delta^{l+1}_{00}\times x_{11} + \delta^{l+1}_{01}\times x_{12} + \delta^{l+1}_{10}\times x_{21} + \delta^{l+1}_{11}\times x_{22}\\
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;举个实例体会下！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;输入的数据是&lt;span class=&#34;math inline&#34;&gt;\(3\times 3\)&lt;/span&gt;的矩阵： &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
1 &amp;amp; 2 &amp;amp; 1\\
3 &amp;amp; 2 &amp;amp; 1\\
2 &amp;amp; 1 &amp;amp; 1
\end{vmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;假设有两个卷积核分别为： &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
0.1 &amp;amp; 0.2\\
0.2 &amp;amp; 0.4
\end{vmatrix}
和
\begin{vmatrix}
-0.3 &amp;amp; 0.1\\
0.1 &amp;amp; 0.2
\end{vmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;假设经过两个卷积核卷积之后的残差值分别为： &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
1 &amp;amp; 3\\
2 &amp;amp; 2
\end{vmatrix}
和
\begin{vmatrix}
2 &amp;amp; 1\\
1 &amp;amp; 1
\end{vmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;那么，第一个卷积核卷积之前各个节点的残差为： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\delta^l_{00} &amp;amp;= \delta^{l+1}_{00}\times k_{00}\\
&amp;amp;=\ 1\times 0.1 = 0.1\\
\delta^l_{01} &amp;amp;= \delta^{l+1}_{00}\times k_{01} + \delta^{l+1}_{01}\times k_{00}\\
&amp;amp;=\ 1\times 0.2 + 3\times  0.1 = 0.5\\
\delta^l_{02} &amp;amp;= \delta^{l+1}_{01}\times k_{01}\\
&amp;amp;=\ 3\times 0.2 = 0.6\\
\delta^l_{10} &amp;amp;= \delta^{l+1}_{00}\times k_{10} + \delta^{l+1}_{10}\times k_{00}\\
&amp;amp;=\ 1\times 0.2 + 2\times 0.1 = 0.4\\
\delta^l_{11} &amp;amp;= \delta^{l+1}_{00}\times k_{11} + \delta^{l+1}_{01}\times k_{10} + \delta^{l+1}_{10}\times k_{01} + \delta^{l+1}_{11}\times k_{00}\\
&amp;amp;=\ 1\times 0.4 + 3\times 0.2 + 2\times 0.2 + 2\times 0.1 = 1.6\\
\delta^l_{12} &amp;amp;= \delta^{l+1}_{01}\times k_{11} + \delta^{l+1}_{11}\times k_{01}\\
&amp;amp;=\ 3\times 0.4 + 2\times 0.2 = 1.6\\
\delta^l_{20} &amp;amp;= \delta^{l+1}_{10}\times k_{10}\\
&amp;amp;=\ 2\times 0.2 = 0.4\\
\delta^l_{21} &amp;amp;= \delta^{l+1}_{10}\times k_{11} + \delta^{l+1}_{11}\times k_{10}\\
&amp;amp;=\ 2\times 0.4 + 2\times 0.2 = 1.2\\
\delta^l_{22} &amp;amp;= \delta^{l+1}_{11}\times k_{11}\\
&amp;amp;=\ 2\times 0.4 = 0.8
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;即第一个卷积核反向传播计算过程中卷积之前的残差为： &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
0.1 &amp;amp; 0.5 &amp;amp; 0.6\\
0.4 &amp;amp; 1.6 &amp;amp; 1.6\\
0.4 &amp;amp; 1.2 &amp;amp; 0.8
\end{vmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;同理可以得到第二个卷积核反向传播计算过程中卷积之前的残差为： &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
-0.6 &amp;amp; -0.1 &amp;amp; 0.1\\
-0.1 &amp;amp; 0.3 &amp;amp; 0.3\\
0.1 &amp;amp; 0.3 &amp;amp; 0.2
\end{vmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;对于第一个卷积核，通过上面推导的公式可以计算： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\frac{\partial L}{\partial k_{00}} &amp;amp;= \delta^{l+1}_{00}\times x_{00} + \delta^{l+1}_{01}\times x_{01} + \delta^{l+1}_{10}\times x_{10} + \delta^{l+1}_{11}\times x_{11}\\
&amp;amp;=\ 1\times 1 + 3\times 2 + 2\times 3 + 2\times 2 = 17\\
\frac{\partial L}{\partial k_{01}} &amp;amp;= \delta^{l+1}_{00}\times x_{01} + \delta^{l+1}_{02}\times x_{01} + \delta^{l+1}_{10}\times x_{11} + \delta^{l+1}_{11}\times x_{12}\\
&amp;amp;=\ 1\times 2 + 3\times 1 + 2\times 2 + 2\times 1 = 11\\
\frac{\partial L}{\partial k_{10}} &amp;amp;= \delta^{l+1}_{00}\times x_{010} + \delta^{l+1}_{01}\times x_{11} + \delta^{l+1}_{10}\times x_{20} + \delta^{l+1}_{11}\times x_{21}\\
&amp;amp;=\ 1\times 3 + 3\times 2 + 2\times 2 + 2\times 1 = 15\\
\frac{\partial L}{\partial k_{11}} &amp;amp;= \delta^{l+1}_{00}\times x_{11} + \delta^{l+1}_{01}\times x_{12} + \delta^{l+1}_{10}\times x_{21} + \delta^{l+1}_{11}\times x_{22}\\
&amp;amp;=\ 1\times 2 + 3\times 1 + 2\times 1 + 2\times 1 = 9
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;则第一个卷积核的更新计算为： &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
k&amp;#39;_{00} &amp;amp; k&amp;#39;_{01}\\
k&amp;#39;_{10} &amp;amp; k&amp;#39;_{11}
\end{vmatrix}
=
\begin{vmatrix}
0.1 &amp;amp; 0.2\\
0.2 &amp;amp; 0.4
\end{vmatrix}
-\alpha\times 
\begin{vmatrix}
17 &amp;amp; 11\\
15 &amp;amp; 9
\end{vmatrix}
\]&lt;/span&gt; 其中&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;为学习率，&lt;span class=&#34;math inline&#34;&gt;\(k&amp;#39;_{ij}\)&lt;/span&gt;为更新之后的第一个卷积核的参数。同理，可更新第二个卷积核的参数&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level4&#34;&gt;
&lt;h4&gt;池化层的反向传播&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;平均池化&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;假设输入的是一个&lt;span class=&#34;math inline&#34;&gt;\(4\times 4\)&lt;/span&gt;的矩阵，池化区域是&lt;span class=&#34;math inline&#34;&gt;\(2\times 2\)&lt;/span&gt;的矩阵，经过池化后得到的是&lt;span class=&#34;math inline&#34;&gt;\(2\times 2\)&lt;/span&gt;的矩阵。我们假设在反向传播计算过程中，最后一层4个节点的残差值为： &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
1 &amp;amp; 3\\
2 &amp;amp; 4
\end{vmatrix}
\]&lt;/span&gt; 那么由于一个节点对应池化之前的4个节点，同时需要满足反向传播过程中各层的残差总和不变，所以池化之前的神经元的残差值是池化之后的残差值得平均；在这个例子中，池化之前&lt;span class=&#34;math inline&#34;&gt;\(4\times 4\)&lt;/span&gt;的神经元的残差值为： &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
0.25 &amp;amp; 0.25 &amp;amp; 0.75 &amp;amp; 0.75\\
0.25 &amp;amp; 0.25 &amp;amp; 0.75 &amp;amp; 0.75\\
0.5 &amp;amp; 0.5 &amp;amp; 1 &amp;amp; 1\\
0.5 &amp;amp; 0.5 &amp;amp; 1 &amp;amp; 1
\end{vmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最大池化&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在这里也用和&lt;strong&gt;平均池化&lt;/strong&gt;一样的例子，我们假设在反向传播计算过程中，最后一层4个节点的残差值为： &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
1 &amp;amp; 3\\
2 &amp;amp; 4
\end{vmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;最大池化在前向计算的过程中，需要记录被池化的&lt;span class=&#34;math inline&#34;&gt;\(2\times 2\)&lt;/span&gt;区域中哪个位置被选取（即最大值），我们假设被选中的最大值所在的位置就是下面星星所在位置： &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
* &amp;amp; - &amp;amp; - &amp;amp; -\\
- &amp;amp; - &amp;amp; * &amp;amp; -\\
- &amp;amp; - &amp;amp; - &amp;amp; *\\
- &amp;amp; * &amp;amp; - &amp;amp; -
\end{vmatrix}
\]&lt;/span&gt; 在反向传播中，将残差直接给上述星星位置，其他位置则赋为0，即 &lt;span class=&#34;math display&#34;&gt;\[
\begin{vmatrix}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 2 &amp;amp; 0\\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 4\\
0 &amp;amp; 3 &amp;amp; 0 &amp;amp; 0
\end{vmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;代码与实现&lt;/h3&gt;
&lt;p&gt;在知乎&lt;a href=&#34;https://www.zhihu.com/question/52668301&#34;&gt;“CNN(卷积神经网络)是什么？有入门简介或文章吗？”&lt;/a&gt;这个问题上，&lt;a href=&#34;https://www.zhihu.com/question/52668301/answer/536176496&#34;&gt;“阿里云云栖社区”&lt;/a&gt;在它的答案中给出了CNN实现的代码ヾ(&lt;em&gt;´▽‘&lt;/em&gt;)ﾉ&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Import the deep learning library
import tensorflow as tf
import time
# Import the MNIST dataset
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(&amp;quot;./mnist&amp;quot;, one_hot=True)
# Network inputs and outputs
# The network&amp;#39;s input is a 28×28 dimensional input
n = 28
m = 28
num_input = n * m # MNIST data input 
num_classes = 10 # MNIST total classes (0-9 digits)
# tf Graph input
X = tf.placeholder(tf.float32, [None, num_input])
Y = tf.placeholder(tf.float32, [None, num_classes])
# Storing the parameters of our LeNET-5 inspired Convolutional Neural Network
weights = {
   &amp;quot;W_ij&amp;quot;: tf.Variable(tf.random_normal([5, 5, 1, 32])),
   &amp;quot;W_jk&amp;quot;: tf.Variable(tf.random_normal([5, 5, 32, 64])),
   &amp;quot;W_kl&amp;quot;: tf.Variable(tf.random_normal([7 * 7 * 64, 1024])),
   &amp;quot;W_lm&amp;quot;: tf.Variable(tf.random_normal([1024, num_classes]))
    }
biases = {
   &amp;quot;b_ij&amp;quot;: tf.Variable(tf.random_normal([32])),
   &amp;quot;b_jk&amp;quot;: tf.Variable(tf.random_normal([64])),
   &amp;quot;b_kl&amp;quot;: tf.Variable(tf.random_normal([1024])),
   &amp;quot;b_lm&amp;quot;: tf.Variable(tf.random_normal([num_classes]))
    }
# The hyper-parameters of our Convolutional Neural Network
learning_rate = 1e-3
num_steps = 500
batch_size = 128
display_step = 10
def ConvolutionLayer(x, W, b, strides=1):
    # Convolution Layer
    # &amp;#39;SAME&amp;#39; in padding parameter represents that the size of pics will not change
    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=&amp;#39;SAME&amp;#39;)
    x = tf.nn.bias_add(x, b)
    return x
def ReLU(x):
    # ReLU activation function
    return tf.nn.relu(x)
def PoolingLayer(x, k=2, strides=2):
    # Max Pooling layer
    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, strides, strides, 1],
                          padding=&amp;#39;SAME&amp;#39;)
def Softmax(x):
    # Softmax activation function for the CNN&amp;#39;s final output
    return tf.nn.softmax(x)
# Create model
def ConvolutionalNeuralNetwork(x, weights, biases):
    # MNIST data input is a 1-D row vector of 784 features (28×28 pixels)
    # Reshape to match picture format [Height x Width x Channel]
    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]
    x = tf.reshape(x, shape=[-1, 28, 28, 1])
    # Convolution Layer
    Conv1 = ConvolutionLayer(x, weights[&amp;quot;W_ij&amp;quot;], biases[&amp;quot;b_ij&amp;quot;])
    # Non-Linearity
    ReLU1 = ReLU(Conv1)
    # Max Pooling (down-sampling)
    Pool1 = PoolingLayer(ReLU1, k=2)
    # Convolution Layer
    Conv2 = ConvolutionLayer(Pool1, weights[&amp;quot;W_jk&amp;quot;], biases[&amp;quot;b_jk&amp;quot;])
    # Non-Linearity
    ReLU2 = ReLU(Conv2)
    # Max Pooling (down-sampling)
    Pool2 = PoolingLayer(ReLU2, k=2)
    # Fully connected layer
    # Reshape conv2 output to fit fully connected layer input
    FC = tf.reshape(Pool2, [-1, weights[&amp;quot;W_kl&amp;quot;].get_shape().as_list()[0]])
    FC = tf.add(tf.matmul(FC, weights[&amp;quot;W_kl&amp;quot;]), biases[&amp;quot;b_kl&amp;quot;])
    FC = ReLU(FC)
    # Output, class prediction
    output = tf.add(tf.matmul(FC, weights[&amp;quot;W_lm&amp;quot;]), biases[&amp;quot;b_lm&amp;quot;])
    return output
# Construct model
logits = ConvolutionalNeuralNetwork(X, weights, biases)
prediction = Softmax(logits)
# Softamx cross entropy loss function
loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
    logits=logits, labels=Y))
# Optimization using the Adam Gradient Descent optimizer
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_process = optimizer.minimize(loss_function)
# Evaluate model
correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
# recording how the loss functio varies over time during training
cost = tf.summary.scalar(&amp;quot;cost&amp;quot;, loss_function)
training_accuracy = tf.summary.scalar(&amp;quot;accuracy&amp;quot;, accuracy)
train_summary_op = tf.summary.merge([cost,training_accuracy])
train_writer = tf.summary.FileWriter(&amp;quot;./logs&amp;quot;, graph=tf.get_default_graph())
# Initialize the variables (i.e. assign their default value)
init = tf.global_variables_initializer()
# Start training
with tf.Session() as sess:
    # Run the initializer
    sess.run(init)
    start_time = time.time()
    for step in range(1, num_steps+1):
        batch_x, batch_y = mnist.train.next_batch(batch_size)
        # Run optimization op (backprop)
        sess.run(training_process, feed_dict={X: batch_x, Y: batch_y})
        if step % display_step == 0 or step == 1:
            # Calculate batch loss and accuracy
            loss, acc, summary = sess.run([loss_function, accuracy, train_summary_op], feed_dict={X: batch_x,
                                                                 Y: batch_y})
            train_writer.add_summary(summary, step)
            print(&amp;quot;Step &amp;quot; + str(step) + &amp;quot;, Minibatch Loss= &amp;quot; + \
                  &amp;quot;{:.4f}&amp;quot;.format(loss) + &amp;quot;, Training Accuracy= &amp;quot; + \
                  &amp;quot;{:.3f}&amp;quot;.format(acc))
    end_time = time.time() 
    print(&amp;quot;Time duration: &amp;quot; + str(int(end_time-start_time)) + &amp;quot; seconds&amp;quot;)
    print(&amp;quot;Optimization Finished!&amp;quot;)
    # Calculate accuracy for 256 MNIST test images
    print(&amp;quot;Testing Accuracy:&amp;quot;, \
        sess.run(accuracy, feed_dict={X: mnist.test.images[:256],
                                      Y: mnist.test.labels[:256]}))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;计算结果中可以得到Testing accuracy为94.53125%&lt;/p&gt;
&lt;p&gt;计算输出如下：&lt;/p&gt;
&lt;p&gt;Step 1, Minibatch Loss= 48469.1758, Training Accuracy= 0.125&lt;br /&gt;
Step 10, Minibatch Loss= 20853.4180, Training Accuracy= 0.273&lt;br /&gt;
Step 20, Minibatch Loss= 7270.9775, Training Accuracy= 0.539&lt;br /&gt;
Step 30, Minibatch Loss= 6032.3340, Training Accuracy= 0.625&lt;br /&gt;
Step 40, Minibatch Loss= 3758.7192, Training Accuracy= 0.727&lt;br /&gt;
Step 50, Minibatch Loss= 2683.9473, Training Accuracy= 0.805&lt;br /&gt;
Step 60, Minibatch Loss= 2468.1599, Training Accuracy= 0.836&lt;br /&gt;
Step 70, Minibatch Loss= 1618.9688, Training Accuracy= 0.852&lt;br /&gt;
Step 80, Minibatch Loss= 1811.2744, Training Accuracy= 0.836&lt;br /&gt;
Step 90, Minibatch Loss= 2332.1758, Training Accuracy= 0.836&lt;br /&gt;
Step 100, Minibatch Loss= 2061.6094, Training Accuracy= 0.875&lt;br /&gt;
Step 110, Minibatch Loss= 1141.6943, Training Accuracy= 0.914&lt;br /&gt;
Step 120, Minibatch Loss= 1826.9503, Training Accuracy= 0.867&lt;br /&gt;
Step 130, Minibatch Loss= 2125.6672, Training Accuracy= 0.875&lt;br /&gt;
Step 140, Minibatch Loss= 1881.3708, Training Accuracy= 0.867&lt;br /&gt;
Step 150, Minibatch Loss= 1131.6064, Training Accuracy= 0.867&lt;br /&gt;
Step 160, Minibatch Loss= 691.9032, Training Accuracy= 0.938&lt;br /&gt;
Step 170, Minibatch Loss= 1885.9514, Training Accuracy= 0.844&lt;br /&gt;
Step 180, Minibatch Loss= 1068.9915, Training Accuracy= 0.914&lt;br /&gt;
Step 190, Minibatch Loss= 1600.2179, Training Accuracy= 0.906&lt;br /&gt;
Step 200, Minibatch Loss= 618.6491, Training Accuracy= 0.930&lt;br /&gt;
Step 210, Minibatch Loss= 1556.0098, Training Accuracy= 0.836&lt;br /&gt;
Step 220, Minibatch Loss= 894.7733, Training Accuracy= 0.922&lt;br /&gt;
Step 230, Minibatch Loss= 1324.5962, Training Accuracy= 0.930&lt;br /&gt;
Step 240, Minibatch Loss= 906.6990, Training Accuracy= 0.914&lt;br /&gt;
Step 250, Minibatch Loss= 997.3357, Training Accuracy= 0.906&lt;br /&gt;
Step 260, Minibatch Loss= 411.8199, Training Accuracy= 0.953&lt;br /&gt;
Step 270, Minibatch Loss= 1174.1459, Training Accuracy= 0.938&lt;br /&gt;
Step 280, Minibatch Loss= 1530.4811, Training Accuracy= 0.930&lt;br /&gt;
Step 290, Minibatch Loss= 1549.0024, Training Accuracy= 0.883&lt;br /&gt;
Step 300, Minibatch Loss= 871.0544, Training Accuracy= 0.945&lt;br /&gt;
Step 310, Minibatch Loss= 596.3419, Training Accuracy= 0.953&lt;br /&gt;
Step 320, Minibatch Loss= 760.7213, Training Accuracy= 0.922&lt;br /&gt;
Step 330, Minibatch Loss= 1059.3221, Training Accuracy= 0.891&lt;br /&gt;
Step 340, Minibatch Loss= 918.7598, Training Accuracy= 0.922&lt;br /&gt;
Step 350, Minibatch Loss= 1347.2892, Training Accuracy= 0.914&lt;br /&gt;
Step 360, Minibatch Loss= 854.8413, Training Accuracy= 0.922&lt;br /&gt;
Step 370, Minibatch Loss= 938.9316, Training Accuracy= 0.930&lt;br /&gt;
Step 380, Minibatch Loss= 963.6599, Training Accuracy= 0.914&lt;br /&gt;
Step 390, Minibatch Loss= 443.7957, Training Accuracy= 0.922&lt;br /&gt;
Step 400, Minibatch Loss= 899.7906, Training Accuracy= 0.922&lt;br /&gt;
Step 410, Minibatch Loss= 754.6378, Training Accuracy= 0.906&lt;br /&gt;
Step 420, Minibatch Loss= 25.4618, Training Accuracy= 0.992&lt;br /&gt;
Step 430, Minibatch Loss= 1071.7233, Training Accuracy= 0.914&lt;br /&gt;
Step 440, Minibatch Loss= 67.5639, Training Accuracy= 0.961&lt;br /&gt;
Step 450, Minibatch Loss= 805.5145, Training Accuracy= 0.906&lt;br /&gt;
Step 460, Minibatch Loss= 555.4743, Training Accuracy= 0.930&lt;br /&gt;
Step 470, Minibatch Loss= 505.9941, Training Accuracy= 0.930&lt;br /&gt;
Step 480, Minibatch Loss= 358.9708, Training Accuracy= 0.961&lt;br /&gt;
Step 490, Minibatch Loss= 633.9738, Training Accuracy= 0.930&lt;br /&gt;
Step 500, Minibatch Loss= 259.1246, Training Accuracy= 0.969&lt;br /&gt;
Time duration: 148 seconds&lt;br /&gt;
Optimization Finished!&lt;br /&gt;
Testing Accuracy: 0.9453125&lt;/p&gt;
&lt;div id=&#34;tensorboard&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;利用TensorBoard查看过程&lt;/h4&gt;
上述代码&lt;br /&gt;
&lt;code&gt;train_writer = tf.summary.FileWriter(&amp;quot;./logs&amp;quot;,graph=tf.get_default_graph())&lt;/code&gt;&lt;br /&gt;
中会在文件夹目录下新建文件夹&lt;strong&gt;logs&lt;/strong&gt;，并在里面生成名为&lt;strong&gt;events.out.tfevents.{time}.{machine-name}&lt;/strong&gt;的文件。通过打开Anaconda prompt并启动python3.5，打开TensorBoard，如图所示：
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;CNN5.PNG&#34; width = &#34;600&#34; height = &#34;500&#34; alt=&#34;Figure5&#34; /&gt;
&lt;p&gt;
图5. 通过Anaconda prompt打开TensorBoard
&lt;/div&gt;
&lt;p&gt;打开浏览器，地址栏输入 &lt;a href=&#34;http://localhost:6006&#34;&gt;&lt;strong&gt;http://localhost:6006&lt;/strong&gt;&lt;/a&gt; 即可打开TensorBoard。&lt;/p&gt;
在TensorBoard中可以查看cost和accuracy的变化
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;CNN6.PNG&#34; width = &#34;700&#34; height = &#34;600&#34; alt=&#34;Figure6&#34; /&gt;
&lt;p&gt;
图6. TensorBoard中cost和accuracy的变化图
&lt;/div&gt;
同时可以查看Graphs
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;CNN7.PNG&#34; width = &#34;800&#34; height = &#34;1000&#34; alt=&#34;Figure7&#34; /&gt;
&lt;p&gt;
图7. TensorFlow定义的计算图
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/52668301&#34;&gt;知乎. CNN(卷积神经网络)是什么？有入门简介或文章吗？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;罗冬日. Tensorflow入门与实战&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>生成对抗网络的第二Part</title>
      <link>/post/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AC%AC%E4%BA%8Cpart/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AC%AC%E4%BA%8Cpart/</guid>
      <description>


&lt;p&gt;我们认为每张图片对应的是一个高维向量，我们希望能找出这一类图片所在的图像空间的分布&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;，GAN的目的其实就是在寻找这个分布&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;传统的方法我们会想到用最大似然估计：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;首先给定一些样本数据，可以得到它的分布&lt;span class=&#34;math inline&#34;&gt;\(P_{data}(x)\)&lt;/span&gt;；接着我们假定有一个由参数&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;决定的分布&lt;span class=&#34;math inline&#34;&gt;\(P_G(x;\theta)\)&lt;/span&gt;；我们希望能够找到&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;，满足分布&lt;span class=&#34;math inline&#34;&gt;\(P_G(x;\theta)\)&lt;/span&gt;尽可能靠近分布&lt;span class=&#34;math inline&#34;&gt;\(P_{data}(x)\)&lt;/span&gt;；假设&lt;span class=&#34;math inline&#34;&gt;\(P_G(x;\theta)\)&lt;/span&gt;是高斯分布（正态分布），那&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;就是均值和方差。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;从分布&lt;span class=&#34;math inline&#34;&gt;\(P_{data}(x)\)&lt;/span&gt;中进行抽样获得一组样本数据&lt;span class=&#34;math inline&#34;&gt;\(\{x_1,x_2,\dots,x_m\}\)&lt;/span&gt;，我们可以得到似然值的表达式 &lt;span class=&#34;math display&#34;&gt;\[L = \prod_{i=1}^{m} P_G(x^i;\theta)\]&lt;/span&gt;我们希望能够找到&lt;span class=&#34;math inline&#34;&gt;\(\theta^*\)&lt;/span&gt;能够使得似然值&lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;最大。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;kl&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;最大似然估计与最小KL散度的等价性&lt;/h2&gt;
&lt;p&gt;通过对&lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;取对数，我们可以得到对数似然值，同时&lt;span class=&#34;math inline&#34;&gt;\(\theta^*\)&lt;/span&gt;可以通过下式得到： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\theta^* &amp;amp;=\ \arg \max_{\theta} \log L\\ 
&amp;amp;=\ \arg \max_{\theta} \log \prod_{i=1}^{m} P_G(x^i;\theta)\\
&amp;amp;=\ \arg \max_{\theta} \sum_{i=1}^m \log P_G(x^i;\theta)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由于样本&lt;span class=&#34;math inline&#34;&gt;\(\{x^1,x^2,\dots,x^m\}\)&lt;/span&gt;是随机抽取来自于分布&lt;span class=&#34;math inline&#34;&gt;\(P_{data}(x)\)&lt;/span&gt;，上式可以近似求&lt;span class=&#34;math inline&#34;&gt;\(\log P_G(x;\theta)\)&lt;/span&gt;的期望的最大点，可以表示为 &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\theta^* &amp;amp;\approx\ \arg \max_{\theta} E_{x\sim P_{data}}[\log P_G(x;\theta)]\\
&amp;amp;=\ \arg \max_{\theta} \int\limits_x P_{data}(x) \log P_G(x;\theta) dx
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;从式子中知道，我们所求的&lt;span class=&#34;math inline&#34;&gt;\(\theta^*\)&lt;/span&gt;只跟分布&lt;span class=&#34;math inline&#34;&gt;\(P_G(x;\theta)\)&lt;/span&gt;相关，我们可以在后面加上一项&lt;span class=&#34;math inline&#34;&gt;\(-\int\limits_x P_{data}(x) \log P_{data}(x)dx\)&lt;/span&gt;，这不影响求最大化下对应得&lt;span class=&#34;math inline&#34;&gt;\(\theta^*\)&lt;/span&gt;，那么由上式就可以得到 &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\theta^* &amp;amp;\approx\ \arg \max_{\theta} \int\limits_x P_{data}(x) \log P_G(x;\theta) dx - \int\limits_x P_{data}(x) \log P_{data}(x)dx\\
&amp;amp;=\ \arg \max_{\theta} \int\limits_x P_{data}(x) \Big[\log P_G(x;\theta) - \log P_{data}(x)\Big] dx\\
&amp;amp;=\ \arg \min_{\theta} \int\limits_x P_{data}(x) \Big[\log P_{data}(x) - \log P_G(x;\theta)\Big] dx\\
&amp;amp;=\ \arg \min_{\theta} \int\limits_x P_{data}(x) \log \frac{P_{data}(x)}{P_G(x;\theta)} dx\\
&amp;amp;=\ \arg \min_{\theta} KL(P_{data}||P_G)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;从上述推导可以发现&lt;strong&gt;最大似然估计实际上等价于最小化KL散度&lt;/strong&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generatorp_g&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;用生成器Generator定义概率分布&lt;span class=&#34;math inline&#34;&gt;\(P_G\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;如果&lt;span class=&#34;math inline&#34;&gt;\(P_G\)&lt;/span&gt;是很复杂的形式，那我们很难去直接计算我们的目标值。虽然我们可以假定一些简单的已知的分布来确定&lt;span class=&#34;math inline&#34;&gt;\(P_G\)&lt;/span&gt;，比如高斯分布；但是很多情况下，高斯分布的实验结果并不是很好；所以，我们希望用一个更generalized的函数来定义&lt;span class=&#34;math inline&#34;&gt;\(P_G\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;我们用生成器Generator（一个网络结构）来定义概率分布&lt;span class=&#34;math inline&#34;&gt;\(P_G\)&lt;/span&gt;，我们希望输入某个已知分布（比如高斯分布）随机产生的一个数&lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;，通过Generator之后可以得到&lt;span class=&#34;math inline&#34;&gt;\(x=G(z)\)&lt;/span&gt;，那么这些&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;可以构成某个复杂的分布，也可以说这个复杂的分布是我们希望得到的分布&lt;span class=&#34;math inline&#34;&gt;\(P_G\)&lt;/span&gt;，我们希望这个分布&lt;span class=&#34;math inline&#34;&gt;\(P_G\)&lt;/span&gt;能够和分布&lt;span class=&#34;math inline&#34;&gt;\(P_{data}\)&lt;/span&gt;越接近越好；从散度（Divergence）的角度来说，我们希望分布&lt;span class=&#34;math inline&#34;&gt;\(P_G\)&lt;/span&gt;和分布&lt;span class=&#34;math inline&#34;&gt;\(P_{data}\)&lt;/span&gt;的散度能够越小越好，即 &lt;span class=&#34;math display&#34;&gt;\[
G^* = \arg \min_G \text{Div}(P_G,P_{data})
\tag{1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果是最大似然估计的话，我们会希望KL散度越小越好，即 &lt;span class=&#34;math display&#34;&gt;\[G^* = \arg \min_G KL(P_G,P_{data})\]&lt;/span&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN1.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure1&#34; /&gt;
&lt;p&gt;
图1. 生成器Generator流程示意图
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;divergence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;如何获得我们需要的散度Divergence？&lt;/h2&gt;
&lt;p&gt;由于我们无法确定&lt;span class=&#34;math inline&#34;&gt;\(P_{data}(x)\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P_G(x)\)&lt;/span&gt;的分布，所以我们也无法直接计算得到他们的Divergence，当然就没办法直接用&lt;strong&gt;梯度下降&lt;/strong&gt;去找到最小的Divergence小对应的Generator。&lt;/p&gt;
&lt;p&gt;虽然我们无法知道&lt;span class=&#34;math inline&#34;&gt;\(P_{data}(x)\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P_G(x)\)&lt;/span&gt;的分布，但是利用重抽样（Resample）的想法，我们可以对它们这两个分布进行抽样；对于&lt;span class=&#34;math inline&#34;&gt;\(P_{data}(x)\)&lt;/span&gt;而言，我们从原数据抽取样本；对于&lt;span class=&#34;math inline&#34;&gt;\(P_G(x)\)&lt;/span&gt;而言，我们从高斯分布抽样再通过前面确定的Generator得到样本。&lt;/p&gt;
&lt;p&gt;我们引入上一Part讲到的监督器Discriminator（我们说到Discriminator也是一个network），对于训练Discriminator的时候，假设我们的目标函数是 &lt;span class=&#34;math display&#34;&gt;\[
V(G,D) = E_{x\sim P_{data}}[\log D(x)] + E_{x\sim P_{G}}[\log (1-D(x))]
\tag{2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;span class=&#34;math inline&#34;&gt;\(G\)&lt;/span&gt;代表Generator，它在这一步是固定不变的；我们希望最大化&lt;span class=&#34;math inline&#34;&gt;\(V(G,D)\)&lt;/span&gt;，式（2）第一项&lt;span class=&#34;math inline&#34;&gt;\(E_{x\sim P_{data}}[\log D(x)]\)&lt;/span&gt;表示从&lt;span class=&#34;math inline&#34;&gt;\(P_{data}(x)\)&lt;/span&gt;得到的&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;，我们希望&lt;span class=&#34;math inline&#34;&gt;\(D(x)\)&lt;/span&gt;能够尽可能大；而对于第二项&lt;span class=&#34;math inline&#34;&gt;\(E_{x\sim P_{G}}[\log (1-D(x))]\)&lt;/span&gt;，表示从&lt;span class=&#34;math inline&#34;&gt;\(P_G(x)\)&lt;/span&gt;得到的&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;，我们希望&lt;span class=&#34;math inline&#34;&gt;\(D(x)\)&lt;/span&gt;能够尽可能小，因为这样&lt;span class=&#34;math inline&#34;&gt;\(1-D(x)\)&lt;/span&gt;才能尽可能大；整体两部分加起来才能够做到尽可能大，实现&lt;span class=&#34;math inline&#34;&gt;\(V(G,D)\)&lt;/span&gt;的最大化。由于抽取的样本&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;在这个环节是固定的，相当于我们需要找到一个Discriminator满足最大化&lt;span class=&#34;math inline&#34;&gt;\(V(G,D)\)&lt;/span&gt;，即 &lt;span class=&#34;math display&#34;&gt;\[
D^* = \arg \max_D V(G,D)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;【注：使用式（2）作为目标函数相当于训练一个二分类器】&lt;/p&gt;
&lt;p&gt;当抽取的样本分散的很开的时候，即它们的散度很大的时候，我们很容易训练得到一个Discriminator能够容易区分来自两个不同分布的样本（很容易使我们的目标函数变大）；但如果抽取的样本分散的不开的时候，即它们的散度比较小时，我们训练得到的Discriminator是很难区分来自两个不同分布的样本的（很难使我们的目标函数变大）。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN2.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure2&#34; /&gt;
&lt;p&gt;
图2. 生成器Discriminator流程示意图
&lt;/div&gt;
&lt;div id=&#34;vdgdivergence&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;最大化目标函数&lt;span class=&#34;math inline&#34;&gt;\(V(D,G)\)&lt;/span&gt;与散度Divergence的联系&lt;/h3&gt;
&lt;p&gt;在给定Generator情况下，我们想要找到一个Discriminator使得我们的目标函数&lt;span class=&#34;math inline&#34;&gt;\(V(G,D)\)&lt;/span&gt;最大，依据上面式（2），我们可以得到： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
V(G,D) &amp;amp;=\ E_{x\sim P_{data}}[\log D(x)] + E_{x\sim P_G}[\log (1-D(x))]\\
&amp;amp;=\ \int\limits_x P_{data}(x) \log D(x) dx + \int\limits_x P_G(x) \log (1-D(x)) dx\\
&amp;amp;=\ \int\limits_x \Big[P_{data}(x) \log D(x) + P_G(x) \log (1-D(x))\Big] dx
\end{align}
\tag{3}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;给定&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;的情况下，最大化式（3）等价于最大化 &lt;span class=&#34;math display&#34;&gt;\[
P_{data}(x) \log D(x) + P_G(x) \log (1-D(x))
\tag{4}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;那么，原优化问题等价于找一个最优Discriminator使得式（4）最大。由于&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;固定了，那么&lt;span class=&#34;math inline&#34;&gt;\(P_{data}(x)\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P_G(x)\)&lt;/span&gt;是确定的；我们将&lt;span class=&#34;math inline&#34;&gt;\(P_{data}(x)\)&lt;/span&gt;记作&lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(P_G(x)\)&lt;/span&gt;记作&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(D(x)\)&lt;/span&gt;记作&lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;，那式（4）可以变化为： &lt;span class=&#34;math display&#34;&gt;\[
f(D) = a\log(D) + b\log (1-D)
\tag{5}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;对式（5）求导并赋值为0，可以得到： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
&amp;amp; \ \frac{df(D)}{dD} = \frac{a}{D} - \frac{b}{1-D} = 0\\
\Rightarrow&amp;amp; \ \frac{a}{D} = \frac{b}{1-D}\\
\Rightarrow&amp;amp; \ a\times (1-D) = b\times D\\
\Rightarrow&amp;amp; \ D = \frac{a}{a+b}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;将符号复原，我们知道满足式（4）最大的&lt;span class=&#34;math inline&#34;&gt;\(D^*\)&lt;/span&gt;满足 &lt;span class=&#34;math display&#34;&gt;\[
D^* = \frac{P_{data}(x)}{P_{data}(x)+P_G(x)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;由二阶导数我们可以知道该点为极大值点： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\frac{d^2f(D)}{dD^2} &amp;amp;=\ -\frac{a}{D^2} - \frac{b}{1-D^2}\\
&amp;amp;=\ -\frac{a}{(\frac{a}{a+b})^2}-\frac{b}{1-(\frac{a}{a+b})^2}\\
&amp;amp;=\ -\frac{(a+b)^2}{a}-\frac{(a+b)^2}{b}\\
&amp;amp;&amp;lt;\ 0
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;将&lt;span class=&#34;math inline&#34;&gt;\(D^*\)&lt;/span&gt;带入&lt;span class=&#34;math inline&#34;&gt;\(V(G,D) = E_{x\sim P_{data}}[\log D(x)] + E_{x\sim P_{G}}[\log (1-D(x))]\)&lt;/span&gt;，我们可以得到 &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\max_D V(G,D) &amp;amp;=\ V(G,D^*)\\
&amp;amp;=\ E_{x\sim P_{data}}[\log D(x)] + E_{x\sim P_G}[\log (1-D(x))]\\
&amp;amp;=\ E_{x\sim P_{data}}[\log \frac{P_{data}(x)}{P_{data}(x)+P_G(x)}] + E_{x\sim P_G}[\log (1-\frac{P_{data}(x)}{P_{data}(x)+P_G(x)})]\\
&amp;amp;=\ E_{x\sim P_{data}}[\log \frac{P_{data}(x)}{P_{data}(x)+P_G(x)}] + E_{x\sim P_G}[\log \frac{P_G(x)}{P_{data}(x)+P_G(x)}]\\
&amp;amp;=\ \int\limits_x P_{data}(x)\log \frac{P_{data}(x)}{P_{data}(x)+P_G(x)}dx + \int\limits_x P_G(x)\log \frac{P_G(x)}{P_{data}(x)+P_G(x)}dx\\
&amp;amp;=\ \int\limits_x P_{data}(x)\log \frac{\frac{1}{2}P_{data}(x)}{\frac{1}{2} [P_{data}(x)+P_G(x)]}dx + \int\limits_x P_G(x)\log \frac{\frac{1}{2}P_G(x)}{\frac{1}{2} [P_{data}(x)+P_G(x)]}dx\\
&amp;amp;=\ \int\limits_x P_{data}(x)\log \frac{P_{data}(x)}{\frac{1}{2} [P_{data}(x)+P_G(x)]}dx + \int\limits_x P_G(x)\log \frac{P_G(x)}{\frac{1}{2} [P_{data}(x)+P_G(x)]}dx - 2\log2\\
&amp;amp;=\ KL(P_{data} || \frac{P_{data}+P_G}{2}) + KL(P_G || \frac{P_{data}+P_G}{2}) - 2\log2
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;JS散度（Jensen-Shannon Divergence）是这样定义的： &lt;span class=&#34;math display&#34;&gt;\[
JS(P_1||P_2)=\frac{1}{2}KL(P_1||\frac{P_1+P_2}{2})+\frac{1}{2}KL(P_2||\frac{P_1+P_2}{2})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因此，我们可以推得 &lt;span class=&#34;math display&#34;&gt;\[
\max_D V(G,D) = V(G,D^*) = -2\log 2 + 2JS(P_{data}||P_G)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;从上面可以看出，我们在固定Generator之后，对&lt;span class=&#34;math inline&#34;&gt;\(V(G,D)\)&lt;/span&gt;的最大化相当于计算&lt;span class=&#34;math inline&#34;&gt;\(P_{data}\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P_G\)&lt;/span&gt;之间的JS散度再减去2倍&lt;span class=&#34;math inline&#34;&gt;\(\log2\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;在式（1）中散度Divergence是无法直接计算的，但是从刚刚的推导中，我们知道最大化目标函数&lt;span class=&#34;math inline&#34;&gt;\(V(G,D)\)&lt;/span&gt;可以得到散度的值，我们进一步用最大化&lt;span class=&#34;math inline&#34;&gt;\(V(G,D)\)&lt;/span&gt;替代&lt;span class=&#34;math inline&#34;&gt;\(P_{data}\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P_G\)&lt;/span&gt;之间的散度，可以得到一个Min-Max的优化问题。 &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
G^* &amp;amp;=\ \arg \min_G \text{Div}(P_G,P_{data})\\
&amp;amp;=\ \arg \min_G \max_D V(G,D)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;min-max&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;求解Min-Max问题&lt;/h3&gt;
&lt;p&gt;上面我们最终得到了一个Min-Max最优化问题 &lt;span class=&#34;math display&#34;&gt;\[G^* = \arg \min_G \max_D V(G,D)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如果我们手中没有Generator的话，我们无法得到&lt;span class=&#34;math inline&#34;&gt;\(V(G,D)\)&lt;/span&gt;的最大值，在解决这个优化问题之时，我们可以按一下步骤进行：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;初始化一个带随机参数的Generator &lt;span class=&#34;math inline&#34;&gt;\(G_0\)&lt;/span&gt;；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于&lt;span class=&#34;math inline&#34;&gt;\(i = 0,1,\dots,n\)&lt;/span&gt;，重复进行下列步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;通过梯度上升（Gradient Ascent）方法找到满足最大化&lt;span class=&#34;math inline&#34;&gt;\(V(G_i,D)\)&lt;/span&gt;的&lt;span class=&#34;math inline&#34;&gt;\(D_i\)&lt;/span&gt;；（这一步等价于获得了&lt;span class=&#34;math inline&#34;&gt;\(P_{data}\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(P_{G_i}\)&lt;/span&gt;之间的JS散度）&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;通过梯度下降（Gradient Descent）方法得到&lt;span class=&#34;math inline&#34;&gt;\(G_{i+1}\)&lt;/span&gt;：&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta_{G_{i+1}} = \theta_{G_i} - \eta\frac{\partial V(G,D_i)}{\partial \theta_G}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;算法的大致框架得到了，但是对于细节&lt;span class=&#34;math inline&#34;&gt;\(V(G,D)\)&lt;/span&gt;的形式我们只有式（2）；对于式（2）中的期望，实际上我们是很难得到的，在实践中，我们用样本均值来代替期望值；因此最大化&lt;span class=&#34;math inline&#34;&gt;\(V(G,D)\)&lt;/span&gt;可以转换为最大化 &lt;span class=&#34;math display&#34;&gt;\[ \tilde{V} = \frac{1}{m}\sum_{i=1}^m\log D(x^i) + \frac{1}{m}\sum_{i=1}^m\log (1-D(\tilde{x}^i)) \]&lt;/span&gt; 其中样本&lt;span class=&#34;math inline&#34;&gt;\(\{x^1,x^2,\dots,x^m\}\)&lt;/span&gt;来自分布&lt;span class=&#34;math inline&#34;&gt;\(P_{data}(x)\)&lt;/span&gt;，样本&lt;span class=&#34;math inline&#34;&gt;\(\{\tilde{x}^1,\tilde{x}^2,\dots,\tilde{x}^m\}\)&lt;/span&gt;来自分布&lt;span class=&#34;math inline&#34;&gt;\(P_G(x)\)&lt;/span&gt;。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gan&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;GAN的算法&lt;/h3&gt;
通过上面的推导介绍，我们可以总结出GAN的算法框架出来：
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN3.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure3&#34; /&gt;
&lt;p&gt;
图3. GAN的算法框架
&lt;/div&gt;
&lt;p&gt;可以注意到，在训练Generator的时候，其中&lt;span class=&#34;math inline&#34;&gt;\(\tilde{V}\)&lt;/span&gt;的第一项实际上是与Generator是无关的，去掉第一项不影响我们最小化目标函数&lt;span class=&#34;math inline&#34;&gt;\(\tilde{V}\)&lt;/span&gt;。显然，整个GAN算法分为两部分，我们可以这么理解：第一步训练Discriminator实际上是度量两个分布间JS散度，而训练Generator是在最小化JS散度。&lt;/p&gt;
&lt;p&gt;从数学推导可以帮助我们更好理解GAN，这一Part就结束啦๑乛◡乛๑&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://speech.ee.ntu.edu.tw/~tlkagk/index.html&#34;&gt;李宏毅个人主页&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>生成对抗网络的第一Part</title>
      <link>/post/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AC%AC%E4%B8%80part/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AC%AC%E4%B8%80part/</guid>
      <description>


&lt;p&gt;从知乎上了解到台大有位著名的教授&lt;a href=&#34;http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html&#34;&gt;李宏毅&lt;/a&gt;超级会讲&lt;a href=&#34;https://arxiv.org/abs/1406.2661v1&#34;&gt;Generative Adversarial Networks, GAN&lt;/a&gt;技术，所以慕名而到Youtube找到他的上课视频成为他的“课外学生”。李教授真的厉害，形象生动地讲解GAN的各个知识点。那么，我把我学到的整理为一篇博客，尝试作为一名“GAN路上的导游”。&lt;/p&gt;
&lt;p&gt;Yann LeCun是Facebook的AI研究部门的Director，同时也是NYU（New York University）的一位教授，维基百科上是这么介绍他：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;He is the Chief Artificial Intelligence Scientist at Facebook AI Research, and he is well known for his work on optical character recognition and computer vision using convolutional neural networks (CNN), and is a founding father of convolutional nets.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;做Deep Learning的人多多少少会听过这个名字，他曾经这样回答了Quora论坛上的一个问题（What are some recent and potentially upcoming breakthroughs in unsupervised learning?）：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Adversarial training is the coolest thing since sliced bread.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这里sliced bread的中文意思是切片面包，但其实这里是表示了有一个好东西问世，给某个领域带来了巨大发展，维基百科这么说：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The phrase “the greatest thing since sliced bread” is a common hyperbole used to praise an invention or development.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这也说明了GAN推进了整个领域的发展。Yann LeCun对GAN也有过这样极高的评价：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This, and the variations that are now being proposed is the most interesting idea in the last 10 years in ML, in my opinion.&lt;/p&gt;
&lt;/blockquote&gt;
李教授统计了ICASSP（International Conference on Acoustics, Speech and Signal Processing）的文章题目涵盖关键词的数量：
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN1.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure1&#34; /&gt;
&lt;p&gt;
图1. ICASSP文章题目涵盖关键词的数量变化图
&lt;/div&gt;
&lt;p&gt;很明显，从17年的2篇Adversarial到18年的42篇Adversarial，同年增长了21倍！相当惊人！既然GAN这么Popular又是这么酷炫，那么我们就开始正文吧！(๑•̀ㅂ•́)و✧&lt;/p&gt;
&lt;div id=&#34;gangan&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;什么是GAN？说GAN就干！&lt;/h1&gt;
&lt;p&gt;GAN里面主要分为Generator和Discriminator这两部分，其实原理很简单，Generator是负责训练样本并能生成对的Output，而Discriminator像是一位老师，看看Generator这位学生交的作业质量怎么样，会给Generator的作业一个分数。下面我们更详细地介绍他们这两部分~&lt;/p&gt;
&lt;div id=&#34;generator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;一无所知的Generator生成器&lt;/h2&gt;
Generator其实就是neural network (NN)，它的输入是向量，那我们如果丢一个向量到Generator里面，它就能产生某个Output（可能是一张相片或者一句话等等）
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN2.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure2&#34; /&gt;
&lt;p&gt;
图2. Generator生成器的示例图
&lt;/div&gt;
&lt;p&gt;李教授酷爱二次元，所以他举了这样的例子，丢一些向量给生成图片的Generator就生成了一些二次元的图片；另外，对应句子生成的例子，丢一些向量给生成句子的Generator就生成了一些句子。简单来说，我们给Generator这个function（NN其实就是一个复杂的function）赋值，它就会产生对应的结果。&lt;/p&gt;
我们以图片为例，输入的向量中的每一个元素，可能对应着图片中不同的特征。假设第一位是改变头发的长短特征，那从下图中可以看到0.1改为3后，图片中的女孩从短头发变为长头发；假设向量倒数第二位是改变头发的颜色特征，那从下图可以看到5.4改为2.4后，图片中的女孩从紫色变为了蓝色；假设向量倒数第一位是改变嘴巴特征，那从下图可以看到0.9改为3.5后，图片中的女孩从小嘴巴变为了张开嘴巴。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN3.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure3&#34; /&gt;
&lt;p&gt;
图3. 输入向量对结果输出的影响
&lt;/div&gt;
&lt;p&gt;在实际操作中，如果我们通过大量的样本让Generator训练，使其能够输出与图片尽可能相似的结果，那这样就只是普通的NN（神经网络），但我们想要更高级！想要输出的图片质量更好！而GAN满足了我们的需求，它的idea妙就妙在搞多了一个Discriminator（监督器），审判Generator输出的结果是不是真的“好”。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;discriminator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;超严厉的Discriminator监督器&lt;/h2&gt;
Discriminator也是一个NN，但是它的输入是一张图片或是一句话（Generator的输出）。而它的输出是一个数值，这个数值代表了这张图片的质量如何，数值越大，那图片的质量就越好，越像是真实的图片；相反，数值越小，图片质量越差。下面可以看到不同质量的二次元图片对应着不同的得分数值~
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN4.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure4&#34; /&gt;
&lt;p&gt;
图4. 不同二次元图片对应不同的得分
&lt;/div&gt;
&lt;p&gt;前面大致讲了一下Generator和Discriminator的关系，但是不算很生动详细！下面我来举个栗子！&lt;/p&gt;
我们可以把Generator和Discriminator当做捕食者和被捕食者，那Pokemon里面鸟系对虫系就有着威慑能力，天生虫系会怕鸟系嘛，这很合理╮(๑•́ ₃•̀๑)╭
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN5.PNG&#34; width = &#34;300&#34; height = &#34;300&#34; alt=&#34;Figure5&#34; /&gt;
&lt;p&gt;
图5. 小智的比比鸟和绿毛虫的初次相遇
&lt;/div&gt;
&lt;p&gt;其实一开始绿毛虫（一代Generator）就很怕波波（一代Discriminator）嘛，所以它就会进化成铁甲蛹（二代Generator），那波波个子小就拿它没办法了，吃也吃不了╮(๑•́ ₃•̀๑)╭所以波波也进化了变成了比比鸟（二代Discriminator）。那现在比比鸟翅膀大了，爪子一夹，把铁甲蛹抓到空中再丢下来（极其残忍！），铁甲蛹也会痛！所以铁甲蛹不甘示弱，它接着进化成巴大蝴（三代Generator）！这下比比鸟没办法它也进化成比雕（三代Discriminator），它以为比雕可能会搞得定巴大蝴！奈何巴大蝴也有翅膀了，还有催眠粉！比雕觉得没办法了，就这样吧，两人实力相当了！&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN6.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure6&#34; /&gt;
&lt;p&gt;
图6. Generator和Discriminator的相互关系
&lt;/div&gt;
&lt;p&gt;回到现实！这里的巴大蝴就是我们的最终Generator（这里以三代为例，实际需要N代），那它所生成的图片可以让最终的Discriminator认为是真实的，这就达到了我们的目的(*•̀ㅂ•́)و&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GAN的算法框架&lt;/h2&gt;
&lt;p&gt;我们了解了Generator和Discriminator的基本知识后，我们来看看算法~&lt;/p&gt;
&lt;p&gt;首先随机产生了两个NN作为Generator和Discriminator，然后不断循环：固定Generator调Discriminator的参数，再固定Discriminator调Generator的参数。具体过程如下所示：&lt;/p&gt;
在第一步中固定Generator调Discriminator的参数，对于从Database出来的真实案例，我们希望Discriminator给高分；对于Generator生成的案例，我们希望Discriminator给低分。Discriminator通过训练会在这个过程中学会给真实的案例高分，给假的案例低分。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN7.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure7&#34; /&gt;
&lt;p&gt;
图7. 固定Generator下训练Discriminator
&lt;/div&gt;
在下一步中我们固定Discriminator调Generator的参数，我们希望调整Generator后，我们给一个向量生成一个案例，这个案例通过Discriminator得到的分数能够尽量高。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN8.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure8&#34; /&gt;
&lt;p&gt;
图8. 固定Discriminator下训练Generator
&lt;/div&gt;
如果我们写成Pseudocode（伪代码），就变成下面这样：
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN9.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure9&#34; /&gt;
&lt;p&gt;
图9. 关于GAN算法的伪代码
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;unsupervised-conditional-generation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Unsupervised Conditional Generation非监督的条件生成器&lt;/h1&gt;
&lt;p&gt;通常在普通的神经网络方法下，但是我们需要对应的样本来帮助我们生成我们需要的图片；我们给一个输入，再给一个Label告诉机器，你看到这个输入就需要有这样的Label的输出，但如果对于某些样本我们刚好没有对应Label的话，但我们手中有其他类似的，那我们可以通过非监督的条件生成器来生成我们想要的结果。&lt;/p&gt;
我们希望能够创造一个Generator，输入一个来自Domain X的样本，可以输出一个对应在Domain Y的样本。&lt;strong&gt;这相当于一种风格转化的案例。&lt;/strong&gt;举个例子，现在许多人喜欢拍照用滤镜，通常我们拍的普通相片是没有带滤镜的，那现在我希望能够得到水彩画形式的相片，如下所示：
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN10.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure10&#34; /&gt;
&lt;p&gt;
图10. 变成水彩画形式的相片
&lt;/div&gt;
&lt;div id=&#34;direct-transformation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;样本的直接转化问题（Direct Transformation）&lt;/h2&gt;
&lt;p&gt;像上面叙述的例子，其实普通相片到油画形式的相片，只是&lt;strong&gt;色彩质地&lt;/strong&gt;有所区别，但是总体的框架基本不变，相当于说只是小部分修改了我们输入的相片，那Direct Transformation就足够帮我们处理这个问题了。&lt;/p&gt;
通常我们会用GAN的技术来实现，GAN其实也是可以解决这个问题，如果Generator的层数很少，不是那么复杂，在Discrimiantor的监督下生成油画型的图片跟输入的图片差距不会太大。但是如果Generator很复杂的情况下，Generator是存在可能会生成在Domain Y的相片与在Domain X的相片差距很大。即使在Discriminator中拿到了很高的分数，但是与原始输入的样本差距甚远，如下图所示：
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN11.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure11&#34; /&gt;
&lt;p&gt;
图11. 普通GAN的弱点
&lt;/div&gt;
&lt;p&gt;在Domain X的河道图输入之后，尽管训练得到的Generator产生了艺术型的油画图，但是明显上图中的梵高自画像不是我们希望得到的输出，但它确实是属于Domain Y类型的图片。&lt;/p&gt;
&lt;div id=&#34;cycle-gan&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cycle GAN&lt;/h3&gt;
存在上述的问题，那么我们想在生成Domain Y的图片之后，我们可以做一个逆函数（NN）来返回去检验这个图片，看看能否恢复成我们输入图片的样子；当然，我们需要Discriminator帮助我们将输入的图片训练生成在Domain Y的图片。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN12.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure12&#34; /&gt;
&lt;p&gt;
图12. Cycle GAN处理过程示意图
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;stargan&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;StarGAN&lt;/h3&gt;
&lt;p&gt;如果现在我们不仅仅希望普通相片可以变成一张油画照，我们还希望可以变成黑白照或者素描照，那我们的案例就变得更复杂，需要多个Domain，而且我们希望能够在多个Domain里面互转。假设我们有普通照片，油画照，黑白照和素描照这四种类型照片，那其实我们需要创建&lt;span class=&#34;math inline&#34;&gt;\(C_4^2\)&lt;/span&gt;个Cycle GAN来解决这样一个大问题，使得在这四个Domain之间实现互转（如下图(a)所示）。&lt;/p&gt;
&lt;p&gt;2017年arXiv上Yunjey Choi等人发表了文章&lt;a href=&#34;https://arxiv.org/abs/1711.09020&#34;&gt;StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation&lt;/a&gt;，StarGAN的方便之处是在于只学习了一个Generator，就可以在多个Domain之间实现互转（如下图(b)所示）。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN13.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure13&#34; /&gt;
&lt;p&gt;
图13. Cross-domain models和StarGAN的示意图
&lt;/div&gt;
&lt;p&gt;下面我们通过原文提供的案例来进一步了解StarGAN：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;首先，我们先&lt;strong&gt;训练Discriminator&lt;/strong&gt;。这个Discriminator的输入有真实的照片和假的照片，我们希望Discriminator可以输出判断输入的照片是真的还是假的；同时希望可以输出判断真实的照片属于哪一个Domain。在下图案例可以看到，我们真实判断的Domain是&lt;span class=&#34;math inline&#34;&gt;\(0\ 0\ 1\ 0\ 1\)&lt;/span&gt;相当于真实照片是一个棕色头发年轻的女性角色。在这个地方可以注意到，我们只在考虑CelebA label，但是没有考虑RaFD label，而这个是由Mask vector所控制的（先设定为&lt;span class=&#34;math inline&#34;&gt;\(1\ 0\)&lt;/span&gt;）。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;接着我们以真实照片（&lt;strong&gt;棕色头发年轻的女性角色&lt;/strong&gt;）和我们希望得到的Domain（&lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\ 0\ 0\ 1\ 1\)&lt;/span&gt;&lt;/strong&gt;）作为我们&lt;strong&gt;训练Generator&lt;/strong&gt;的输入，我们希望输出得到目标Domain的照片（转化案例是希望得到一个&lt;strong&gt;黑色头发年轻的男性角色&lt;/strong&gt;）。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;利用Cycle GAN的想法，我们用输出的照片（&lt;strong&gt;黑色头发年轻的男性角色&lt;/strong&gt;）和我们原始的Domain（&lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(0\ 0\ 1\ 0\ 1\)&lt;/span&gt;&lt;/strong&gt;）作为另外一个Generator的输入，希望得到和原始一模一样的照片（&lt;strong&gt;棕色头发年轻的女性角色&lt;/strong&gt;）。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;最后呢~将第二步训练得到的照片作为Discriminator的输入，训练Discriminator判断能不能认为输入的照片是真实的照片，并且是属于我们希望得到的Domain（&lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\ 0\ 0\ 1\ 1\)&lt;/span&gt;&lt;/strong&gt;）。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;循环前面第二到第四步，直到第四步中Discriminator认为第二步所输出的照片是真实的且属于Domain（&lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(1\ 0\ 0\ 1\ 1\)&lt;/span&gt;&lt;/strong&gt;）。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
同理，我们可以让Mask vector为&lt;span class=&#34;math inline&#34;&gt;\(0\ 1\)&lt;/span&gt;，不考虑CelebA label了而是考虑RaFD label，即考虑情绪的变换。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN14.PNG&#34; width = &#34;600&#34; height = &#34;700&#34; alt=&#34;Figure14&#34; /&gt;
&lt;p&gt;
图14. 来自原Paper的案例
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;projection-to-common-space&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;投影到共有的空间（Projection to Common Space）&lt;/h2&gt;
&lt;p&gt;那如果想要&lt;strong&gt;转化的结果跟你原来输入的样本结果差别很大&lt;/strong&gt;，对于图片而言，可能&lt;strong&gt;画风突变&lt;/strong&gt;！比如从真人头像到二次元的漫画头像，这时没办法简单地进行Direct Transformation了，需要利用Projection这样的技术。如果我们先通过编码器获得latent variables，再通过解码器获得我们想要的输出，这样就有可能做到比较大的变换。&lt;/p&gt;
&lt;strong&gt;问题&lt;/strong&gt;&lt;br /&gt;
对于两个Domain的大变化的互转，我们希望可以先分别对两个不同Domain的样本做Auto-Encoder，同时最小化重建的误差（Minimizing reconstruction error）。我们假设Domain X是漫画版本闪电侠，我们希望直接通过图片转化成真人版本的闪电侠（Domain Y），我们按刚刚说的流程，实现的流程如下图所示。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN15.PNG&#34; width = &#34;700&#34; height = &#34;500&#34; alt=&#34;Figure15&#34; /&gt;
&lt;p&gt;
图15. 漫画闪电侠与真人版的互换
&lt;/div&gt;
但是这样做存在问题就是Domain X做好的Auto-Encoder跟Domain Y做好的Auto-Encoder是完全没有联系的，这种情况下可能我输入一张闪电侠的漫画图片，通过Y的解码器（Decoder）之后，没办法得到闪电侠的真人图片，而是得到了绿箭侠的真人图片(๑•́ ₃ •̀)
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN16.PNG&#34; width = &#34;700&#34; height = &#34;500&#34; alt=&#34;Figure16&#34; /&gt;
&lt;p&gt;
图16. 漫画闪电侠的变换结果出错的可能性
&lt;/div&gt;
【注：美剧闪电侠第五季的第九集和美剧绿箭第七季的第九集中，Oliver Queen变成了闪电侠。。。Barry Allen变成了绿箭侠。。。源自异世界的剧情_(:з」∠)_ 】
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;GAN21.PNG&#34; width = &#34;500&#34; height = &#34;700&#34;/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;回到正题！！！&lt;/p&gt;
&lt;strong&gt;方法1&lt;/strong&gt;&lt;br /&gt;
那有一种方法呢，就是在获得中间的latent variables之前时，两个encoder最后有几层是共享相同参数的；同样在进入decoder部分时，前面几层也是共享相同参数的。这样在获得latent variables的时候，能够尽可能落在相同的latent space。这样的方法出现在文章&lt;a href=&#34;https://arxiv.org/abs/1606.07536&#34;&gt;Coupled Generative Adversarial Networks&lt;/a&gt;和文章&lt;a href=&#34;https://arxiv.org/abs/1703.00848&#34;&gt;Unsupervised Image-to-Image Translation Networks&lt;/a&gt;。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN17.PNG&#34; width = &#34;700&#34; height = &#34;500&#34; alt=&#34;Figure17&#34; /&gt;
&lt;p&gt;
图17. 共享参数的方法示意图
&lt;/div&gt;
&lt;strong&gt;方法2&lt;/strong&gt;&lt;br /&gt;
另外一种方法就是我们加一个Domain Discriminator，对中间产生的latent variables进行判定，判定产生的latent variables是来自&lt;span class=&#34;math inline&#34;&gt;\(EN_X\)&lt;/span&gt;还是来自&lt;span class=&#34;math inline&#34;&gt;\(EN_Y\)&lt;/span&gt;，那么两个Encoder就会训练使得产生的latent variables能够骗过Discriminator。这样相当于这个Domain Discriminator促使两个Encoder产生的latent variables来自相同的分布。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN18.PNG&#34; width = &#34;700&#34; height = &#34;500&#34; alt=&#34;Figure18&#34; /&gt;
&lt;p&gt;
图18. 加入Domain Discriminator的示意图
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;方法3&lt;/strong&gt;&lt;br /&gt;
利用Cycle GAN的想法，我们还可以输入一张我们想要转换的图片（比如输入是漫画闪电侠），通过Domain X的encoder和Domain Y的decoder得到目标输出结果；然后，将这个结果作为Domain Y的encoder的输入，再通过Domain X的decoder得到我们想要转换的图片，当然我们会希望重建误差尽可能小。&lt;/p&gt;
另外，我们可以加入两个Discriminator来判断两个decoder生成的图片是否属于各自相对应的Domain，这样的想法就有用在ComboGAN上，ComboGAN的文章全名叫做&lt;a href=&#34;https://arxiv.org/abs/1712.06909&#34;&gt;ComboGAN: Unrestrained Scalability for Image Domain Translation&lt;/a&gt;。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN19.PNG&#34; width = &#34;700&#34; height = &#34;500&#34; alt=&#34;Figure19&#34; /&gt;
&lt;p&gt;
图19. Cycle GAN想法下的流程示意图
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;方法4&lt;/strong&gt;&lt;br /&gt;
除了最小化重建误差，我们也可以最小化从Domain X和Domain Y分别编码得到的latent variables。跟方法3有点相似，先输入一张我们想要转换的图片（比如输入是漫画闪电侠），通过Domain X的encoder得到latent variables，再通过Domain Y的decoder得到目标输出结果；然后，将这个结果作为Domain Y的encoder的输入，可以再次得到latent variables。其中这两次获得的latent variables，我们希望它们能够越接近越好。那这样的方法就有用在DTN和XGAN上，它们的文章全名分别为&lt;a href=&#34;https://arxiv.org/abs/1611.02200&#34;&gt;Unsupervised Cross-Domain Image Generation&lt;/a&gt;和&lt;a href=&#34;https://arxiv.org/abs/1711.05139&#34;&gt;XGAN: Unsupervised Image-to-Image Translation for Many-to-Many Mappings&lt;/a&gt;。&lt;/p&gt;
&lt;div class=&#34;section level3&#34;&gt;
&lt;h3&gt;更多的应用！语音变换！&lt;/h3&gt;
其实这项技术还可以用在语音变换上，可能生活中我们希望做一个变声器，我们说一句话但是扬声器出来的是另外一个人的声音。其实阿笠博士在20年前就已经给柯南一个变声器了，每次柯南都用来假装毛利小五郎的声音(..•˘_˘•..)
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;GAN20.PNG&#34; width = &#34;500&#34; height = &#34;500&#34; alt=&#34;Figure20&#34; /&gt;
&lt;p&gt;
图20. 柯南的蝴蝶结变声器
&lt;/div&gt;
&lt;p&gt;结束基础普及知识的第一Part！！！第二Part再从基础理论来看GAN！&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;http://speech.ee.ntu.edu.tw/~tlkagk/index.html&#34;&gt;李宏毅个人主页&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/DQNNMiAP5lw&#34;&gt;李宏毅，Youtube：GAN Lecture 1 (2018): Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1406.2661v1&#34;&gt;I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial networks,” in Advances in Neural Information Processing Systems (NIPS), 2014. 1, 3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1711.09020&#34;&gt;Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo, “Stargan: Unified generative adversarial networks for multi-domain image-toimage translation,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 1, 3, 6, 7, 8, 9, 10&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>深度神经网络基础</title>
      <link>/post/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</guid>
      <description>


&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;神经元及神经网络基础结构&lt;/h1&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;NN1.PNG&#34; width = &#34;400&#34; height = &#34;200&#34; alt=&#34;Figure1&#34; /&gt;
&lt;p&gt;
图1. 神经元的组成（源自维基百科）
&lt;/div&gt;
&lt;p&gt;神经元这个图大多数理科生在高中生物课本都学过~神经网络则由许许多多的神经元所组成，通常一个神经元具有多个树突，主要用来接收消息；轴突只有一条，相当于我们定义的一个计算过程；而轴突尾部的许许多多轴突末梢，将传递信息给其他神经元。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;NN2.PNG&#34; width = &#34;400&#34; height = &#34;200&#34; alt=&#34;Figure2&#34; /&gt;
&lt;p&gt;
图2. 神经网络基础结构
&lt;/div&gt;
&lt;p&gt;通常这里的非线性函数会用上各式各样的激活函数，比如Sigmoid函数，tanh函数和ReLu函数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sigmoid函数&lt;/strong&gt;&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[f(z) = \frac{1}{1+e^{-z}}\]&lt;/span&gt; &lt;strong&gt;tanh函数&lt;/strong&gt;&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[f(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}}\]&lt;/span&gt; &lt;strong&gt;ReLu函数&lt;/strong&gt;&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[f(z) = \max(0,z)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-01-05-深度神经网络基础_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;神经网络基础认知&lt;/h1&gt;
我们把许多神经元组合起来就可以得到一个神经网络，由于有输入的数据和我们想得到的输出数据，便会有“输入层”（Input layer）和“输出层”（Output layer）；中间的神经元则组成了“隐藏层”（Hidden layer）。在下面图3中，输入层有3个神经元，隐藏层有4个神经元，输出层有2个神经元。在实际情况中，输入层和输出层通常是固定的，而隐藏层的层数和节点数则可以自由调节。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;NN3.PNG&#34; width = &#34;300&#34; height = &#34;300&#34; alt=&#34;Figure3&#34; /&gt;
&lt;p&gt;
图3. 神经网络基础层级结构
&lt;/div&gt;
&lt;p&gt;我们假设一个全连接的网络结构，其中隐藏层只有一层。另外，假设输入层和隐藏层之间的边的权值构成的矩阵为 &lt;span class=&#34;math display&#34;&gt;\[
\left [
\begin{matrix}
w_{11} &amp;amp; w_{12} &amp;amp; w_{13} \\
w_{21} &amp;amp; w_{22} &amp;amp; w_{23} \\
w_{31} &amp;amp; w_{32} &amp;amp; w_{33}
\end{matrix}
\right ]
\]&lt;/span&gt; 其中，第一列的&lt;span class=&#34;math inline&#34;&gt;\(w_{11}, w_{21}, w_{31}\)&lt;/span&gt;代表的是输入层的点&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;分别连接隐藏层的三个节点的边的权值；第二列的&lt;span class=&#34;math inline&#34;&gt;\(w_{12}, w_{22}, w_{32}\)&lt;/span&gt;代表的是输入层的点&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;分别连接隐藏层的三个节点的边的权值；第三列的&lt;span class=&#34;math inline&#34;&gt;\(w_{13}, w_{23}, w_{33}\)&lt;/span&gt;代表的是输入层的点&lt;span class=&#34;math inline&#34;&gt;\(x_3\)&lt;/span&gt;分别连接隐藏层的三个节点的边的权值。&lt;/p&gt;
&lt;p&gt;图中的“+1”点代表我们添加了一个值b，称其为偏置项。那么，隐藏层的节点可以由下计算得到： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
a_1 = w_{11}\times x_1 + w_{12}\times x_2 + w_{13}\times x_3 + b_1\\
a_2 = w_{21}\times x_1 + w_{22}\times x_2 + w_{23}\times x_3 + b_2\\
a_3 = w_{31}\times x_1 + w_{32}\times x_2 + w_{33}\times x_3 + b_3
\end{align}
\tag{1}
\]&lt;/span&gt; 由于线性计算的表现能力比较差，所以考虑用非线性函数进行计算，即使用激活函数&lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt;（前面已提及）。（1）式可以变换为（2）式： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
a_1 = f(w_{11}\times x_1 + w_{12}\times x_2 + w_{13}\times x_3 + b_1)\\
a_2 = f(w_{21}\times x_1 + w_{22}\times x_2 + w_{23}\times x_3 + b_2)\\
a_3 = f(w_{31}\times x_1 + w_{32}\times x_2 + w_{33}\times x_3 + b_3)
\end{align}
\tag{2}
\]&lt;/span&gt; 将（2）式改写为矩阵运算形式（3）式： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\boldsymbol{a} = f \begin{pmatrix} \begin{pmatrix} w_{11},w_{12},w_{13}\\w_{21},w_{22},w_{23}\\w_{31},w_{32},w_{33} \end{pmatrix} \begin{pmatrix} x_1\\x_2\\x_3 \end{pmatrix} + \begin{pmatrix} b_1\\b_2\\b_3 \end{pmatrix} \end{pmatrix} = f(\boldsymbol{W}x+\boldsymbol{B})
\end{align}
\tag{3}
\]&lt;/span&gt;&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;NN4.PNG&#34; width = &#34;400&#34; height = &#34;400&#34; alt=&#34;Figure4&#34; /&gt;
&lt;p&gt;
图4. 简单全连接网络中层之间的计算方式
&lt;/div&gt;
当我们增加隐藏层的层数，便可以构成更复杂的网络，即深度神经网络。
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;NN5.PNG&#34; width = &#34;600&#34; height = &#34;400&#34; alt=&#34;Figure5&#34; /&gt;
&lt;p&gt;
图5. 深度神经网络示意图
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;loss-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;损失函数（Loss Function）&lt;/h1&gt;
&lt;p&gt;训练数据通常是一系列“输入-输出”数据对组成的集合，我们希望输入一个数据，尽可能与配对的输出数据相同。那么网络的输出结果和实际的真实结果差多少，我们需要一定数学形式进行量化，所以引入了损失函数（Loss Function）。常见的损失函数有以下几种：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;0-1损失函数&lt;/strong&gt;&lt;br /&gt;
如果预测值和真实值一样，则损失值为0；若不等，则为1；公式表达为： &lt;span class=&#34;math display&#34;&gt;\[
L(y,f(x)) = \begin{cases}
1, &amp;amp; y = f(x)\\
0, &amp;amp; y \neq f(x)
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;绝对值损失函数（1-范数形式）&lt;/strong&gt;&lt;br /&gt;
通过预测值和真实值之差的绝对值进行衡量，公式表达为： &lt;span class=&#34;math display&#34;&gt;\[
L(y,f(x)) = |y-f(x)|
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;均方误差损失函数（2-范数形式）&lt;/strong&gt;&lt;br /&gt;
通过计算预测值和真实值之差的平方再求均值，可得到均方误差，公式表达为： &lt;span class=&#34;math display&#34;&gt;\[
L(y,f(x)) = \frac{1}{n}\sum_{i=1}^n(y_i-f(x_i))^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level1&#34;&gt;
&lt;h1&gt;优化算法&lt;/h1&gt;
&lt;div id=&#34;gradient-descent-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;梯度下降法（Gradient Descent Method）&lt;/h2&gt;
&lt;p&gt;传统的梯度下降法是通过计算损失函数的一阶导数作为方向进行下降计算，计算方法可表示为： &lt;span class=&#34;math display&#34;&gt;\[
W_{ij} = W_{ij} - \alpha\frac{\partial}{\partial W_{ij}}L(w,b)\\
b_i = b_i - \alpha\frac{\partial}{\partial b_i}L(w,b)
\]&lt;/span&gt; 其中&lt;span class=&#34;math inline&#34;&gt;\(W_{ij}\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(b_i\)&lt;/span&gt;是需要优化的参数，&lt;span class=&#34;math inline&#34;&gt;\(L(w,b)\)&lt;/span&gt;是损失函数，&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;在深度学习中通常称为学习率（learning rate），在机器学习或最优化计算领域中我们通常称为步长（stepsize）。&lt;/p&gt;
&lt;p&gt;传统的梯度下降法需要计算&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;个梯度，即样本数量的梯度个数，在数据越来越大的时代，这会大大降低我们需要的计算速度，因此也产生了随机梯度下降（Stochastic gradient descent）这一类的方法。随机梯度下降通常随机选取某个样本并计算其相应导数，作为所有样本相同的导数进行计算，这种方法在实践上有不错的效果。当然，我们可以随机选取一小批样本，样本数量记为batch size，将batch size个样本的导数进行累加后求均值作为所有样本相同的导数，再进一步计算；这种方法我们称为小批量随机梯度下降法（mini-batch stochastic gradient descent）。&lt;/p&gt;
&lt;p&gt;虽然梯度下降直接快速，但是也有一定的不足，由于我们需要选取stepsize，若stepsize太大，那可能无法达到优化问题的最优点；若stepsize太小，则收敛速度太慢，大大降低了模型训练速度。同时，不变的stepsize可能会使结果无法收敛到全局最优解，并可能停在局部最小值（局部最优解），当然很容易陷入到“鞍点”。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;NN6.PNG&#34; width = &#34;400&#34; height = &#34;400&#34; alt=&#34;Figure6&#34; /&gt;
&lt;p&gt;
图6. “鞍点”示意图
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;momentummomentum-optimizer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Momentum优化器（Momentum Optimizer）&lt;/h2&gt;
&lt;p&gt;Momentum优化器也可称为基于动量的优化算法，其中参数的更新会根据梯度的变化而变化：动量再梯度连续指向同一方向上时会增加，而在梯度方向变化时会减小；这样可以更快地收敛并减少震荡。公式表示为： &lt;span class=&#34;math display&#34;&gt;\[
v_t^{W} = \gamma \times v_{t-1}^{W} + \alpha \times \frac{\partial}{\partial W_{ij}}L(w,b)\\
W_{ij} = W_{ij} - v_t^{W}\\
v_t^{b} = \gamma \times v_{t-1}^{b} + \alpha \times \frac{\partial}{\partial b_i}L(w,b)\\
b_i = b_i - v_t^{b}
\]&lt;/span&gt; 其中，&lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;是动量更新值，通常取0.9。这样，基于Momentum的随机梯度下降可以更快地收敛，并减少陷入局部最优点的概率。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adagradadaptive-gradient-optimizer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adagrad优化器（Adaptive Gradient Optimizer）&lt;/h2&gt;
&lt;p&gt;Momentum优化器虽然加速了参数的更新并加速收敛，但存在缺点是没有对不同的参数进行区别对待。Adagrad优化器则基于这样的梯度优化思想：根据参数自适应地更新学习率（也为步长stepsize），对于不频繁更新的参数做较大更新，而对于频繁更新的参数做较小的更新。&lt;/p&gt;
&lt;p&gt;Adagrad对于每个参数&lt;span class=&#34;math inline&#34;&gt;\(\theta_{t,i}\)&lt;/span&gt;，在每个时间点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;使用不同的学习率。首先我们先考虑Adagrad的单参数情况，为了公式形式的整洁，我们记各个时间点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;的参数&lt;span class=&#34;math inline&#34;&gt;\(\theta_{t,i}\)&lt;/span&gt;下的目标函数梯度为&lt;span class=&#34;math inline&#34;&gt;\(g_{t,i}\)&lt;/span&gt;： &lt;span class=&#34;math display&#34;&gt;\[g_{t,i} = \frac{\partial}{\partial \theta_{t,i}}L(\theta_{t,i})\]&lt;/span&gt; 在Adagrad的更新规则中，我们会根据每个时间点&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;对每个参数&lt;span class=&#34;math inline&#34;&gt;\(\theta_{t+1,i}\)&lt;/span&gt;基于上次已经计算过的梯度&lt;span class=&#34;math inline&#34;&gt;\(\theta_{t,i}\)&lt;/span&gt;来修改步长： &lt;span class=&#34;math display&#34;&gt;\[\theta_{t+1,i} = \theta_{t,i} - \frac{\alpha}{\sqrt{G_{t,ii}+\epsilon}}\times g_{t,i}\]&lt;/span&gt; 其中，&lt;span class=&#34;math inline&#34;&gt;\(G_{t,ii}\in R^{d\times d}\)&lt;/span&gt;，&lt;span class=&#34;math inline&#34;&gt;\(G_{t,ii}\)&lt;/span&gt;是一个对角矩阵，其对角元素&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;时刻参数&lt;span class=&#34;math inline&#34;&gt;\(\theta_{t,i}\)&lt;/span&gt;的梯度平方和，&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;是一个光滑项，防止分母为0，通常取1e-8级别的数。另外，&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;使用默认值0.01。Adagrad有个缺点就是其分母实际上累积了梯度的平方，会使得步长（学习率）越来越小。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adadelta&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adadelta优化器&lt;/h2&gt;
&lt;p&gt;Adadelta是对Adagrad的改进，通过用过去计算的梯度平方的均值代替单纯的累加梯度平方，可以避免一味地降低步长。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;时刻的梯度平方均值表示为： &lt;span class=&#34;math display&#34;&gt;\[
E[g^2]_{t,i} = \gamma\times E[g^2]_{t-1,i} + (1-\gamma)\times g_{t,i}^2
\]&lt;/span&gt; 其中，&lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;和前面提到的Momentum优化器中的&lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;类似，通常取0.9。将累积梯度平方更改为梯度平方均值，可得到： &lt;span class=&#34;math display&#34;&gt;\[\theta_{t+1,i} = \theta_{t,i} - \frac{\alpha}{\sqrt{E_t+\epsilon}}\times g_{t,i}\]&lt;/span&gt; 另外，我们还想要变换分子的&lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;，将&lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;改为&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{E[\Delta\theta^2]_t+\epsilon}\)&lt;/span&gt;，便得到Adadelta的计算形式，以上内容可以总结为： &lt;span class=&#34;math display&#34;&gt;\[
E[g^2]_{t,i} = \gamma\times E[g^2]_{t-1,i} + (1-\gamma)\times g_{t,i}^2\\
E[\Delta\theta^2]_{t,i} = \gamma\times E[\Delta\theta^2]_{t-1,i} + (1-\gamma)\times \Delta\theta_{t,i}^2\\
\theta_{t+1,i} = \theta_{t,i} - \frac{\sqrt{E[\Delta\theta^2]_{t-1,i}+\epsilon}}{\sqrt{E[g^2]_{t,i}+\epsilon}}\times g_{t,i}
\]&lt;/span&gt; 显然，我们不再需要提前设定步长了。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adamadaptive-moment-estimation-optimizer-adam-optimizer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adam优化器（Adaptive Moment Estimation Optimizer, Adam Optimizer）&lt;/h2&gt;
&lt;p&gt;Adam也是个人名，圣经中说他是世上的第一个人类也是第一个男人，接着和夏娃结为夫妻，过上了幸福的生活…跑远了！回正题！其实Adam的全称中文是自适应矩估计，它不仅像Adadelta一样存储过去梯度平方&lt;span class=&#34;math inline&#34;&gt;\(v_t\)&lt;/span&gt;的平均值之外，还保留了像Momentum一样的保留了过去梯度&lt;span class=&#34;math inline&#34;&gt;\(m_t\)&lt;/span&gt;，其计算公式为： &lt;span class=&#34;math display&#34;&gt;\[
m_t = \beta_1m_{t-1} + (1-\beta_1)g_t\\
v_t = \beta_2v_{t-1} + (1-\beta_2)g_t^2
\]&lt;/span&gt; 由于&lt;span class=&#34;math inline&#34;&gt;\(m_t\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(v_t\)&lt;/span&gt;在计算上会存在偏差，所以进行了偏差校正： &lt;span class=&#34;math display&#34;&gt;\[
\hat{m_t} = \frac{m_t}{1-\beta_1^t}\\
\hat{v_t} = \frac{v_t}{1-\beta_2^t}
\]&lt;/span&gt; Adam的更新规则： &lt;span class=&#34;math display&#34;&gt;\[\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v_t}+\epsilon}}\times \hat{m_t}\]&lt;/span&gt; 其中，&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;通常取0.9，&lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt;通常取0.999，&lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;通常取1e-8。大多实验表明，Adam比其他自适应学习算法表现更优。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;算法表现效果&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;http://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif&#34; alt=&#34;Figure7&#34; /&gt;
&lt;p&gt;
图7. 不同优化器的随机梯度下降法在鞍点处的不同表现
&lt;/div&gt;
&lt;p&gt;注：动图来源于Sebastian Ruder的文章，由&lt;a href=&#34;https://twitter.com/alecrad&#34;&gt;Alec Radford&lt;/a&gt;制作。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;题外话（跳过这段吧~）&lt;/strong&gt;&lt;br /&gt;
在整理学习优化算法的时候，发现一件有趣的事。我边看着《Tensorflow入门与实战》的第四章边学习优化算法，同时网上边找找资料帮助理解。然而有趣的是我找到了一位业界大神Sebastian Ruder的主页，并看到了他在2016年1月6日写下了&lt;em&gt;An overview of gradient descent optimization algorithms&lt;/em&gt;。看着看着我发现手中拿的书竟然是电脑屏幕上显示的文章的中文版，我便好奇地寻找手中这本实战书的出版时间 —— 2018年2月17日。怪哉怪哉~再翻翻书，发现并无任何引用。算了，回归主题！（Reference选了日期比较前的S.R.大佬的文章作为引用）&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;backpropagation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;反向传播算法（Backpropagation）&lt;/h1&gt;
&lt;p&gt;反向传播算法是目前用来训练人工神经网络（Artificial Neural Network，ANN）的最常用且最有效的算法。首先我们先定义变量：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(v_i^{(l)}\)&lt;/span&gt;：第&lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;层的第&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;个节点的输入值，&lt;span class=&#34;math inline&#34;&gt;\(v_i^{(l)} = \sum_{j=0}^n w_{ij}^{(l)}a_j^{(l-1)} + b_i^{(l)}\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(a_i^{(l)}\)&lt;/span&gt;：第&lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;层的第&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;个节点的输出值，&lt;span class=&#34;math inline&#34;&gt;\(a_i^{(l)} = f(v_i^{(l)})\)&lt;/span&gt;，其中&lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt;是激活函数；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(w_{ij}^{(l)}\)&lt;/span&gt;：第&lt;span class=&#34;math inline&#34;&gt;\(l-1\)&lt;/span&gt;层的第&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;个节点到第&lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;层的第&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;个节点的权值；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(b_i^{(l)}\)&lt;/span&gt;：第&lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;层计算第&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;个节点的输入值时的偏置项的值；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;：神经网络的总层数；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt;：激活函数，例如sigmoid函数，tanh函数或者ReLu函数；&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(L(w,b)\)&lt;/span&gt;：整体损失函数，常用的损失函数为&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{2n}\sum_{i=1}^n(y_i-f(x_i))^2\)&lt;/span&gt;，其中n是样本的个数。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;反向传播计算过程的细节如下所示：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;参数初始化&lt;/strong&gt;&lt;br /&gt;
随机初始化网络中的各层的参数&lt;span class=&#34;math inline&#34;&gt;\(w_{ij}^{(l)}\)&lt;/span&gt;和&lt;span class=&#34;math inline&#34;&gt;\(b_i^{(l)}\)&lt;/span&gt;，且满足&lt;span class=&#34;math inline&#34;&gt;\(N(0,\ 0.01)\)&lt;/span&gt;分布的随机数；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;前向传播&lt;/strong&gt;&lt;br /&gt;

&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;NN3.PNG&#34; width = &#34;300&#34; height = &#34;300&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以图3中隐藏层的第一个节点（从上往下数第一个）为例，对于这个节点而言，其输入信号为： &lt;span class=&#34;math display&#34;&gt;\[v_1^{(2)} = a_1^{(1)}\times w_{11}^{(2)} + a_2^{(1)}\times w_{12}^{(2)} + a_3^{(1)}\times w_{13}^{(2)} + b_1^{(2)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;同理，我们可以得到该层的其他节点的计算： &lt;span class=&#34;math display&#34;&gt;\[
v_2^{(2)} = a_1^{(1)}\times w_{21}^{(2)} + a_2^{(1)}\times w_{22}^{(2)} + a_3^{(1)}\times w_{23}^{(2)} + b_2^{(2)}\\
v_3^{(2)} = a_1^{(1)}\times w_{31}^{(2)} + a_2^{(1)}\times w_{32}^{(2)} + a_3^{(1)}\times w_{33}^{(2)} + b_3^{(2)}\\
v_4^{(2)} = a_1^{(1)}\times w_{41}^{(2)} + a_2^{(1)}\times w_{42}^{(2)} + a_3^{(1)}\times w_{43}^{(2)} + b_4^{(2)}\\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;若用矩阵形式进行表达： &lt;span class=&#34;math display&#34;&gt;\[V^{(2)} = A^{(1)}\times W^{(2)} + B^{(2)}\]&lt;/span&gt; 其中， &lt;span class=&#34;math display&#34;&gt;\[
V^{(2)} = (v_1^{(2)}, v_2^{(2)}, v_3^{(2)}, v_4^{(2)})\\
A^{(1)} = (a_1^{(1)}, a_2^{(1)}, a_3^{(1)})\\
W^{(2)} = \begin{pmatrix}
w_{11}^{(2)} &amp;amp; w_{21}^{(2)} &amp;amp; w_{31}^{(2)} &amp;amp; w_{41}^{(2)}\\
w_{12}^{(2)} &amp;amp; w_{22}^{(2)} &amp;amp; w_{32}^{(2)} &amp;amp; w_{42}^{(2)}\\
w_{13}^{(2)} &amp;amp; w_{23}^{(2)} &amp;amp; w_{33}^{(2)} &amp;amp; w_{43}^{(2)}
\end{pmatrix}\\
B^{(2)} = (b_1^{(2)}, b_2^{(2)}, b_3^{(2)}, b_4^{(2)})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;再经过激活函数（非线性函数）变换后得到： &lt;span class=&#34;math display&#34;&gt;\[A^{(2)} = f(V^{(2)})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;同理，经由 &lt;span class=&#34;math display&#34;&gt;\[
V^{(3)} = A^{(2)}\times W^{(3)} + B^{(3)}\\
A^{(3)} = f(V^{(3)})
\]&lt;/span&gt; 可以得到最终输出。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;反向传播&lt;/strong&gt;&lt;br /&gt;
首先对于最后一层节点的偏导数，其实我们很容易得到，我们定义神经网络总共有&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;层，对于最后一层即第&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;层（输出层），根据偏导数的定义：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\delta_i^{(K)} =&amp;amp;\ \frac{\partial}{\partial v_i^{(K)}}L(w,b)\\
=&amp;amp;\ \frac{\partial L(w,b)}{\partial a_i^{(K)}}\times \frac{\partial a_i^{(K)}}{\partial v_i^{(K)}}\\
=&amp;amp;\ \frac{\partial L(w,b)}{\partial a_i^{(K)}}\times f&amp;#39;(v_i^{(K)})
\end{align}
\tag{4}
\]&lt;/span&gt; 明显的是，&lt;span class=&#34;math inline&#34;&gt;\(a_i^{(K)}\)&lt;/span&gt;是最后一层（即输出层）的输出值，&lt;span class=&#34;math inline&#34;&gt;\(f&amp;#39;(v_i^{(K)})\)&lt;/span&gt;则是激活函数对&lt;span class=&#34;math inline&#34;&gt;\(v_i^{(K)}\)&lt;/span&gt;的导数。&lt;/p&gt;
&lt;p&gt;对（4）式进一步推导可以得到： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\delta_i^{(K)} =&amp;amp;\ \frac{\partial}{\partial a_i^{(K)}}\Big[\frac{1}{2n_K}\sum_{j=1}^{n_K}\Big(y_j-a_j^{(K)}\Big)^2\Big]\times f&amp;#39;(v_i^{(K)})\\
=&amp;amp;\ -\frac{1}{n_k}(y_i-a_i^{(K)})\times f&amp;#39;(v_i^{(K)})
\end{align}
\]&lt;/span&gt; 其中，&lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;是样本对应的正确值，&lt;span class=&#34;math inline&#34;&gt;\(n_K\)&lt;/span&gt;是第K层节点个数。&lt;/p&gt;
&lt;p&gt;因此，可得到最后一层（第K层）的计算公式： &lt;span class=&#34;math display&#34;&gt;\[
\delta_i^{(K)} = -\frac{1}{n_k}(y_i-a_i^{(K)})\times f&amp;#39;(v_i^{(K)})
\tag{5}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;那么对于第&lt;span class=&#34;math inline&#34;&gt;\(K-1\)&lt;/span&gt;层的偏导数，可以根据第&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;层的计算出来： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\delta_i^{(K-1)} =&amp;amp;\ \frac{\partial}{\partial v_i^{(K-1)}}L(w,b)\\
=&amp;amp;\ \frac{\partial}{\partial v_i^{(K-1)}}\Big[\frac{1}{2n_K}\sum_{j=1}^{n_K}\Big(y_j-a_j^{(K)}\Big)^2\Big]\\
=&amp;amp;\ \frac{1}{2n_K}\Big[\frac{\partial}{\partial v_i^{(K-1)}} \sum_{j=1}^{n_K}\Big(y_j-f(v_j^{(K)})\Big)^2\Big]
\end{align}
\tag{6}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;利用连续函数的求导和求和顺序可互换，（6）式可以推得： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\delta_i^{(K-1)} =&amp;amp;\ -\frac{1}{n_K} \sum_{j=1}^{n_K} \Big[(y_j-f(v_j^{(K)}))\times \frac{\partial}{\partial v_i^{(K-1)}}f(v_j^{(K)})\Big]\\
=&amp;amp;\ -\frac{1}{n_K} \sum_{j=1}^{n_K} \Big[(y_j-f(v_j^{(K)}))\times \frac{\partial f(v_j^{(K)})}{\partial v_i^{(K)}}\times \frac{\partial v_i^{(K)}}{\partial v_i^{(K-1)}} \Big]\\
=&amp;amp;\ \sum_{j=1}^{n_K} \Big[-\frac{1}{n_K} (y_j-f(v_j^{(K)}))\times f&amp;#39;(v_j^{(K)})\times \frac{\partial v_i^{(K)}}{\partial v_i^{(K-1)}} \Big]
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;联合（5）式，由（6）式可以得到： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\delta_i^{(K-1)} =&amp;amp;\ \sum_{j=1}^{n_K} \Big[ \delta_i^{(K)}\times \frac{\partial v_i^{(K)}}{\partial v_i^{(K-1)}} \Big]\\
=&amp;amp;\ \sum_{j=1}^{n_K}\Bigg[ \delta_i^{(K)}\times \frac{\partial }{\partial v_i^{(K-1)}}\Big[ \sum_{m=0}^{n_{K-1}}a_m^{(K-1)}\times w_{jm}^{(K)}+b_j^{(K)} \Big]\Bigg]\\
=&amp;amp;\ \sum_{j=1}^{n_K}\Bigg[ \delta_i^{(K)}\times \frac{\partial }{\partial v_i^{(K-1)}}\Big[ \sum_{m=0}^{n_{K-1}}f(v_m^{(K-1)})\times w_{jm}^{(K)}+b_j^{(K)} \Big]\Bigg]\\
=&amp;amp;\ \sum_{j=1}^{n_K}\Bigg[ \delta_i^{(K)}\times f&amp;#39;(v_i^{(K-1)})\times w_{ji}^{(K)} \Bigg]\\
=&amp;amp;\ \Bigg[\sum_{j=1}^{n_K}\Big[ \delta_i^{(K)}\times w_{ji}^{(K)} \Big]\Bigg] \times f&amp;#39;(v_i^{(K-1)})
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因此，可得到第K-1层的计算公式： &lt;span class=&#34;math display&#34;&gt;\[
\delta_i^{(K-1)} = \Bigg[\sum_{j=1}^{n_K}\Big[ \delta_i^{(K)}\times w_{ji}^{(K)} \Big]\Bigg] \times f&amp;#39;(v_i^{(K-1)})
\tag{7}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;同理，用&lt;span class=&#34;math inline&#34;&gt;\(K-2\)&lt;/span&gt;替换&lt;span class=&#34;math inline&#34;&gt;\(K-1\)&lt;/span&gt;，用&lt;span class=&#34;math inline&#34;&gt;\(K-1\)&lt;/span&gt;替换&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;，则可计算第&lt;span class=&#34;math inline&#34;&gt;\(K-2\)&lt;/span&gt;层的偏导数。 &lt;span class=&#34;math display&#34;&gt;\[
\delta_i^{(K-2)} = \Bigg[\sum_{j=1}^{n_{K-1}}\Big[ \delta_i^{(K-1)}\times w_{ji}^{(K-1)} \Big]\Bigg] \times f&amp;#39;(v_i^{(K-2)})
\tag{7}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;同样的，可以根据（7）式计算得到网络中所有节点的偏导数。&lt;/p&gt;
&lt;p&gt;回归我们的参数迭代公式： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
w_{ij}^{(l)} =&amp;amp;\ w_{ij}^{(l)} - \alpha\times \frac{\partial}{\partial w_{ij}^{(l)}}L(w,b)\\
b_i^{(l)} =&amp;amp;\ b_i^{(l)} - \alpha\times \frac{\partial}{\partial b_i^{(l)}}L(w,b)
\end{align}
\tag{8}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;对于后面的偏导数部分，我们可以加以处理，对于参数&lt;span class=&#34;math inline&#34;&gt;\(w_{ij}^{(l)}\)&lt;/span&gt;部分： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\frac{\partial L(w,b)}{\partial w_{ij}^{(l)}} =&amp;amp;\ \frac{\partial L(w,b)}{\partial v_i^{(l)}}\times \frac{\partial  v_i^{(l)}}{\partial w_{ij}^{(l)}}\\
=&amp;amp;\ \delta_i^{(l)}\times \frac{\partial v_i^{(l)}}{\partial w_{ij}^{(l)}}\\
=&amp;amp;\ \delta_i^{(l)}\times \frac{\partial }{\partial w_{ij}^{(l)}}\Big[\sum_{j=0}^{n_{l-1}}a_j^{(l-1)}\times w_{ij}^{(l)}+b_i^{(l)}\Big]\\
=&amp;amp;\ \delta_i^{(l)}\times a_j^{(l-1)}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;对于参数&lt;span class=&#34;math inline&#34;&gt;\(b_i^{(l)}\)&lt;/span&gt;部分： &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\frac{\partial L(w,b)}{\partial b_i^{(l)}} =&amp;amp;\ \frac{\partial L(w,b)}{\partial v_i^{(l)}}\times \frac{\partial v_i^{(l)}}{\partial b_i^{(l)}}\\
=&amp;amp;\ \delta_i^{(l)}\times \frac{\partial}{\partial b_i^{(l)}}\Big[\sum_{j=0}^{n_{l-1}}a_j^{(l-1)}\times w_{ij}^{(l)}+b_i^{(l)}\Big]\\
=&amp;amp;\ \delta_i^{(l)}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;因此，由（8）式可以推得 &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
w_{ij}^{(l)} =&amp;amp;\ w_{ij}^{(l)} - \alpha\times \delta_i^{(l)}\times a_j^{(l-1)}\\
b_i^{(l)} =&amp;amp;\ b_i^{(l)} - \alpha\times \delta_i^{(l)}
\end{align}
\tag{8}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Over！反向传播算法到此结束！&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;http://ruder.io/optimizing-gradient-descent/index.html&#34;&gt;Sebastian Ruder. An overview of gradient descent optimization algorithms&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>关于Rmarkdown生成中文内容pdf的那些事</title>
      <link>/post/%E5%85%B3%E4%BA%8Ermarkdown%E7%94%9F%E6%88%90%E4%B8%AD%E6%96%87%E5%86%85%E5%AE%B9pdf%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B/</link>
      <pubDate>Wed, 02 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/%E5%85%B3%E4%BA%8Ermarkdown%E7%94%9F%E6%88%90%E4%B8%AD%E6%96%87%E5%86%85%E5%AE%B9pdf%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B/</guid>
      <description>


&lt;p&gt;缘于某人用Rmarkdown搞不出中文内容的pdf而引发一场激战之下，TT只能忍气吞声继续走上帮人帮到底的道路，于是网上搜出一大堆关于Rmarkdown生成中文pdf的麻烦事。无奈，众里寻它千百度，最终发现解决问题的YAML模板及相关的解决方案，怕在接下来的日子可能遭受同样的折磨，并以扩充Blog文章为前提，书写此文。&lt;/p&gt;
首先，让我们先在RStudio菜单栏选择Tools并点击Global Options。选择Sweaver并按图勾选，最后点OK~
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;RMD2PDF.PNG&#34; width = &#34;500&#34; height = &#34;470&#34; alt=&#34;Figure1&#34; /&gt;
&lt;p&gt;
Figure1. 可爱的Global Options窗口
&lt;/div&gt;
&lt;p&gt;然后.Rmd文件中的YAML模板如下设置：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;--- 
title: &amp;quot;我是一个Test文档的标题&amp;quot; 
author: &amp;quot;我是一个Test文档的作者名称&amp;quot; 
date: &amp;quot;我是一个Test文档的写作日期&amp;quot; 
CJKmainfont: Microsoft YaHei
output:
  pdf_document:
    includes:
      header-includes:
        - \usepackage{xeCJK}
    keep_tex: yes
    latex_engine: xelatex
---&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;注：介个模板用上了大微软的雅黑字体，如若想修改，那请继续摸索摸索。（T.T累了不想改了~）&lt;/p&gt;
&lt;p&gt;搞定！Over！愿你的探索之路不与我一样艰辛((٩(//̀Д/́/)۶))&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;课外补充：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;关于Rmarkdown to pdf的美好世界&lt;br /&gt;
如果你也被这样的问题所困扰，那么你会发现R界的大佬&lt;a href=&#34;https://yihui.name/&#34;&gt;谢益辉&lt;/a&gt;搞了个包叫&lt;a href=&#34;https://github.com/rstudio/rticles&#34;&gt;rticles&lt;/a&gt;，直接提供template给你写中文文档。然而，无奈Tex世界的混乱，还是遇到奇奇怪怪的乱七八糟的问题，但是大佬说大家用&lt;a href=&#34;https://yihui.name/tinytex/&#34;&gt;TinyTex&lt;/a&gt;吧，那将提供Rmarkdown to pdf的一片美好世界。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Variational Auto-Encoder</title>
      <link>/post/variational-auto-encoder/</link>
      <pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/variational-auto-encoder/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;We assume the observed variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is a random sample from an &lt;strong&gt;unknown underlying process&lt;/strong&gt;, whose &lt;strong&gt;true distribution&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(p^*(x)\)&lt;/span&gt; is &lt;strong&gt;unknown&lt;/strong&gt;. We attempt to approximate this underlying process with a chosen model &lt;span class=&#34;math inline&#34;&gt;\(p_{\theta}(x)\)&lt;/span&gt;, with parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[x\sim p_{\theta}(x)\]&lt;/span&gt; We always talk about learning like &lt;strong&gt;Deep Learning&lt;/strong&gt;, and actually the &lt;strong&gt;learning&lt;/strong&gt; is the process of searching for a value of the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; in model &lt;span class=&#34;math inline&#34;&gt;\(p_{\theta}(x)\)&lt;/span&gt;, which can approximate the true distribution of the data, denoted by &lt;span class=&#34;math inline&#34;&gt;\(p^*(x)\)&lt;/span&gt;. In other words, &lt;span class=&#34;math display&#34;&gt;\[p_{\theta}(x)\approx p^*(x)\]&lt;/span&gt; Latent variables are variables that are part of the model, but which we don’t observe, and are therefore not part of the dataset. We typically use &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; to denote such latent variables.&lt;/p&gt;
&lt;p&gt;The marginal distribution over the observed variables &lt;span class=&#34;math inline&#34;&gt;\(p_{\theta}(x)\)&lt;/span&gt;, is given by: &lt;span class=&#34;math display&#34;&gt;\[
p_{\theta}(x) = \int p_{\theta}(x,z) dz = \int p_{\theta}(z) p_{\theta}(x|z) dz
\]&lt;/span&gt; We use the term &lt;strong&gt;deep latent variable model (DLVM)&lt;/strong&gt; to denote a &lt;strong&gt;latent variable model&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(p_{\theta}(x,z)\)&lt;/span&gt; whose distributions are parameterized by neural networks.&lt;/p&gt;
&lt;div id=&#34;example-dlvm-for-multivariate-bernoulli-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example DLVM for multivariate Bernoulli data&lt;/h2&gt;
&lt;p&gt;A simple example DLVM for binary data &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, with a spherical Gaussian latent space, and a factorized Bernoulli obervation model &lt;span class=&#34;math display&#34;&gt;\[
p(z) = \mathcal{N}(0,\text{I})\\
\text{p} = \text{DecoderNeuralNet}_{\theta}(z)\\
\begin{align}
\log p(x|z) =&amp;amp; \sum_{j=1}^J \log p(x_j|z) = \sum_{j=1}^J \text{Bernoulli}(x_j,p_j)\\
=&amp;amp; \sum_{j=1}^Jx_j \log p_j + (1-x_j)\log (1-p_j)
\end{align}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(0\leq p_j\leq 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Therefore, we easily get &lt;span class=&#34;math inline&#34;&gt;\(p(x,z) = p(x|z)\times p(z)\)&lt;/span&gt; by the term we described above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some problem&lt;/h2&gt;
&lt;p&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(p_{\theta}(x,z)\)&lt;/span&gt; is efficient to compute. Since the intractability of &lt;span class=&#34;math inline&#34;&gt;\(p_{\theta}(x)\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(p_{\theta}(x) = \int p_{\theta}(x,z) dz\)&lt;/span&gt;), the posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(p_{\theta}(z|x)\)&lt;/span&gt; is also intractable, because their densities are related through the basic identity: &lt;span class=&#34;math display&#34;&gt;\[p_{\theta}(z|x) = \frac{p_{\theta}(x,z)}{p_{\theta}(x)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;How can we perform efficient approximate posterior inference and efficient approximate maximum likelihood estimation in deep latent variable models, in the presence of large datasets?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;similar-method-like-dlvm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Similar method like DLVM&lt;/h2&gt;
&lt;p&gt;We introduce a parametric inference model &lt;span class=&#34;math inline&#34;&gt;\(q_{\phi}(z|x)\)&lt;/span&gt; (also called as &lt;strong&gt;encoder&lt;/strong&gt;)in this part and we try to optimize the variational parameters &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; such that: &lt;span class=&#34;math display&#34;&gt;\[q_{\phi}(z|x) \approx p_{\theta}(z|x)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Similar to &lt;strong&gt;DLVM&lt;/strong&gt;, the distribution of &lt;span class=&#34;math inline&#34;&gt;\(q_{\phi}(z|x)\)&lt;/span&gt; also can be parameterized using deep neural networks. In this case, the variational parameters &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; include the weights and biases of the neural network. For example: &lt;span class=&#34;math display&#34;&gt;\[
(\mu,\sigma) = \text{EncoderNeuralNet}_{\phi}(x)\\
q_{\phi}(z|x) = \mathcal{N}(\mu,\text{diag}(\sigma^2))
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;evidence-lower-bound-elbo-and-kl-divergence&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Evidence lower bound (ELBO) and KL divergence&lt;/h1&gt;
&lt;p&gt;The optimization objective of the variational autoencoder is the &lt;strong&gt;evidence lower bound&lt;/strong&gt;, abbreviated as ELBO. An alternative term for this objective is &lt;strong&gt;variational lower bound&lt;/strong&gt;. We can obtain the lower bound by: &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\log p_{\theta}(x) =&amp;amp;\ \mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x)] = \mathbb{E}_{q_{\phi}(z|x)} \Big[\log\Big[ \frac{p_{\theta}(x,z)}{p_{\theta}(z|x)}\Big]\Big]\\
=&amp;amp;\ \mathbb{E}_{q_{\phi}(z|x)}\Big[\log\Big[\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}\frac{q_{\phi}(z|x)}{p_{\theta}(z|x)}\Big]\Big]\\
=&amp;amp;\ \mathbb{E}_{q_{\phi}(z|x)}\Big[\log\Big[\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}\Big]\Big] + \mathbb{E}_{q_{\phi}(z|x)}\Big[\log\Big[\frac{q_{\phi}(z|x)}{p_{\theta}(z|x)}\Big]\Big]\\
=&amp;amp;\ \mathcal{L}_{\theta,\phi}(x) + KL[q_{\phi}(z|x)||p_{\theta}(z|x)]\\
\geq&amp;amp;\ \mathcal{L}_{\theta,\phi}(x)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;kl-divergence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;KL divergence&lt;/h2&gt;
&lt;p&gt;We want to find a good probability distribution &lt;span class=&#34;math inline&#34;&gt;\(q_{\phi}(z|x)\)&lt;/span&gt; (‘good’ means the efficient computation) to approximate the true posterior probability &lt;span class=&#34;math inline&#34;&gt;\(p_{\theta}(z|x)\)&lt;/span&gt;, where the &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is the latent variable. &lt;strong&gt;KL divergence&lt;/strong&gt; can measure the distance well between these two distribution. For the discrete probability situation, the &lt;strong&gt;KL divergence&lt;/strong&gt; can be written as &lt;span class=&#34;math display&#34;&gt;\[KL(q||p) = \sum q(x)\log \frac{q(x)}{p(x)}\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;example-of-1-dimension-guassian-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example of 1-dimension Guassian distribution&lt;/h3&gt;
&lt;p&gt;Supposed that we have two random variables &lt;span class=&#34;math inline&#34;&gt;\(x_1, x_2\)&lt;/span&gt; w.r.t the guassian distribution &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}(\mu_1,\sigma_1^2),\mathcal{N}(\mu_2,\sigma_2^2)\)&lt;/span&gt; respectively.&lt;/p&gt;
&lt;p&gt;Recall that the density function of guassian distribution &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{N}(\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\]&lt;/span&gt; Then &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
KL(p_1,p_2) =&amp;amp;\ \int p_1(x)\log \frac{p_1(x)}{p_2(x)}dx\\
=&amp;amp;\ \int p_1(x)(\log p_1(x) - \log p_2(x))dx\\
=&amp;amp;\ \int p_1(x)(\log \frac{1}{\sqrt{2\pi\sigma_1^2}}e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} - \log \frac{1}{\sqrt{2\pi\sigma_2^2}}e^{-\frac{(x-\mu_2)^2}{2\sigma_2^2}})dx\\
=&amp;amp;\ \int p_1(x)(-\log \sqrt{2\pi \sigma_1^2} - \frac{(x-\mu_1)^2}{2\sigma_1^2} + \log \sqrt{2\pi \sigma_2^2} + \frac{(x-\mu_2)^2}{2\sigma_2^2})dx\\
=&amp;amp;\ \int p_1(x)(-\frac{1}{2}\log2\pi-\log\sigma_1+\frac{1}{2}\log2\pi+\log\sigma_2 - (\frac{(x-\mu_1)^2}{2\sigma_1^2}-\frac{(x-\mu_2)^2}{2\sigma_2^2}))dx\\
=&amp;amp;\ \int p_1(x)(\log\frac{\sigma_2}{\sigma_1} - (\frac{(x-\mu_1)^2}{2\sigma_1^2}-\frac{(x-\mu_2)^2}{2\sigma_2^2}))dx\\
=&amp;amp;\ \int p_1(x)(\log\frac{\sigma_2}{\sigma_1})dx + \int p_1(x)(\frac{(x-\mu_2)^2}{2\sigma_2^2})dx - \int p_1(x)(\frac{(x-\mu_1)^2}{2\sigma_1^2})dx\\
=&amp;amp;\ \log\frac{\sigma_2}{\sigma_1} + \frac{1}{2\sigma_2^2}\int p_1(x)(x-\mu_2)^2dx - \frac{1}{2\sigma_1^2}\int p_1(x)(x-\mu_1)^2dx
\end{align}
\]&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 = \int p_1(x)(x-\mu_1)^2dx\)&lt;/span&gt;, then &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
KL(p_1,p_2) =&amp;amp;\ \log\frac{\sigma_2}{\sigma_1} + \frac{1}{2\sigma_2^2}\int p_1(x)(x-\mu_2)^2dx - \frac{1}{2}\\
=&amp;amp;\ \log\frac{\sigma_2}{\sigma_1} + \frac{1}{2\sigma_2^2}\int p_1(x)(x - \mu_1 + \mu_1 - \mu_2)^2dx - \frac{1}{2}\\
=&amp;amp;\ \log\frac{\sigma_2}{\sigma_1} + \frac{1}{2\sigma_2^2}[\int p_1(x)(x-\mu_1)^2dx+\int p_1(x)(\mu_1-\mu_2)^2dx+2\int p_1(x)(x-\mu_1)(\mu_1-\mu_2)dx] - \frac{1}{2}\\
\end{align}
\]&lt;/span&gt; We know that &lt;span class=&#34;math inline&#34;&gt;\(\mu_1 = \int x p_1(x)dx\)&lt;/span&gt;, so &lt;span class=&#34;math inline&#34;&gt;\(2\int p_1(x)(x-\mu_1)(\mu_1-\mu_2)dx = 2(\mu_1-\mu_2)[\int xp_1(x)dx - \mu_1] = 0\)&lt;/span&gt;, thus &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
KL(p_1,p_2) =&amp;amp;\ \log\frac{\sigma_2}{\sigma_1} + \frac{1}{2\sigma_2^2}[\int p_1(x)(x-\mu_1)^2dx + (\mu_1-\mu_2)^2] - \frac{1}{2}\\
=&amp;amp;\ \log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2+(\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac{1}{2}\\
\end{align}
\]&lt;/span&gt; If we suppose that the &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}(\mu_2,\sigma_2^2)\)&lt;/span&gt; is standard guassian distribution, &lt;span class=&#34;math inline&#34;&gt;\(\mu_2 = 0, \sigma_2^2 = 1\)&lt;/span&gt;, so &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
KL =&amp;amp;\ \log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2+(\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac{1}{2}\\
=&amp;amp;\ \log1 - \log\sigma_1 + \frac{\sigma_1^2+(\mu_1 - 0)^2}{2} - \frac{1}{2}\\
=&amp;amp;\ -\log\sigma_1 + \frac{\sigma_1^2+\mu_1^2}{2} - \frac{1}{2}\\
\end{align}
\]&lt;/span&gt; We expect that the &lt;strong&gt;KL&lt;/strong&gt; can be as small as possible, we calculate its derivative, then we get &lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial KL}{\partial \sigma_1} = -\frac{1}{\sigma_1} + \sigma_1\\
\frac{\partial KL}{\partial \mu_1} = \mu_1
\]&lt;/span&gt; We let them equal to zero, then we get &lt;span class=&#34;math display&#34;&gt;\[
-\frac{1}{\sigma_1} + \sigma_1 = 0 \Rightarrow \sigma_1 = 1\\
\mu_1 = 0
\]&lt;/span&gt; which means that the &lt;strong&gt;KL&lt;/strong&gt; becomes the minimum when &lt;span class=&#34;math inline&#34;&gt;\(x_2 \sim \mathcal{N}(0,1)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;minimization-of-kl-divergence&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Minimization of KL divergence&lt;/h3&gt;
&lt;p&gt;If we want to use the ELBO to approximate the log-likelihood, then we need to minimize the &lt;span class=&#34;math inline&#34;&gt;\(D_{KL}[q_{\phi}(z|x)||p_{\theta}(z|x)]\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;From &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
KL[q_{\phi}(z|x)||p_{\theta}(z|x)] =&amp;amp;\ \int q_{\phi}(z|x) \log \frac{q_{\phi}(z|x)}{p_{\theta}(z|x)} dz\\
=&amp;amp;\ \int q_{\phi}(z|x) [\log q_{\phi}(z|x) - \log  p_{\theta}(z|x)]dz
\end{align}
\]&lt;/span&gt; and Bayesian formula &lt;span class=&#34;math display&#34;&gt;\[
p_{\theta}(z|x) = \frac{p_{\theta}(x|z)*p_{\theta}(z)}{p_{\theta}(x)}
\]&lt;/span&gt; We can get &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
KL[q_{\phi}(z|x)||p_{\theta}(z|x)] =&amp;amp;\ \int q_{\phi}(z|x) [\log q_{\phi}(z|x) - \log  \frac{p_{\theta}(x|z)*p_{\theta}(z)}{p_{\theta}(x)}]dz\\
=&amp;amp;\ \int q_{\phi}(z|x) [\log q_{\phi}(z|x) -\log  p_{\theta}(x|z) - \log p_{\theta}(z) + \log p_{\theta}(x)]dz\\
=&amp;amp;\  \int q_{\phi}(z|x) [\log q_{\phi}(z|x) -\log  p_{\theta}(x|z) - \log p_{\theta}(z)]dz + \log p_{\theta}(x)\\
=&amp;amp;\ KL[q_{\phi}(z|x)||p_{\theta}(z)] - \int q_{\phi}(z|x) \log p_{\theta}(x|z)dz + \log p_{\theta}(x)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When the data &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; are provided, then last term in the right side &lt;span class=&#34;math inline&#34;&gt;\(\log p_{\theta}(x)\)&lt;/span&gt; becomes constant, and we wish the &lt;span class=&#34;math inline&#34;&gt;\(D_{KL}[q_{\phi}(z|x)||p_{\theta}(z|x)]\)&lt;/span&gt; can be as small as possible.&lt;/p&gt;
&lt;p&gt;Thus, the optimization problem becomes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\min\limits_x D_{KL}[q_{\phi}(z|x)||p_{\theta}(z)]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\max\limits_x \int q_{\phi}(z|x) \log p_{\theta}(x|z)dz\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It also can be written as &lt;span class=&#34;math display&#34;&gt;\[\min_x KL[q_{\phi}(z|x)||p_{\theta}(z)] - \mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Although we can obtain our new optimization problem, the problem actually is difficult to solve, and thus we would like to straightly optimize the &lt;strong&gt;ELBO&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;variational-auto-encoder&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Variational Auto-Encoder&lt;/h1&gt;
&lt;div id=&#34;connection-with-em&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Connection with EM&lt;/h2&gt;
&lt;p&gt;For standard EM algorithms, the posterior is often known, &lt;span class=&#34;math inline&#34;&gt;\(q_{\phi}(z|x) = q(z|x) = p_{\theta}(z|x)\)&lt;/span&gt;, then the &lt;strong&gt;KL&lt;/strong&gt; term becomes zero, so &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\log p_{\theta}(x) = \mathcal{L}_{\theta}(x) =&amp;amp;\ \mathbb{E}_{q(z|x)}\Big[\log\Big[\frac{p_{\theta}(x,z)}{q(z|x)}\Big]\Big]\\
=&amp;amp;\ \mathbb{E}_{q(z|x)}[\log p_{\theta}(x,z)] - \mathbb{E}_{q(z|x)}[\log q(z|x)]
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above step is indeed the E-step in the standard EM algorithm. The M-step would be &lt;span class=&#34;math display&#34;&gt;\[\theta_{\text{new}} = \arg \max_\theta L_{\theta}(x)\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stochastic-gradient-based-optimization-of-the-elbo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stochastic gradient-based optimization of the ELBO&lt;/h2&gt;
&lt;p&gt;From the &lt;strong&gt;Evidence lower bound (ELBO)&lt;/strong&gt; part, we obtain the inequality fomula as &lt;span class=&#34;math inline&#34;&gt;\(\log p_{\theta}(x) \geq \mathcal{L}_{\theta,\phi}(x)\)&lt;/span&gt;. Recall that EM algorithm is one of the special case of Minorize-Maximization (MM) algorithm, and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}_{\theta,\phi}(x)\)&lt;/span&gt; can be considered as the surrogate function in MM algorithm, so we would get the maximum of log-likelihood by maximizing the lower bound.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;VAE1.PNG&#34; width = &#34;300&#34; height = &#34;300&#34; alt=&#34;Figure1&#34; /&gt;
&lt;p&gt;
Figure1. The EM algorithm involves alternatel computing a lower bound on the log likelihood for the current parameter values and then maximizing this bound to obtain the new parameter values.
&lt;/div&gt;
&lt;p&gt;Given a dataset with i.i.d. data, the ELBO objective is the sum (or average) of individual-datapoint ELBO’s: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L}_{\theta,\phi}(\mathcal{D})=\sum_{x\in\mathcal{D}}\mathcal{L}_{\theta,\phi}(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Apparantly, the individual-datapoint ELBO and its gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla_{\theta,\phi}\mathcal{L}_{\theta,\phi}(x)\)&lt;/span&gt; is intractable in general.&lt;/p&gt;
&lt;div id=&#34;the-sgvb-estimator-and-auto-encoding-vb-aevb-algorithm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The SGVB estimator and Auto-Encoding VB (AEVB) algorithm&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Reparamterization trick&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; be a continuous random variable and &lt;span class=&#34;math inline&#34;&gt;\(z\sim q_{\phi}(z|x)\)&lt;/span&gt; be some conditional distribution. It is often possible to express the random variable &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; as a deterministic variable &lt;span class=&#34;math inline&#34;&gt;\(z=g_{\phi}(\epsilon,x)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is an auxiliary variable with independent marginal &lt;span class=&#34;math inline&#34;&gt;\(p(\epsilon)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We suppose that the recognition model &lt;span class=&#34;math inline&#34;&gt;\(q_{\phi}(z|x)\)&lt;/span&gt; can be written as some differentiable transformation of another randome variable &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(g_{\phi}(\epsilon,x)\)&lt;/span&gt;, and we can form a simple Monte Carlo estimator &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\mathcal{L}}_{\theta,\phi}(x)\)&lt;/span&gt; of the individual-datapoint ELBO: &lt;span class=&#34;math display&#34;&gt;\[
\epsilon \sim p(\epsilon)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;so we can get our generic Stochastic Gradient Variational Bayes (SGVB) estimator from the lower bound &lt;span class=&#34;math display&#34;&gt;\[
\tilde{\mathcal{L}}_{\theta,\phi}^{A}(x^{(i)}) = \frac{1}{L}\sum_{l=1}^L[\log p_{\theta}(x^{(i)},z^{(i,l)}) - \log q_{\phi}(z^{(i,l)}|x^{(i)})]
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(z^{(i,l)} = g_{\phi}(\epsilon^{(i,l)},x^{(i)}),\quad \epsilon^{(i,l)} \sim p(\epsilon)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We try to decompose the &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}_{\theta,\phi}(x)\)&lt;/span&gt;, and we get &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\mathcal{L}_{\theta,\phi}(x^{(i)}) =&amp;amp;\ \mathbb{E}_{q_{\phi}(z|x^{(i)})}\Big[\log\frac{p_{\theta}(x^{(i)},z)}{q_{\phi}(z|x^{(i)})}\Big]\\
=&amp;amp;\ \mathbb{E}_{q_{\phi}(z|x^{(i)})}\Big[\log\frac{p_{\theta}(x^{(i)}|z)p_{\theta}(z)}{q_{\phi}(z|x^{(i)})}\Big]\\
=&amp;amp;\ \mathbb{E}_{q_{\phi}(z|x^{(i)})}\Big[\log p_{\theta}(x^{(i)}|z)-\log\frac{q_{\phi}(z|x^{(i)})}{p_{\theta}(z)}\Big]\\
=&amp;amp;\ \mathbb{E}_{q_{\phi}(z|x^{(i)})}[\log p_{\theta}(x^{(i)}|z)]-KL[q_{\phi}(z|x^{(i)})||p_{\theta}(z)]
\end{align}
\]&lt;/span&gt; The final equality showed the same object result (In Minimization of KL divergence section).&lt;/p&gt;
&lt;p&gt;With this equality, we also can obtain another estimator &lt;span class=&#34;math display&#34;&gt;\[
\tilde{\mathcal{L}}_{\theta,\phi}^{B}(x^{(i)}) = \mathbb{E}_{q_{\phi}(z|x^{(i)})}[\log p_{\theta}(x^{(i)}|z)]-KL[q_{\phi}(z|x^{(i)})||p_{\theta}(z)]\\
=\frac{1}{L}\sum_{l=1}^L\log p_{\theta}(x^{(i)}|z^{(i,l)})-KL[q_{\phi}(z|x^{(i)})||p_{\theta}(z)]
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(z^{(i,l)} = g_{\phi}(\epsilon^{(i,l)},x^{(i)}),\quad \epsilon^{(i,l)} \sim p(\epsilon)\)&lt;/span&gt;. Given multiple datapoints from a dataset &lt;span class=&#34;math inline&#34;&gt;\(\text{X}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L}_{\theta,\phi}(\text{X})\simeq\tilde{\mathcal{L}}_{\theta,\phi}^{M}(\text{X}^M)=\frac{N}{M}\sum_{i=1}^M\tilde{\mathcal{L}}_{\theta,\phi}(x^{(i)})
\]&lt;/span&gt; where the minibatch &lt;span class=&#34;math inline&#34;&gt;\(\text{X}^M=\{x^{(i)}\}_{i=1}^M\)&lt;/span&gt; is randomly drawn sample of &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; datapoints from the full dataset &lt;span class=&#34;math inline&#34;&gt;\(\text{X}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; datapoints. In the paper &lt;a href=&#34;https://arxiv.org/abs/1312.6114&#34;&gt;Auto-Encoding Variational Bayes&lt;/a&gt;, author set &lt;span class=&#34;math inline&#34;&gt;\(M = 100, L = 1\)&lt;/span&gt; in their experiments.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;p&gt;&lt;img src=&#34;VAE2.PNG&#34; width = &#34;700&#34; height = &#34;250&#34; alt=&#34;Figure2&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;variational-auto-encoder-with-specific-case&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variational Auto-Encoder with specific case&lt;/h2&gt;
&lt;p&gt;We know that we can not perform the algorithm that we describe above, because we don’t know the distributions of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon, p_{\theta}(x|z), q_{\phi}(z|x), p_{\theta}(z)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g_{\phi}(\epsilon,x)\)&lt;/span&gt;. In reality, like author described in the paper, we firstly let the prior over the latent variables be the centered isotropic multivariate Guassian &lt;span class=&#34;math inline&#34;&gt;\(p_{\theta}(z) = \mathcal{N}(0,\text{I})\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;variational-approxiamte-posterior-q_phizxi&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variational approxiamte posterior &lt;span class=&#34;math inline&#34;&gt;\(q_{\phi}(z|x^{(i)})\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Let the variational approxiamte posterior be a multivariate Guassian with a diagonal covariance structure: &lt;span class=&#34;math display&#34;&gt;\[
q_{\phi}(z|x^{(i)}) = \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mu^{(i)},\sigma^{(i)}\)&lt;/span&gt; denote the variational mean and s.d. evaluated by datapoint &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Then a valid reparameterization is &lt;span class=&#34;math inline&#34;&gt;\(z=\mu+\sigma\epsilon\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is an auxiliary noise variable &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\sim \mathcal{N}(0,\text{I})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; be the dimensionality of &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu^{(i)}_j, \sigma^{(i)}_j\)&lt;/span&gt; denote the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th element. Recall that &lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}[z] = \int z p(z) dz\\
\mathbb{E}[z^2] = \int z^2 p(z) dz\\
\text{Var}[z] = \mathbb{E}[z^2] - \mathbb{E}^2[z]
\]&lt;/span&gt; Then, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\int q_{\phi}(z|x^{(i)})\log p_{\theta}(z)dz =&amp;amp;\ \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})\log \mathcal{N}(0,\text{I})dz\\
=&amp;amp;\ \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})(\log \frac{1}{\sqrt{2\pi}})dz - \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I}) \frac{z^2}{2}dz\\
=&amp;amp;\ -\frac{J}{2}\log(2\pi)\int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})dz - \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I}) \frac{z^2}{2}dz\\
=&amp;amp;\ -\frac{J}{2}\log(2\pi) - \frac{1}{2}\int z^2 \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})dz\\
=&amp;amp;\ -\frac{J}{2}\log(2\pi) - \frac{1}{2}\sum_{j=1}^J\mathbb{E}_{ q_{\phi}(z_j|x^{(i)})}[z_j^2]\\
=&amp;amp;\ -\frac{J}{2}\log(2\pi) - \frac{1}{2}\sum_{j=1}^J\Big[\mathbb{E}_{ q_{\phi}(z_j|x^{(i)})}^2[z_j]+\text{Var}_{ q_{\phi}(z_j|x^{(i)})}[z_j]\Big]\\
=&amp;amp;\ -\frac{J}{2}\log(2\pi) - \frac{1}{2}\sum_{j=1}^J(\mu_j^2+\sigma_j^2)
\end{align}
\]&lt;/span&gt; and &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\int q_{\phi}(z|x^{(i)})\log q_{\phi}(z|x^{(i)})dz =&amp;amp;\ \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})\log \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})dz\\
=&amp;amp;\ \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})\log \Big[\frac{1}{\sqrt{2\pi\sigma^{(i)^2}}}\exp(\frac{-(z-\mu^{(i)})^2}{2\sigma^{(i)^2}})\Big]dz\\
=&amp;amp;\ \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})\Big[-\frac{1}{2}\log 2\pi - \frac{1}{2}\log \sigma^{(i)^2} - \frac{(z-\mu^{(i)})^2}{2\sigma^{(i)^2}}\Big]dz\\
=&amp;amp;\ -\frac{J}{2}\log(2\pi) - \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})\Big[\frac{\log \sigma^{(i)^2}}{2} - \frac{(z-\mu^{(i)})^2}{2\sigma^{(i)^2}}\Big]dz\\
=&amp;amp;\ -\frac{J}{2}\log(2\pi) - \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I}) \Big[\frac{1}{2}\log \sigma^{(i)^2} - \frac{z^2-2\mu^{(i)}z+\mu^{(i)^2}}{2\sigma^{(i)^2}}\Big]dz\\
=&amp;amp;\ -\frac{J}{2}\log(2\pi) - \frac{1}{2}\sum_{J=1}^J\log \sigma_j^{(i)^2} + \int \frac{1}{2\sigma^{(i)^2}}\mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})(z^2-2\mu^{(i)}z+\mu^{(i)^2})dz\\
=&amp;amp;\ -\frac{J}{2}\log(2\pi) - \frac{1}{2}\sum_{j=1}^J\log \sigma_j^{(i)^2} + \frac{1}{2}\sum_{j=1}^J\frac{\mu^{(i)^2}+\sigma^{(i)^2}-2\mu^{(i)^2}+\mu^{(i)^2}}{\sigma^{(i)^2}}\\
=&amp;amp;\ -\frac{J}{2}\log(2\pi) - \frac{1}{2}\sum_{j=1}^J(1+\log \sigma_j^{(i)^2})
\end{align}
\]&lt;/span&gt; Therefore, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
-KL[q_{\phi}(z|x^{(i)})||p_{\theta}(z)] =&amp;amp; - \int q_{\phi}(z|x^{(i)})\log \frac{q_{\phi}(z|x^{(i)})}{p_{\theta}(z)}dz\\
=&amp;amp;\ - \int q_{\phi}(z|x^{(i)})[\log q_{\phi}(z|x^{(i)}) - \log p_{\theta}(z)]dz\\
=&amp;amp;\ \int q_{\phi}(z|x^{(i)})[\log p_{\theta}(z) - \log q_{\phi}(z|x^{(i)})]dz\\
=&amp;amp;\ -\frac{J}{2}\log(2\pi) - \frac{1}{2}\sum_{j=1}^J(\mu_j^2+\sigma_j^2) - \Big[-\frac{J}{2}\log(2\pi) - \frac{1}{2}\sum_{j=1}^J(1+\log \sigma_j^{(i)^2})\Big]\\
=&amp;amp;\ \frac{1}{2}\sum_{j=1}^J(1+\log \sigma_j^{(i)^2}-\mu_j^2-\sigma_j^2)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;true-posterior-p_thetaxz&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;True posterior &lt;span class=&#34;math inline&#34;&gt;\(p_{\theta}(x|z)\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;We supposed that the true posterior &lt;span class=&#34;math inline&#34;&gt;\(p_{\theta}(x|z)\)&lt;/span&gt; be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution parameters are computed from &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; with a MLP (Multi-Layer Perceptron).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bernoulli MLP as decoder&lt;/strong&gt;&lt;br /&gt;
If the data are binary data, then we would choose &lt;span class=&#34;math display&#34;&gt;\[
\log p_{\theta}(x|z) = \sum_{j=1}^Dx_j \log y_j + (1-x_j)\log (1-y_j)
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(y = f_\sigma(W_2\tanh(W_1z+b_1)+b_2)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f_\sigma(\cdot)\)&lt;/span&gt; is the elementwise sigmoid activation function and &lt;span class=&#34;math inline&#34;&gt;\(\theta=\{W_1,W_2,b_1,b_2\}\)&lt;/span&gt; are the weights and biases of the MLP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gaussian MLP as decoder&lt;/strong&gt;&lt;br /&gt;
Let decoder be a mutivariate Guassian with a diagonal covariance structure: &lt;span class=&#34;math display&#34;&gt;\[
\log p_{\theta}(x|z) = \log \mathcal{N}(\mu,\sigma^2\text{I})
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mu = W_4h+b_4,\ \log\sigma^2 = W_5h+b_5,\ h = \tanh(W_3Z+b_3)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\{W_3,W_4,W_5,b_3,b_4,b_5\}\)&lt;/span&gt; are the weights and biases of the MLP and part of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Analysis in case of binary data&lt;/strong&gt;&lt;br /&gt;
Recall the second estimator we describe above &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\mathcal{L}_{\theta,\phi}(\text{X})\simeq&amp;amp;\ \tilde{\mathcal{L}}_{\theta,\phi}^{B}(x^{(i)})\\
=&amp;amp;\ \frac{1}{L}\sum_{l=1}^L\log p_{\theta}(x^{(i)}|z^{(i,l)}) - KL[q_{\phi}(z|x^{(i)})||p_{\theta}(z)]\\
=&amp;amp;\ \frac{1}{L}\sum_{l=1}^L\log p_{\theta}(x^{(i)}|z^{(i,l)}) + \frac{1}{2}\sum_{j=1}^J(1+\log \sigma_j^{(i)^2}-\mu_j^2-\sigma_j^2)
\end{align}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(z^{(i,l)} = \mu^{(i)} + \sigma^{(i)}\epsilon^{(l)}, \epsilon^{l}\sim p(\epsilon)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\log p_{\theta}(x|z) = \sum_{j=1}^Dx_j \log y_j + (1-x_j)\log (1-y_j)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-variational-auto-encoder-in-python&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Using Variational Auto-Encoder in python&lt;/h1&gt;
&lt;div id=&#34;import-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import packages&lt;/h2&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;function-for-visualizing-batch-images&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Function for visualizing batch images&lt;/h2&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def VisConcatImg(batch_images, title):
    batch_size = np.shape(batch_images)[0]
    sqrt_size = int(batch_size ** 0.5)
    batch_images = batch_images.reshape(batch_size, 28, 28)
    row_concatenated = [np.concatenate(batch_images[i*sqrt_size : (i+1)*sqrt_size], axis=1) for i in range(sqrt_size)]
    concatenated = np.concatenate(row_concatenated, axis=0)
    plt.imshow(concatenated, cmap=&amp;#39;gray&amp;#39;)
    plt.title(title)
    plt.axis(&amp;#39;off&amp;#39;)
    plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mnist-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MNIST Dataset&lt;/h2&gt;
&lt;p&gt;The MNIST includes 60000 training samples and 10000 testing samples. Each sample is a 784-dimensional vector (28??28), with pixel values in [0, 1], which can be assumed as multivariate Bernoulli variables.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Downloading MNIST dataset
mnist = input_data.read_data_sets(&amp;#39;./mnist&amp;#39;, one_hot=False)
# VAE for MNIST
class VAE(object):
    def __init__(self, x_size=28*28, hidden1_size=100, hidden2_size=400, hidden3_size=100, hidden4_size=400, z_size=20, learning_rate=1e-4):
        self.x_size = x_size
        self.hidden1_size = hidden1_size
        self.hidden2_size = hidden2_size
        self.hidden3_size = hidden3_size
        self.hidden4_size = hidden4_size
        self.z_size = z_size
        self.learning_rate = learning_rate
        self.x = tf.placeholder(tf.float32, [None, x_size])
        self.epsilon = tf.placeholder(tf.float32, [None, z_size]) # sample from N(0,1) for every step
        with tf.variable_scope(&amp;#39;encoder&amp;#39;):
            self.encoder()
        with tf.variable_scope(&amp;#39;decoder&amp;#39;):
            self.decoder()
        with tf.variable_scope(&amp;#39;loss&amp;#39;):
            self.compute_loss()
        with tf.variable_scope(&amp;#39;train&amp;#39;):
            self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.total_loss)
    def encoder(self):
        self.hidden1 = tf.layers.dense(self.x, units=self.hidden1_size, activation=tf.nn.relu)
        self.hidden2 = tf.layers.dense(self.hidden1, units=self.hidden2_size, activation=tf.nn.relu)
        self.mu = tf.layers.dense(self.hidden2, units=self.z_size)
        self.sigma = tf.layers.dense(self.hidden2, units=self.z_size, activation=tf.exp)
        self.z = tf.add(self.mu, tf.multiply(self.epsilon, self.sigma))
        
    def decoder(self):
        self.hidden3 = tf.layers.dense(self.z, units=self.hidden3_size, activation=tf.nn.relu)
        self.hidden4 = tf.layers.dense(self.hidden3, units=self.hidden4_size, activation=tf.nn.relu)
        self.y = tf.layers.dense(self.hidden4, units=self.x_size, activation=tf.nn.sigmoid)
        
    # adding 1e-8 before taking the logarithm to avoid numerical instability.
    def compute_loss(self):
        self.recons_loss = tf.reduce_mean(tf.reduce_sum(-(self.x * tf.log(self.y + 1e-8) + (1 - self.x) * tf.log(1 - self.y + 1e-8)), 1))
        self.KL_loss = tf.reduce_mean(-0.5 * tf.reduce_sum(1 + 2 * tf.log(self.sigma + 1e-8) - tf.square(self.mu) - tf.square(self.sigma), 1))
        self.total_loss = self.recons_loss + self.KL_loss
        
# Training VAE
model = VAE()
BATCH_SIZE = 100
EPOCHS = 50
STEPS = int(60000 / BATCH_SIZE)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
for e in range(EPOCHS):
    for i in range(STEPS):
        train_data, _ = mnist.train.next_batch(batch_size=BATCH_SIZE)
        ep = np.random.multivariate_normal(np.zeros(model.z_size), np.eye(model.z_size), size=BATCH_SIZE)
        sess.run(model.train_op, feed_dict={model.x: train_data, model.epsilon: ep})
    REloss, KLloss, Tloss = sess.run([model.recons_loss, model.KL_loss, model.total_loss], feed_dict={model.x: train_data, model.epsilon: ep})
    print(&amp;#39;Epoch: &amp;#39;, e, &amp;#39;| reconstruction loss: &amp;#39;, REloss, &amp;#39;| KL loss:&amp;#39;, KLloss, &amp;#39;| total loss: &amp;#39;, Tloss)
# Visualizing results
test_data, _ = mnist.test.next_batch(batch_size=50)
VisConcatImg(test_data, &amp;#39;raw images&amp;#39;)
ep = np.random.multivariate_normal(np.zeros(model.z_size), np.eye(model.z_size), size=50)
latent, recons_x = sess.run([model.mu, model.y], feed_dict={model.x: test_data, model.epsilon: ep})
VisConcatImg(recons_x, &amp;#39;reconstructed images&amp;#39;)
randoms = np.random.multivariate_normal(np.zeros(model.z_size), np.eye(model.z_size), size=50)
generated_x = sess.run(model.y, feed_dict={model.z: randoms})
VisConcatImg(generated_x, &amp;#39;generated images&amp;#39;)
sess.close()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;result&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Result&lt;/h2&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;VAE3.PNG&#34; width = &#34;300&#34; height = &#34;300&#34; alt=&#34;Figure3&#34; /&gt;
&lt;p&gt;
Figure3. Raw Images
&lt;/div&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;VAE4.PNG&#34; width = &#34;300&#34; height = &#34;300&#34; alt=&#34;Figure4&#34; /&gt;
&lt;p&gt;
Figure4. Reconstructed Images
&lt;/div&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;VAE5.PNG&#34; width = &#34;300&#34; height = &#34;300&#34; alt=&#34;Figure5&#34; /&gt;
&lt;p&gt;
Figure5. Generated Images
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;reference&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1312.6114&#34;&gt;D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2014. 5, 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/eeyangc/Statistical-Machine-Learning&#34;&gt;Yang Can. VAE_demo in python. 2018,12,31&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>小菜鸟的入门TensorFlow</title>
      <link>/post/%E5%B0%8F%E8%8F%9C%E9%B8%9F%E7%9A%84%E5%85%A5%E9%97%A8tensorflow/</link>
      <pubDate>Wed, 26 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/%E5%B0%8F%E8%8F%9C%E9%B8%9F%E7%9A%84%E5%85%A5%E9%97%A8tensorflow/</guid>
      <description>


&lt;div id=&#34;tensorflow&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;迈出TensorFlow世界的第一步&lt;/h1&gt;
&lt;div id=&#34;tensorflow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;安装TensorFlow&lt;/h2&gt;
&lt;p&gt;鉴于python3更高级（传闻python2最终会被淘汰，所以希望选择能陪伴更久的工具:)），本文中会以python3为基准，屏幕前的读者你！也建议你跟我用上python3！另外，如果你刚起步使用python的话，那建议你直接下载 &lt;a href=&#34;https://www.anaconda.com/&#34;&gt;Anaconda&lt;/a&gt;，快捷高效，下载引导详见图1。Anaconda会帮助我们省去下载很多库的时间，把时间留给TensorFlow吧！&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;TS1.PNG&#34; width = &#34;500&#34; height = &#34;300&#34; alt=&#34;Figure1&#34; /&gt;
&lt;p&gt;
图1. 下载Anaconda
&lt;/div&gt;
&lt;div id=&#34;anaconda&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;设置水土不服的Anaconda&lt;/h3&gt;
&lt;p&gt;首先，在安装Anaconda后，先打开Anaconda Prompt（一般在你的应用目录可以找到），打开后是一个跟CMD一样黑乎乎的界面 ╮(๑•́ ₃•̀๑)╭ 打开后呢~通过&lt;code&gt;conda --version&lt;/code&gt;看看是否成功安装了Anaconda。一般会得到版本信息，如下所示&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda 4.5.12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;那么，Anaconda安装成功！&lt;/p&gt;
&lt;p&gt;下一步我们设置下Anaconda的仓库(Repository)镜像，因为默认连接的是境外镜像地址，会超慢（我记得我当时只有10kb的网速T_T），我们把镜像地址改为境内的清华大学开源软件镜像站，所以通过下面指令就可以提高你的下载速度(´•灬•‘)。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --set show_channel_urls yes&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;python3.5&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;创建python3.5环境&lt;/h3&gt;
&lt;p&gt;首先，由于目前TensoFlow官方Only支持python3.5版本，而且在写本文的这个时候，Anaconda官方最新版本中的python是3.7版本，所以我们需要创建一个python3.5的新环境。&lt;/p&gt;
&lt;p&gt;首先在系统菜单栏找到并点击Anaconda Navigator，然后选择Enviroments（如图2所示），然后点击Create创建新环境：&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;TS2.PNG&#34; width = &#34;650&#34; height = &#34;330&#34; alt=&#34;Figure2&#34; /&gt;
&lt;p&gt;
图2. Environments界面
&lt;/div&gt;
&lt;p&gt;我们命名为&lt;em&gt;tensorflow&lt;/em&gt;，并选择python3.5的版本：&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;TS3.PNG&#34; width = &#34;350&#34; height = &#34;180&#34; alt=&#34;Figure3&#34; /&gt;
&lt;p&gt;
图3. 创建新环境窗口
&lt;/div&gt;
&lt;p&gt;安装成功后，Environments界面多了一个我们创建的命名为tensorflow的python3.5的环境，并自动预先安装一些基础的库。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;TS4.PNG&#34; width = &#34;650&#34; height = &#34;330&#34; alt=&#34;Figure4&#34; /&gt;
&lt;p&gt;
图4. 安装成功后的python3.5环境
&lt;/div&gt;
&lt;p&gt;搞定python3.5的环境后，我们顺便在python3.5环境下安装常用的&lt;strong&gt;jupyter notebook&lt;/strong&gt;和&lt;strong&gt;spyder&lt;/strong&gt;。先选择Home界面，然后可以看到&lt;strong&gt;jupyter notebook&lt;/strong&gt;和&lt;strong&gt;spyder&lt;/strong&gt;下方均显示&lt;strong&gt;Install&lt;/strong&gt;，当然就点击&lt;strong&gt;Install&lt;/strong&gt;，就等着Anaconda Navigator帮我们搞定啦！安装成功后，它们下方会变成&lt;strong&gt;Launch&lt;/strong&gt;（如图5所示）。&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;TS5.PNG&#34; width = &#34;650&#34; height = &#34;350&#34; alt=&#34;Figure5&#34; /&gt;
&lt;p&gt;
图5. 安装jupyter和spyder成功后的Home界面
&lt;/div&gt;
&lt;p&gt;如果要在Anaconda prompt界面启动python3.5环境，很简单，一行命令！&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda activate tensorflow&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里的tensorflow其实是我们的python3.5环境的命名~&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;piptensorflow-&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;通过pip安装我们的主角TensorFlow ٩(๑&amp;gt; ₃ &amp;lt;)۶з&lt;/h3&gt;
&lt;p&gt;搞定一切基础的部分后，接下来就开始用pip安装我们TensorFlow的CPU版本了~我们先激活python3.5的环境并用pip安装tensorflow&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda activate tensorflow
pip install tensorflow&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;旋转跳跃~闭着眼~睁开眼后就搞定了你要的TensorFlow (●´▽｀●) 下面我们先测试一下，在激活python3.5之后，输入&lt;code&gt;python&lt;/code&gt;运行python，然后输入下面的命令。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import tensorflow as tf
hello = tf.constant(&amp;#39;Hello, TensorFlow!&amp;#39;)
sess = tf.Session()
print(sess.run(hello))
a = tf.constant(1)
b = tf.constant(2)
c = sess.run(a+b)
print(&amp;quot;1 + 2 = %d&amp;quot; % c)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果输出了&lt;br /&gt;
‘Hello, TensorFlow!’&lt;br /&gt;
1 + 2 = 3&lt;br /&gt;
那么恭喜你，TensorFlow安装成功！！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Attention!&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;CPU有个利器(๑•̀_•́๑)&lt;br /&gt;
当我使用&lt;code&gt;sess = tf.Session()&lt;/code&gt;的时候，对话框告诉我：&lt;br /&gt;
2018-12-26 15:03:57.708274: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2&lt;br /&gt;
然后一脸懵，杰是个啥？&lt;br /&gt;
感恩必应搜索，感恩维基百科！&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;高级矢量扩展（AVX）是英特尔在2008年3月提出的英特尔和AMD微处理器的x86指令集体系结构的扩展，英特尔首先通过Sandy Bridge处理器在2011年第一季度推出，随后由AMD推出Bulldozer处理器在2011年第三季度.AVX提供了新功能，新指令和新编码方案。 特别是，AVX引入了融合乘法累加（FMA）操作，加速了线性代数计算，即点积，矩阵乘法，卷积等。几乎所有机器学习训练都涉及大量这些操作，因此将会支持AVX和FMA的CPU（最高达300％）更快。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;而对话框想告诉我，我的CPU支持AVX，让我赶紧用上它！&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import tensorflow as tf
import os
os.environ[&amp;#39;TF_CPP_MIN_LOG_LEVEL&amp;#39;] = &amp;#39;2&amp;#39;
sess = tf.Session()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这样就不会出现警告了~&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;目前觉得不重要的part(๑•̀_•́๑)&lt;br /&gt;
&lt;strong&gt;查看Windows系统下机器的GPU信息&lt;/strong&gt;&lt;br /&gt;
首先打开“运行”对话框并在“运行”对话框中输入“dxdiag”（如图6），此时会打开“DirextX诊断工具”窗口，再通过选择“显示”标签便可查到机器的GPU信息（如图7）。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;TS6.PNG&#34; width = &#34;360&#34; height = &#34;200&#34; alt=&#34;Figure6&#34; /&gt;
&lt;p&gt;
图6. 输入dxdiag命令
&lt;/div&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;TS7.PNG&#34; width = &#34;650&#34; height = &#34;470&#34; alt=&#34;Figure7&#34; /&gt;
&lt;p&gt;
图7. 查看机器GPU信息
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;简单提及安装GPU版本TensorFlow&lt;/strong&gt;&lt;br /&gt;
如果你觉得运行的速度不满足你的需求，那么你可以选择用上GPU版本的TensorFlow，那将帮助你火箭般的速度运行！下面简单带过如何安装TensorFlow的GPU版本~由于具体操作复杂，暂且跳过~&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install tensorflow-gpu&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;—— —— —— —— —— —— —— 这是一条分割线 —— —— —— —— —— —— ——&lt;/p&gt;
&lt;p&gt;上面操作呢…有时候jupyter和spyder会出现一些路径的问题_(:з」∠)_&lt;/p&gt;
&lt;p&gt;所以我还是直接下载带Python3.5的&lt;a href=&#34;https://repo.anaconda.com/archive/Anaconda3-4.2.0-Windows-x86_64.exe&#34;&gt;Anaconda&lt;/a&gt;吧~&lt;/p&gt;
&lt;p&gt;点击上面Anaconda即可下载(..•˘_˘•..)&lt;/p&gt;
&lt;p&gt;—— —— —— —— —— —— —— 这是另一条分割线 —— —— —— —— —— —— ——&lt;/p&gt;
&lt;p&gt;下载Python3.5的Anaconda可能也有乱七八糟的错误_(:з」∠)_&lt;/p&gt;
&lt;p&gt;所以直接稳妥的方法就是用Anaconda prompt执行下面的命令~&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda create --name python35 python=3.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;安装完成后会提示你&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# To activate this environment, use
#
#     $ conda activate python35
#
# To deactivate an active environment, use
#
#     $ conda deactivate&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;接着先激活python3.5的环境，就可以用pip安装tensorflow&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda activate python35
pip install tensorflow&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Jupyter下设置python3.5的内核&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果不做一些设置操作，默认Jupyter打开还是会以默认Anaconda下的python版本，由于目前已经到了python3.7的版本了，那在Jupyter下使用Tensorflow必定会出问题。&lt;/p&gt;
&lt;p&gt;首先打开Anaconda Prompt并安装&lt;code&gt;ipykernel&lt;/code&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install ipykernel&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;安装成功后，执行下面命令就可以了~&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python  -m ipykernel install --user&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;打开Jupyter就会发现可以运行Tensorflow了！&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Expectation-Maximization Algorithm</title>
      <link>/post/expectation-maximization-algorithm/</link>
      <pubDate>Mon, 24 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/expectation-maximization-algorithm/</guid>
      <description>


&lt;p&gt;From some on-line article, it is interesting that we can consider EM algorithm as some Chinese Kungfu manuals and it contains 9 levels of perspectives. With such metaphor, I can feel how this method powerful it is. Now, I want to share my view with you.&lt;/p&gt;
&lt;div id=&#34;introduction-to-the-em-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction to the EM algorithm&lt;/h2&gt;
&lt;p&gt;Expectation-Maximization (EM) algorithm is an important method to solve the maximum likelihood problem with latent variables in statistics. It is widely used in Machine Learning because it can simplify many difficult problems. One of the famous and classical application is &lt;strong&gt;gaussian mixture model&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;basic-probability-theory&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Basic probability theory&lt;/h3&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(p(x|\theta)\)&lt;/span&gt; be the probability density function of random variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is the parameter of the density function, then we know that the basic probability property is &lt;span class=&#34;math display&#34;&gt;\[
p(\text{x};\theta) \geq 0, \int_{-\infty}^{+\infty}p(\text{x};\theta) d\text{x} = 1.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we take the expectation of x, we get &lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}[\text{x}] = \int\text{x}p(\text{x};\theta) d\text{x}
\]&lt;/span&gt; In the integral, we know that the &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[\text{x}]\)&lt;/span&gt; involves &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; but not &lt;span class=&#34;math inline&#34;&gt;\(\text{x}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If we generalize it to function and let &lt;span class=&#34;math inline&#34;&gt;\(f(\text{x})\)&lt;/span&gt; be a function of &lt;span class=&#34;math inline&#34;&gt;\(\text{x}\)&lt;/span&gt;. Similarly, the expectation of &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; is given by &lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}[f] = \int f(\text{x})p(\text{x};\theta) d\text{x}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With the similar result, we know that the &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[f]\)&lt;/span&gt; involves &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; but not &lt;span class=&#34;math inline&#34;&gt;\(\text{x}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;motivation-of-the-em-algorithm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Motivation of the EM algorithm&lt;/h3&gt;
&lt;p&gt;At the beginning, we denote that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\text{X}\)&lt;/span&gt;: Set of all observed data (incomplete-data)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\text{Z}\)&lt;/span&gt;: Set of all latent variables&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;: Set of all model parameters&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\{ \text{X,Z} \}\)&lt;/span&gt;: Each observation in &lt;span class=&#34;math inline&#34;&gt;\(\text{X}\)&lt;/span&gt; is corresponding value of the latent variable &lt;span class=&#34;math inline&#34;&gt;\(\text{Z}\)&lt;/span&gt; (complete-data)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then the log-likelihood function can be written as &lt;span class=&#34;math display&#34;&gt;\[
L(\text{X};\theta) = \ln p(\text{X};\theta) = \ln \{ \sum_{\text{Z}}p(\text{X}, \text{Z};\theta) \}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is too hard to straightly solve the problem with &lt;span class=&#34;math inline&#34;&gt;\(\ln\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sum\)&lt;/span&gt;. The likelihood function for the complete data set simply takes the form &lt;span class=&#34;math inline&#34;&gt;\(\ln p(\text{X,Z}|\theta)\)&lt;/span&gt;, and we shall suppose that maximization of this complete-data log-likelihood function is straightforward.&lt;/p&gt;
&lt;p&gt;In practice, we are not given the latent variable &lt;span class=&#34;math inline&#34;&gt;\(\text{Z}\)&lt;/span&gt; but we know the posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\text{Z}|\text{X};\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-level-1-of-the-em-algorithm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Level 1 of the EM algorithm&lt;/h3&gt;
&lt;p&gt;In the level 1, we just need to know the basic knowledge of EM algorithm.&lt;/p&gt;
&lt;p&gt;In the &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}\)&lt;/span&gt;-step, we use the current parameter values &lt;span class=&#34;math inline&#34;&gt;\(\theta_{\text{old}}\)&lt;/span&gt; to find the posterior distribution of the latent variables given by &lt;span class=&#34;math inline&#34;&gt;\(p(\text{Z}|\text{X};\theta_{\text{old}})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Then, we would use this posterior distribution to find the expectation of the &lt;strong&gt;complete-data&lt;/strong&gt; log-likelihood evaluated for some general parameter value &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. This expectation, denoted &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Q}(\theta,\theta_{\text{old}})\)&lt;/span&gt;, is given by &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{Q}(\theta,\theta_{\text{old}}) = \mathbb{E}_{\text{Z}|\text{X};\theta_{\text{old}}}[\ln p(\text{X,Z};\theta)] = \sum_{\text{Z}} \ln p(\text{X,Z};\theta)p(\text{Z}|\text{X};\theta_{\text{old}})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{M}\)&lt;/span&gt; (Maximization) step, we determine the revised parameter estimate &lt;span class=&#34;math inline&#34;&gt;\(\theta_{\text{new}}\)&lt;/span&gt; by maximizing the function &lt;span class=&#34;math display&#34;&gt;\[
\theta_{\text{new}} = \arg \max_\limits{\theta} \mathcal{Q}(\theta,\theta_{\text{old}})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since the &lt;strong&gt;complete-data&lt;/strong&gt; log-likelihood involves unobversed data &lt;span class=&#34;math inline&#34;&gt;\(\text{Z}\)&lt;/span&gt;, we use &lt;strong&gt;Expectation&lt;/strong&gt; to eliminate the uncertainty, and the function &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}_{\text{Z}|\text{X},\theta_{\text{old}}}[\ln p(\text{X,Z}|\theta)]\)&lt;/span&gt; does not involve &lt;span class=&#34;math inline&#34;&gt;\(\text{Z}\)&lt;/span&gt; but involve &lt;span class=&#34;math inline&#34;&gt;\((\theta,\theta_{\text{old}})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We can summarize the procedure as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;While &lt;span class=&#34;math inline&#34;&gt;\(\theta_{\text{new}} - \theta_{old} &amp;gt; \epsilon\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\quad\)&lt;/span&gt; Expectation-Step on log likelihood function: &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{Q}(\theta,\theta_{\text{old}}) = \mathbb{E}_{\text{Z}|\text{X};\theta_{\text{old}}}[\ln p(\text{X,Z};\theta)]= \sum_{\text{Z}} \ln p(\text{X,Z};\theta)p(\text{Z}|\text{X};\theta_{\text{old}})
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\quad\)&lt;/span&gt; Maximization-Step on &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Q}(\theta,\theta_{\text{old}})\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[
\theta_{\text{new}} = \arg \max_\limits{\theta} \mathcal{Q}(\theta,\theta_{\text{old}})
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;the-level-2-of-the-em-algorithm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Level 2 of the EM algorithm&lt;/h3&gt;
&lt;p&gt;After writting down the pseudocode of EM algorithm, we want to know the reason that we can use expectation to approximate the maximum log-likelihood by repeating &lt;strong&gt;Expectation&lt;/strong&gt; and &lt;strong&gt;Maximization&lt;/strong&gt;. In other words, we want to prove that &lt;span class=&#34;math display&#34;&gt;\[
\arg \max_\theta \mathbb{E}_{\text{Z}|\text{X};\theta_{\text{old}}}[\ln p(\text{X};\theta)] \approx \arg \max_\theta \ln p(\text{X};\theta)
\]&lt;/span&gt; where the joint distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\text{X}, \text{Z};\theta)\)&lt;/span&gt; is governed by a set of parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Next we introduce a distribution &lt;span class=&#34;math inline&#34;&gt;\(q(\text{Z})\)&lt;/span&gt; defined over the latent variables.&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math display&#34;&gt;\[
p(\text{X},\text{Z};\theta) = p(\text{Z}|\text{X};\theta)p(\text{X};\theta)
\]&lt;/span&gt; and &lt;span class=&#34;math display&#34;&gt;\[
\sum_\text{Z}q(\text{Z}) = 1
\]&lt;/span&gt; we can get decomposition by &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\ln p(\text{X};\theta) =&amp;amp;\ \ln \frac{p(\text{X},\text{Z};\theta)}{p(\text{Z}|\text{X};\theta)}\\
=&amp;amp;\ \ln p(\text{X},\text{Z};\theta) - \ln p(\text{Z}|\text{X};\theta) + \ln q(\text{Z}) - \ln q(\text{Z})\\
=&amp;amp;\ \ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})} - \ln \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}\\
=&amp;amp; \sum_\text{Z}q(\text{Z}) \{ \ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})} - \ln \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})} \}\\
=&amp;amp;\ \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})} - \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}\\
=&amp;amp;\ \mathcal{L}(q,\theta) + KL(q||p)
\end{align}
\]&lt;/span&gt; where &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\mathcal{L}(q,\theta) = \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})}\\
KL(q||p) = - \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We call the &lt;span class=&#34;math inline&#34;&gt;\(KL(q||p)\)&lt;/span&gt; as the Kullback-Leibler divergence (KL divergence, also known as relative entropy).&lt;/p&gt;
&lt;p&gt;Recall that &lt;strong&gt;Jensen’s inequality&lt;/strong&gt; holds for convex function &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;. &lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}[f(x)] \geq f(\mathbb{E}[x])
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Applying Jensen’s inequality in KL divergence, we have &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
KL(q||p) =&amp;amp; - \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})} = -\mathbb{E}_q[\ln\{\frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}\} ]\\
\geq&amp;amp; -\ln \mathbb{\text{E}_q}[\frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}] = -\ln \sum_\text{Z} q(\text{Z}) \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}\\
=&amp;amp; -\ln \sum_\text{Z} p(\text{Z}|\text{X};\theta) = -\ln 1\\ =&amp;amp;\ 0
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we let &lt;span class=&#34;math inline&#34;&gt;\(q(\text{Z}) = p(\text{Z}|\text{X};\theta)\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(p(\text{X},\text{Z};\theta) = p(\text{Z}|\text{X};\theta)p(\text{X};\theta)\)&lt;/span&gt;, then &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\mathcal{L}(q,\theta) =&amp;amp; \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})} = \mathbb{E}_q[\ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})}]\\
=&amp;amp;\ \mathbb{E}_{\text{Z}|\text{X};\theta}[\ln p(X;\theta)]
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus, we can get the result that &lt;span class=&#34;math inline&#34;&gt;\(\ln p(\text{X};\theta) \geq \mathcal{L}(q,\theta) = \mathbb{E}_{\text{Z}|\text{X};\theta}[\ln p(X;\theta)]\)&lt;/span&gt;, so we can say that &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}(q,\theta) = \mathbb{E}_{\text{Z}|\text{X};\theta}[\ln p(X;\theta)]\)&lt;/span&gt; is the low bound of &lt;span class=&#34;math inline&#34;&gt;\(\ln p(\text{X};\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;EM1.PNG&#34; width = &#34;300&#34; height = &#34;300&#34; alt=&#34;Figure1&#34; /&gt;
&lt;p&gt;
Figure1. The EM algorithm involves alternatel computing a lower bound on the log likelihood for the current parameter values and then maximizing this bound to obtain the new parameter values.
&lt;/div&gt;
&lt;p&gt;Actually, EM algorithm is one of the special case of Minorize-Maximization (MM) algorithm, and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}(q,\theta)\)&lt;/span&gt; can be considered as the &lt;strong&gt;surrogate function&lt;/strong&gt; in MM algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Made by Thompson</title>
      <link>/privacy/</link>
      <pubDate>Thu, 20 Dec 2018 18:00:00 +0800</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;©️ 2018 $\cdot$ Made by Thompson Hu.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coordinate Descent Algorithm</title>
      <link>/post/coordinate-descent-algorithm/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/coordinate-descent-algorithm/</guid>
      <description>


&lt;div id=&#34;coordinate-descent-framework&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Coordinate Descent Framework&lt;/h3&gt;
&lt;p&gt;At the begining of this section, we start to discuss three different types of function.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Given convex, differentiable function &lt;span class=&#34;math inline&#34;&gt;\(f: \mathbb{R}^n \to \mathbb{R}\)&lt;/span&gt;, we know &lt;span class=&#34;math inline&#34;&gt;\(f(x+\delta \cdot e_i)\geq f(x)\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; because &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(x) = (\frac{\partial f}{\partial x_1}(x),\dots,\frac{\partial f}{\partial x_n}(x)) = 0\)&lt;/span&gt;. Here, &lt;span class=&#34;math inline&#34;&gt;\(e_i = (0,\dots,1,\dots,0) \in \mathbb{R}^n\)&lt;/span&gt;, the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th standard basis vactor.&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;CD1.PNG&#34; width = &#34;200&#34; height = &#34;200&#34; alt=&#34;Figure1&#34; /&gt;
&lt;p&gt;
Figure1. Convex and differential function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Given convex but not differentiable function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, we can not found a global minimizer.&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;CD2.PNG&#34; width = &#34;400&#34; height = &#34;200&#34; alt=&#34;Figure2&#34; /&gt;
&lt;p&gt;
Figure2. Convex but not differential function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;
&lt;/div&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Given convex &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; and each convex but not differentiable &lt;span class=&#34;math inline&#34;&gt;\(h_i\)&lt;/span&gt;, so we get &lt;span class=&#34;math inline&#34;&gt;\(f(x)=g(x)+\sum_{i=1}^n h_i(x_i)\)&lt;/span&gt;. In this function, the non-smooth part is called as &lt;strong&gt;separable&lt;/strong&gt;.&lt;br /&gt;
For any &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, we get
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
f(y) - f(x) \geq&amp;amp; \nabla g(x)^T (y-x) + \sum_{i=1}^n [h_i(y_i)-h_i(x_i)]\\
=&amp;amp; \sum\limits_{i=1}^n [\nabla_ig(x)(y_i-x_i)+h_i(y_i)-h_i(x_i)] \geq 0
\end{align}\]&lt;/span&gt;
Thus, we can get global minimizer.&lt;/li&gt;
&lt;/ol&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;img src=&#34;CD3.PNG&#34; width = &#34;400&#34; height = &#34;200&#34; alt=&#34;Figure3&#34; /&gt;
&lt;p&gt;
Figure3. Convex, not differential but separable function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;
&lt;/div&gt;
If we get a function with the formula like &lt;span class=&#34;math inline&#34;&gt;\(f(x) = g(x) + \sum_{i=1}^n h_i(x_i)\)&lt;/span&gt;, where the &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; is convex and differentiable function, each &lt;span class=&#34;math inline&#34;&gt;\(h_i\)&lt;/span&gt; is convex functions, then we can use coordinate descent to find global minimizer. The procedure is following: start with some initial guess &lt;span class=&#34;math inline&#34;&gt;\(x^{(0)}\)&lt;/span&gt;, and repeat
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
x_1^{(k)} \in&amp;amp; \mathop{\arg\min}_{x_1} f(x_1,x_2^{(k-1)},x_3^{(k-1)},\dots,x_n^{(k-1)})\\
x_2^{(k)} \in&amp;amp; \mathop{\arg\min}_{x_2} f(x_1^{(k)},x_2,x_3^{(k-1)},\dots,x_n^{(k-1)})\\
x_3^{(k)} \in&amp;amp; \mathop{\arg\min}_{x_3} f(x_1^{(k)},x_2^{(k)},x_3,\dots,x_n^{(k-1)})\\
\cdots&amp;amp; \\
x_n^{(k)} \in&amp;amp; \mathop{\arg\min}_{x_n} f(x_1^{(k)},x_2^{(k)},x_3^{(k)},\dots,x_n)\\
\end{align}\]&lt;/span&gt;
&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(k=1,2,3,\dots,K\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; after we solve for &lt;span class=&#34;math inline&#34;&gt;\(x_i^{(k)}\)&lt;/span&gt;, we use its new value from then on.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;coordinate-descent-for-linear-regression-with-convex-penalties&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Coordinate descent for linear regression with convex penalties&lt;/h3&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
