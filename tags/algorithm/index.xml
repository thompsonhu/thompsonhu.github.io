<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Algorithm on Bonbon Blog</title>
    <link>/tags/algorithm/</link>
    <description>Recent content in Algorithm on Bonbon Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Tue, 15 Jan 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/algorithm/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>生成对抗网络的第二Part</title>
      <link>/post/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AC%AC%E4%BA%8Cpart/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AC%AC%E4%BA%8Cpart/</guid>
      <description>我们认为每张图片对应的是一个高维向量，我们希望能找出这一类图片所在的图像空间的分布\(P\)，GAN的目的其实就是在寻找这个分布\(P\)。
传统的方法我们会想到用最大似然估计：
首先给定一些样本数据，可以得到它的分布\(P_{data}(x)\)；接着我们假定有一个由参数\(\theta\)决定的分布\(P_G(x;\theta)\)；我们希望能够找到\(\theta\)，满足分布\(P_G(x;\theta)\)尽可能靠近分布\(P_{data}(x)\)；假设\(P_G(x;\theta)\)是高斯分布（正态分布），那\(\theta\)就是均值和方差。
从分布\(P_{data}(x)\)中进行抽样获得一组样本数据\(\{x_1,x_2,\dots,x_m\}\)，我们可以得到似然值的表达式 \[L = \prod_{i=1}^{m} P_G(x^i;\theta)\]我们希望能够找到\(\theta^*\)能够使得似然值\(L\)最大。
最大似然估计与最小KL散度的等价性通过对\(L\)取对数，我们可以得到对数似然值，同时\(\theta^*\)可以通过下式得到： \[\begin{align}\theta^* &amp;amp;=\ \arg \max_{\theta} \log L\\ &amp;amp;=\ \arg \max_{\theta} \log \prod_{i=1}^{m} P_G(x^i;\theta)\\&amp;amp;=\ \arg \max_{\theta} \sum_{i=1}^m \log P_G(x^i;\theta)\end{align}\]
由于样本\(\{x^1,x^2,\dots,x^m\}\)是随机抽取来自于分布\(P_{data}(x)\)，上式可以近似求\(\log P_G(x;\theta)\)的期望的最大点，可以表示为 \[\begin{align}\theta^* &amp;amp;\approx\ \arg \max_{\theta} E_{x\sim P_{data}}[\log P_G(x;\theta)]\\&amp;amp;=\ \arg \max_{\theta} \int\limits_x P_{data}(x) \log P_G(x;\theta) dx\end{align}\]
从式子中知道，我们所求的\(\theta^*\)只跟分布\(P_G(x;\theta)\)相关，我们可以在后面加上一项\(-\int\limits_x P_{data}(x) \log P_{data}(x)dx\)，这不影响求最大化下对应得\(\theta^*\)，那么由上式就可以得到 \[\begin{align}\theta^* &amp;amp;\approx\ \arg \max_{\theta} \int\limits_x P_{data}(x) \log P_G(x;\theta) dx - \int\limits_x P_{data}(x) \log P_{data}(x)dx\\&amp;amp;=\ \arg \max_{\theta} \int\limits_x P_{data}(x) \Big[\log P_G(x;\theta) - \log P_{data}(x)\Big] dx\\&amp;amp;=\ \arg \min_{\theta} \int\limits_x P_{data}(x) \Big[\log P_{data}(x) - \log P_G(x;\theta)\Big] dx\\&amp;amp;=\ \arg \min_{\theta} \int\limits_x P_{data}(x) \log \frac{P_{data}(x)}{P_G(x;\theta)} dx\\&amp;amp;=\ \arg \min_{\theta} KL(P_{data}||P_G)\end{align}\]</description>
    </item>
    
    <item>
      <title>Expectation-Maximization Algorithm</title>
      <link>/post/expectation-maximization-algorithm/</link>
      <pubDate>Mon, 24 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/expectation-maximization-algorithm/</guid>
      <description>From some on-line article, it is interesting that we can consider EM algorithm as some Chinese Kungfu manuals and it contains 9 levels of perspectives. With such metaphor, I can feel how this method powerful it is. Now, I want to share my view with you.
Introduction to the EM algorithmExpectation-Maximization (EM) algorithm is an important method to solve the maximum likelihood problem with latent variables in statistics. It is widely used in Machine Learning because it can simplify many difficult problems.</description>
    </item>
    
    <item>
      <title>Coordinate Descent Algorithm</title>
      <link>/post/coordinate-descent-algorithm/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/coordinate-descent-algorithm/</guid>
      <description>Coordinate Descent FrameworkAt the begining of this section, we start to discuss three different types of function.
Given convex, differentiable function \(f: \mathbb{R}^n \to \mathbb{R}\), we know \(f(x+\delta \cdot e_i)\geq f(x)\) for all \(\delta\) because \(\nabla f(x) = (\frac{\partial f}{\partial x_1}(x),\dots,\frac{\partial f}{\partial x_n}(x)) = 0\). Here, \(e_i = (0,\dots,1,\dots,0) \in \mathbb{R}^n\), the \(i\)th standard basis vactor.Figure1. Convex and differential function \(f\)Given convex but not differentiable function \(f\), we can not found a global minimizer.</description>
    </item>
    
  </channel>
</rss>