<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 3.3.0">
  <meta name="generator" content="Hugo 0.52" />
  <meta name="author" content="Thompson Hu">

  
  
  
  
    
  
  <meta name="description" content="From some on-line article, it is interesting that we can consider EM algorithm as some Chinese Kungfu manuals and it contains 9 levels of perspectives. With such metaphor, I can feel how this method powerful it is. Now, I want to share my view with you.
Introduction to the EM algorithmExpectation-Maximization (EM) algorithm is an important method to solve the maximum likelihood problem with latent variables in statistics. It is widely used in Machine Learning because it can simplify many difficult problems.">

  
  <link rel="alternate" hreflang="en-us" href="../../post/expectation-maximization-algorithm/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  <link rel="stylesheet" href="../../styles.css">
  

  
  
  

  
  <link rel="alternate" href="../../index.xml" type="application/rss+xml" title="Bonbon Blog">
  <link rel="feed" href="../../index.xml" type="application/rss+xml" title="Bonbon Blog">
  

  <link rel="manifest" href="../../site.webmanifest">
  <link rel="icon" type="image/png" href="../../img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="../../img/icon-192.png">

  <link rel="canonical" href="../../post/expectation-maximization-algorithm/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Bonbon Blog">
  <meta property="og:url" content="/post/expectation-maximization-algorithm/">
  <meta property="og:title" content="Expectation-Maximization Algorithm | Bonbon Blog">
  <meta property="og:description" content="From some on-line article, it is interesting that we can consider EM algorithm as some Chinese Kungfu manuals and it contains 9 levels of perspectives. With such metaphor, I can feel how this method powerful it is. Now, I want to share my view with you.
Introduction to the EM algorithmExpectation-Maximization (EM) algorithm is an important method to solve the maximum likelihood problem with latent variables in statistics. It is widely used in Machine Learning because it can simplify many difficult problems."><meta property="og:image" content="/img/portrait.jpg">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-12-24T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2018-12-24T00:00:00&#43;00:00">
  

  

  

  <title>Expectation-Maximization Algorithm | Bonbon Blog</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="../../">Bonbon Blog</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="../../#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="../../#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Expectation-Maximization Algorithm</h1>

  

  
    

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Thompson Hu">
  </span>
  

  <span class="article-date">
    
    <meta content="2018-12-24 00:00:00 &#43;0000 UTC" itemprop="datePublished">
    <time datetime="2018-12-24 00:00:00 &#43;0000 UTC" itemprop="dateModified">
      Dec 24, 2018
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Thompson Hu">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    4 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="../../post/expectation-maximization-algorithm/#disqus_thread"></a>
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    
    <a href="../../categories/deep-learning/">Deep Learning</a>
    
  </span>
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Expectation-Maximization%20Algorithm&amp;url=%2fpost%2fexpectation-maximization-algorithm%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fpost%2fexpectation-maximization-algorithm%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2fexpectation-maximization-algorithm%2f&amp;title=Expectation-Maximization%20Algorithm"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fpost%2fexpectation-maximization-algorithm%2f&amp;title=Expectation-Maximization%20Algorithm"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Expectation-Maximization%20Algorithm&amp;body=%2fpost%2fexpectation-maximization-algorithm%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    















  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      


<p>From some on-line article, it is interesting that we can consider EM algorithm as some Chinese Kungfu manuals and it contains 9 levels of perspectives. With such metaphor, I can feel how this method powerful it is. Now, I want to share my view with you.</p>
<div id="introduction-to-the-em-algorithm" class="section level2">
<h2>Introduction to the EM algorithm</h2>
<p>Expectation-Maximization (EM) algorithm is an important method to solve the maximum likelihood problem with latent variables in statistics. It is widely used in Machine Learning because it can simplify many difficult problems. One of the famous and classical application is <strong>gaussian mixture model</strong>.</p>
<div id="basic-probability-theory" class="section level3">
<h3>Basic probability theory</h3>
<p>Let <span class="math inline">\(p(x|\theta)\)</span> be the probability density function of random variable <span class="math inline">\(x\)</span>, where <span class="math inline">\(\theta\)</span> is the parameter of the density function, then we know that the basic probability property is <span class="math display">\[
p(\text{x};\theta) \geq 0, \int_{-\infty}^{+\infty}p(\text{x};\theta) d\text{x} = 1.
\]</span></p>
<p>If we take the expectation of x, we get <span class="math display">\[
\mathbb{E}[\text{x}] = \int\text{x}p(\text{x};\theta) d\text{x}
\]</span> In the integral, we know that the <span class="math inline">\(\mathbb{E}[\text{x}]\)</span> involves <span class="math inline">\(\theta\)</span> but not <span class="math inline">\(\text{x}\)</span>.</p>
<p>If we generalize it to function and let <span class="math inline">\(f(\text{x})\)</span> be a function of <span class="math inline">\(\text{x}\)</span>. Similarly, the expectation of <span class="math inline">\(f(x)\)</span> is given by <span class="math display">\[
\mathbb{E}[f] = \int f(\text{x})p(\text{x};\theta) d\text{x}
\]</span></p>
<p>With the similar result, we know that the <span class="math inline">\(\mathbb{E}[f]\)</span> involves <span class="math inline">\(\theta\)</span> but not <span class="math inline">\(\text{x}\)</span>.</p>
</div>
<div id="motivation-of-the-em-algorithm" class="section level3">
<h3>Motivation of the EM algorithm</h3>
<p>At the beginning, we denote that</p>
<ul>
<li><span class="math inline">\(\text{X}\)</span>: Set of all observed data (incomplete-data)</li>
<li><span class="math inline">\(\text{Z}\)</span>: Set of all latent variables</li>
<li><span class="math inline">\(\theta\)</span>: Set of all model parameters</li>
<li><span class="math inline">\(\{ \text{X,Z} \}\)</span>: Each observation in <span class="math inline">\(\text{X}\)</span> is corresponding value of the latent variable <span class="math inline">\(\text{Z}\)</span> (complete-data)</li>
</ul>
<p>Then the log-likelihood function can be written as <span class="math display">\[
L(\text{X};\theta) = \ln p(\text{X};\theta) = \ln \{ \sum_{\text{Z}}p(\text{X}, \text{Z};\theta) \}
\]</span></p>
<p>It is too hard to straightly solve the problem with <span class="math inline">\(\ln\)</span> and <span class="math inline">\(\sum\)</span>. The likelihood function for the complete data set simply takes the form <span class="math inline">\(\ln p(\text{X,Z}|\theta)\)</span>, and we shall suppose that maximization of this complete-data log-likelihood function is straightforward.</p>
<p>In practice, we are not given the latent variable <span class="math inline">\(\text{Z}\)</span> but we know the posterior distribution <span class="math inline">\(p(\text{Z}|\text{X};\theta)\)</span>.</p>
</div>
<div id="the-level-1-of-the-em-algorithm" class="section level3">
<h3>The Level 1 of the EM algorithm</h3>
<p>In the level 1, we just need to know the basic knowledge of EM algorithm.</p>
<p>In the <span class="math inline">\(\mathbb{E}\)</span>-step, we use the current parameter values <span class="math inline">\(\theta_{\text{old}}\)</span> to find the posterior distribution of the latent variables given by <span class="math inline">\(p(\text{Z}|\text{X};\theta_{\text{old}})\)</span>.</p>
<p>Then, we would use this posterior distribution to find the expectation of the <strong>complete-data</strong> log-likelihood evaluated for some general parameter value <span class="math inline">\(\theta\)</span>. This expectation, denoted <span class="math inline">\(\mathcal{Q}(\theta,\theta_{\text{old}})\)</span>, is given by <span class="math display">\[
\mathcal{Q}(\theta,\theta_{\text{old}}) = \mathbb{E}_{\text{Z}|\text{X};\theta_{\text{old}}}[\ln p(\text{X,Z};\theta)] = \sum_{\text{Z}} \ln p(\text{X,Z};\theta)p(\text{Z}|\text{X};\theta_{\text{old}})
\]</span></p>
<p>In the <span class="math inline">\(\mathbb{M}\)</span> (Maximization) step, we determine the revised parameter estimate <span class="math inline">\(\theta_{\text{new}}\)</span> by maximizing the function <span class="math display">\[
\theta_{\text{new}} = \arg \max_\limits{\theta} \mathcal{Q}(\theta,\theta_{\text{old}})
\]</span></p>
<p>Since the <strong>complete-data</strong> log-likelihood involves unobversed data <span class="math inline">\(\text{Z}\)</span>, we use <strong>Expectation</strong> to eliminate the uncertainty, and the function <span class="math inline">\(\mathbb{E}_{\text{Z}|\text{X},\theta_{\text{old}}}[\ln p(\text{X,Z}|\theta)]\)</span> does not involve <span class="math inline">\(\text{Z}\)</span> but involve <span class="math inline">\((\theta,\theta_{\text{old}})\)</span>.</p>
<p>We can summarize the procedure as:</p>
<blockquote>
<p>While <span class="math inline">\(\theta_{\text{new}} - \theta_{old} &gt; \epsilon\)</span><br />
<span class="math inline">\(\quad\)</span> Expectation-Step on log likelihood function: <span class="math display">\[
\mathcal{Q}(\theta,\theta_{\text{old}}) = \mathbb{E}_{\text{Z}|\text{X};\theta_{\text{old}}}[\ln p(\text{X,Z};\theta)]= \sum_{\text{Z}} \ln p(\text{X,Z};\theta)p(\text{Z}|\text{X};\theta_{\text{old}})
\]</span> <span class="math inline">\(\quad\)</span> Maximization-Step on <span class="math inline">\(\mathcal{Q}(\theta,\theta_{\text{old}})\)</span>: <span class="math display">\[
\theta_{\text{new}} = \arg \max_\limits{\theta} \mathcal{Q}(\theta,\theta_{\text{old}})
\]</span></p>
</blockquote>
</div>
<div id="the-level-2-of-the-em-algorithm" class="section level3">
<h3>The Level 2 of the EM algorithm</h3>
<p>After writting down the pseudocode of EM algorithm, we want to know the reason that we can use expectation to approximate the maximum log-likelihood by repeating <strong>Expectation</strong> and <strong>Maximization</strong>. In other words, we want to prove that <span class="math display">\[
\arg \max_\theta \mathbb{E}_{\text{Z}|\text{X};\theta_{\text{old}}}[\ln p(\text{X};\theta)] \approx \arg \max_\theta \ln p(\text{X};\theta)
\]</span> where the joint distribution <span class="math inline">\(p(\text{X}, \text{Z};\theta)\)</span> is governed by a set of parameters <span class="math inline">\(\theta\)</span>.</p>
<p>Next we introduce a distribution <span class="math inline">\(q(\text{Z})\)</span> defined over the latent variables.</p>
<p>Since <span class="math display">\[
p(\text{X},\text{Z};\theta) = p(\text{Z}|\text{X};\theta)p(\text{X};\theta)
\]</span> and <span class="math display">\[
\sum_\text{Z}q(\text{Z}) = 1
\]</span> we can get decomposition by <span class="math display">\[
\begin{align}
\ln p(\text{X};\theta) =&amp;\ \ln \frac{p(\text{X},\text{Z};\theta)}{p(\text{Z}|\text{X};\theta)}\\
=&amp;\ \ln p(\text{X},\text{Z};\theta) - \ln p(\text{Z}|\text{X};\theta) + \ln q(\text{Z}) - \ln q(\text{Z})\\
=&amp;\ \ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})} - \ln \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}\\
=&amp; \sum_\text{Z}q(\text{Z}) \{ \ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})} - \ln \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})} \}\\
=&amp;\ \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})} - \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}\\
=&amp;\ \mathcal{L}(q,\theta) + KL(q||p)
\end{align}
\]</span> where <span class="math display">\[
\begin{align}
\mathcal{L}(q,\theta) = \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})}\\
KL(q||p) = - \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}
\end{align}
\]</span></p>
<p>We call the <span class="math inline">\(KL(q||p)\)</span> as the Kullback-Leibler divergence (KL divergence, also known as relative entropy).</p>
<p>Recall that <strong>Jensen’s inequality</strong> holds for convex function <span class="math inline">\(f(x)\)</span>. <span class="math display">\[
\mathbb{E}[f(x)] \geq f(\mathbb{E}[x])
\]</span></p>
<p>Applying Jensen’s inequality in KL divergence, we have <span class="math display">\[
\begin{align}
KL(q||p) =&amp; - \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})} = -\mathbb{E}_q[\ln\{\frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}\} ]\\
\geq&amp; -\ln \mathbb{\text{E}_q}[\frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}] = -\ln \sum_\text{Z} q(\text{Z}) \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}\\
=&amp; -\ln \sum_\text{Z} p(\text{Z}|\text{X};\theta) = -\ln 1\\ =&amp;\ 0
\end{align}
\]</span></p>
<p>If we let <span class="math inline">\(q(\text{Z}) = p(\text{Z}|\text{X};\theta)\)</span>, and <span class="math inline">\(p(\text{X},\text{Z};\theta) = p(\text{Z}|\text{X};\theta)p(\text{X};\theta)\)</span>, then <span class="math display">\[
\begin{align}
\mathcal{L}(q,\theta) =&amp; \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})} = \mathbb{E}_q[\ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})}]\\
=&amp;\ \mathbb{E}_{\text{Z}|\text{X};\theta}[\ln p(X;\theta)]
\end{align}
\]</span></p>
<p>Thus, we can get the result that <span class="math inline">\(\ln p(\text{X};\theta) \geq \mathcal{L}(q,\theta) = \mathbb{E}_{\text{Z}|\text{X};\theta}[\ln p(X;\theta)]\)</span>, so we can say that <span class="math inline">\(\mathcal{L}(q,\theta) = \mathbb{E}_{\text{Z}|\text{X};\theta}[\ln p(X;\theta)]\)</span> is the low bound of <span class="math inline">\(\ln p(\text{X};\theta)\)</span>.</p>
<div align="center">
<img src="EM1.PNG" width = "300" height = "300" alt="Figure1" />
<p>
Figure1. The EM algorithm involves alternatel computing a lower bound on the log likelihood for the current parameter values and then maximizing this bound to obtain the new parameter values.
</div>
<p>Actually, EM algorithm is one of the special case of Minorize-Maximization (MM) algorithm, and <span class="math inline">\(\mathcal{L}(q,\theta)\)</span> can be considered as the <strong>surrogate function</strong> in MM algorithm.</p>
</div>
</div>

    </div>

    


<div class="article-tags">
  
  <a class="badge badge-light" href="../../tags/algorithm/">Algorithm</a>
  
</div>



    






<div class="media author-card" itemscope itemtype="http://schema.org/Person">
  
  <img class="portrait mr-3" src="../../img/portrait.jpg" itemprop="image" alt="Avatar">
  
  <div class="media-body">
    <h5 class="card-title" itemprop="name"><a href="../../">Thompson Hu</a></h5>
    
    
    <ul class="network-icon" aria-hidden="true">
      
      
      
      
        
      
      
      
      
      
      <li>
        <a itemprop="sameAs" href="mailto:ttxinlinhu@qq.com" >
          <i class="fas fa-envelope"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://github.com/thompsonhu" target="_blank" rel="noopener">
          <i class="fab fa-github"></i>
        </a>
      </li>
      
    </ul>
  </div>
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="../../post/coordinate-descent-algorithm/">Coordinate Descent Algorithm</a></li>
        
      </ul>
    </div>
    

    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "thompsonhu" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<div class="container">
  <footer class="site-footer">
  
  <p class="powered-by">
    <a href="../../privacy/">Made by Thompson</a>
  </p>
  

  <p class="powered-by">
    &copy; 2018 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    
    <script src="../../js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//thompsonhu.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    <script src="../../js/academic.min.70f0041f5a24c6a675ac218c98d7ef71.js"></script>

    

  </body>
</html>

