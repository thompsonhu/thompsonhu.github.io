<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 3.3.0">
  <meta name="generator" content="Hugo 0.52" />
  <meta name="author" content="Thompson Hu">

  
  
  
  
    
  
  <meta name="description" content="前言 循环神经网络（Recurrent Nerual Network, RNN）在Deep Learning领域中是一个经典有很重要的神经网络模型。RNN是在自然语言处理（Natural Language Processing, NLP）领域最先被使用发展起来的。在NLP中通常会处理一些文字句子，比如我们想做一个机器翻译，把中文转换成英文。初中生可能在翻译的时候只会逐个中文词翻译成英文，但高中生就可能会对翻译中的词进行调整，结合前后的词汇，那我们也希望机器这样做。
假设有一段对话：
A：你好吗？
B：我很好。
我们想让机器翻译成英文，对于A而言，机器可能在一些训练后很容易翻译成“How are you?”，但对于B可能就翻译成了“I’m ok.”。那就相当尴尬了~（想起某军的Are you ok?╭(●｀∀´●)╯）机器其实需要参考A问了什么，再来对B的回答进行翻译，才能获得比较好的回答翻译“I’m fine.”
RNN就是专门解决了处理序列化数据的问题，像上面这样的小例子。
 简单循环神经网络 对于简单RNN，它由输入层，一个隐藏层和一个输出层构成的，像这样子：  图1. 简单RNN示意图  先看左边的图时会觉得不能理解，但是如果将其展开得到右边的图，就比较好理解了。在\(t\)时刻，\(x_t\)作为输入的同时，还有上一个时刻隐藏层的\(h_{t-1}\)，以\(V\)为权重作为第\(t\)时刻的输入。我们用计算公式来表示，可以表示为： \[ h_t = f (U x_t &#43; V h_{t-1})\\ o_t = \sigma (W h_t) \] 其中\(f\)和\(\sigma\)是激活函数（Activation function）。
如果将上面两个式子联合可以得到： \[ \begin{align} o_t &amp;= \sigma (W h_t)\\ &amp;= \sigma (W f (U x_t &#43; V h_{t-1}))\\ &amp;= \sigma (W f (U x_t &#43; V f (U x_{t-1} &#43; V h_{t-2})))\\ &amp;=\ .">

  
  <link rel="alternate" hreflang="en-us" href="../../post/recurrent-nerual-network-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  <link rel="stylesheet" href="../../styles.css">
  

  
  
  

  
  <link rel="alternate" href="../../index.xml" type="application/rss+xml" title="Bonbon Blog">
  <link rel="feed" href="../../index.xml" type="application/rss+xml" title="Bonbon Blog">
  

  <link rel="manifest" href="../../site.webmanifest">
  <link rel="icon" type="image/png" href="../../img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="../../img/icon-192.png">

  <link rel="canonical" href="../../post/recurrent-nerual-network-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Bonbon Blog">
  <meta property="og:url" content="/post/recurrent-nerual-network-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
  <meta property="og:title" content="Recurrent Nerual Network 循环神经网络 | Bonbon Blog">
  <meta property="og:description" content="前言 循环神经网络（Recurrent Nerual Network, RNN）在Deep Learning领域中是一个经典有很重要的神经网络模型。RNN是在自然语言处理（Natural Language Processing, NLP）领域最先被使用发展起来的。在NLP中通常会处理一些文字句子，比如我们想做一个机器翻译，把中文转换成英文。初中生可能在翻译的时候只会逐个中文词翻译成英文，但高中生就可能会对翻译中的词进行调整，结合前后的词汇，那我们也希望机器这样做。
假设有一段对话：
A：你好吗？
B：我很好。
我们想让机器翻译成英文，对于A而言，机器可能在一些训练后很容易翻译成“How are you?”，但对于B可能就翻译成了“I’m ok.”。那就相当尴尬了~（想起某军的Are you ok?╭(●｀∀´●)╯）机器其实需要参考A问了什么，再来对B的回答进行翻译，才能获得比较好的回答翻译“I’m fine.”
RNN就是专门解决了处理序列化数据的问题，像上面这样的小例子。
 简单循环神经网络 对于简单RNN，它由输入层，一个隐藏层和一个输出层构成的，像这样子：  图1. 简单RNN示意图  先看左边的图时会觉得不能理解，但是如果将其展开得到右边的图，就比较好理解了。在\(t\)时刻，\(x_t\)作为输入的同时，还有上一个时刻隐藏层的\(h_{t-1}\)，以\(V\)为权重作为第\(t\)时刻的输入。我们用计算公式来表示，可以表示为： \[ h_t = f (U x_t &#43; V h_{t-1})\\ o_t = \sigma (W h_t) \] 其中\(f\)和\(\sigma\)是激活函数（Activation function）。
如果将上面两个式子联合可以得到： \[ \begin{align} o_t &amp;= \sigma (W h_t)\\ &amp;= \sigma (W f (U x_t &#43; V h_{t-1}))\\ &amp;= \sigma (W f (U x_t &#43; V f (U x_{t-1} &#43; V h_{t-2})))\\ &amp;=\ ."><meta property="og:image" content="/img/portrait.jpg">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-02-01T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2019-02-01T00:00:00&#43;00:00">
  

  

  

  <title>Recurrent Nerual Network 循环神经网络 | Bonbon Blog</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="../../">Bonbon Blog</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="../../#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="../../#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Recurrent Nerual Network 循环神经网络</h1>

  

  
    

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Thompson Hu">
  </span>
  

  <span class="article-date">
    
    <meta content="2019-02-01 00:00:00 &#43;0000 UTC" itemprop="datePublished">
    <time datetime="2019-02-01 00:00:00 &#43;0000 UTC" itemprop="dateModified">
      Feb 1, 2019
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Thompson Hu">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    7 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="../../post/recurrent-nerual-network-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/#disqus_thread"></a>
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    
    <a href="../../categories/deep-learning/">Deep Learning</a>
    
  </span>
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Recurrent%20Nerual%20Network%20%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&amp;url=%2fpost%2frecurrent-nerual-network-%25E5%25BE%25AA%25E7%258E%25AF%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fpost%2frecurrent-nerual-network-%25E5%25BE%25AA%25E7%258E%25AF%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2frecurrent-nerual-network-%25E5%25BE%25AA%25E7%258E%25AF%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%2f&amp;title=Recurrent%20Nerual%20Network%20%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fpost%2frecurrent-nerual-network-%25E5%25BE%25AA%25E7%258E%25AF%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%2f&amp;title=Recurrent%20Nerual%20Network%20%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Recurrent%20Nerual%20Network%20%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&amp;body=%2fpost%2frecurrent-nerual-network-%25E5%25BE%25AA%25E7%258E%25AF%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    















  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      


<div class="section level2">
<h2>前言</h2>
<p>循环神经网络（Recurrent Nerual Network, RNN）在Deep Learning领域中是一个经典有很重要的神经网络模型。RNN是在<strong>自然语言处理（Natural Language Processing, NLP）</strong>领域最先被使用发展起来的。在NLP中通常会处理一些文字句子，比如我们想做一个机器翻译，把中文转换成英文。初中生可能在翻译的时候只会逐个中文词翻译成英文，但高中生就可能会对翻译中的词进行调整，结合前后的词汇，那我们也希望机器这样做。</p>
<p>假设有一段对话：</p>
<p>A：你好吗？<br />
B：我很好。</p>
<p>我们想让机器翻译成英文，对于A而言，机器可能在一些训练后很容易翻译成“How are you?”，但对于B可能就翻译成了“I’m ok.”。那就相当尴尬了~（想起某军的Are you ok?╭(●｀∀´●)╯）机器其实需要参考A问了什么，再来对B的回答进行翻译，才能获得比较好的回答翻译“I’m fine.”</p>
<p>RNN就是专门解决了处理序列化数据的问题，像上面这样的小例子。</p>
</div>
<div class="section level2">
<h2>简单循环神经网络</h2>
对于简单RNN，它由输入层，一个隐藏层和一个输出层构成的，像这样子：
<div align="center">
<img src="RNN1.PNG" width = "700" height = "600" alt="Figure1" />
<p>
图1. 简单RNN示意图
</div>
<p>先看左边的图时会觉得不能理解，但是如果将其展开得到右边的图，就比较好理解了。在<span class="math inline">\(t\)</span>时刻，<span class="math inline">\(x_t\)</span>作为输入的同时，还有上一个时刻隐藏层的<span class="math inline">\(h_{t-1}\)</span>，以<span class="math inline">\(V\)</span>为权重作为第<span class="math inline">\(t\)</span>时刻的输入。我们用计算公式来表示，可以表示为： <span class="math display">\[
h_t = f (U x_t + V h_{t-1})\\
o_t = \sigma (W h_t)
\]</span> 其中<span class="math inline">\(f\)</span>和<span class="math inline">\(\sigma\)</span>是激活函数（Activation function）。</p>
<p>如果将上面两个式子联合可以得到： <span class="math display">\[
\begin{align}
o_t &amp;= \sigma (W h_t)\\
&amp;= \sigma (W f (U x_t + V h_{t-1}))\\
&amp;= \sigma (W f (U x_t + V f (U x_{t-1} + V h_{t-2})))\\
&amp;=\ ...
\end{align}
\]</span> 可以看到我们会得到一个循环向前的公式，循环神经网络的输出值<span class="math inline">\(o_t\)</span>是受到前面历次输入值<span class="math inline">\(x_t, x_{t-1}, x_{t-2}, \dots\)</span>的影响的。</p>
</div>
<div id="rnn-" class="section level2">
<h2>深度RNN &amp; 梯度消失与爆炸</h2>
在介绍完简单RNN后，我们会考虑添加更多的隐藏层，那就可以得到深度RNN（如图2）；在这样多个隐藏层的神经网络结构中，我们通过<strong>反向传播</strong>就可以计算更新网络的参数。
<div align="center">
<img src="RNN2.PNG" width = "400" height = "500" alt="Figure2" />
<p>
图2. 深度RNN示意图
</div>
<p>当神经网络中层数太多的话，可能就会出现<strong>梯度消失</strong>。举个例子：</p>
<p>假设我们用了激活函数sigmoid函数 <span class="math display">\[
f(x) = \frac{1}{1+e^{-x}}
\]</span></p>
<p>它的导数为 <span class="math display">\[
f&#39;(x) = f(x)(1-f(x))
\]</span></p>
<p>我们知道<span class="math inline">\(1 + e^{-x} &gt; 1\)</span>，所以可以得到<span class="math inline">\(0 &lt; f(x) &lt; 1\)</span>。</p>
<p>对于导数<span class="math inline">\(f&#39;(x)\)</span>，我们可以通过<span class="math inline">\(-\frac{b}{2a}\)</span>计算得到，当<span class="math inline">\(f(x) = 0.5\)</span>时，<span class="math inline">\(f&#39;(x)\)</span>取得最大值为<span class="math inline">\(0.25\)</span>。 <img src="../../post/2019-02-01-recurrent-nerual-network-循环神经网络_files/figure-html/unnamed-chunk-1-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>在反向传播计算过程中，每经过一个sigmoid函数，便需要乘以一个sigmoid函数的导数值，那残差就至少会被削减变为原来的0.25倍；如果整个循环神经网络的时间长度很长，那么可能残差传递到前面的某一时刻就基本为0了；这样会导致在往前的时刻中无法更新参数，这就是<strong>梯度消失</strong>。</p>
<p>同理，如果导数的最小值是不小于1的，那么残差在传递中就会一次比一次大，如果整个循环神经网络的时间长度很长，那每次参数的更新就会非常剧烈，这就是<strong>梯度爆炸</strong>。下面用数学推导表示梯度消失与梯度爆炸。</p>
<div class="section level3">
<h3>数学推导下的梯度消失与梯度爆炸</h3>
<p>我们在前面的时候用计算公式表达了简单RNN： <span class="math display">\[
h_t = f (U x_t + V h_{t-1})\\
o_t = \sigma (W h_t)
\]</span></p>
<p>如果将公式写的更明细的话，可以表达为： <span class="math display">\[
\begin{bmatrix}
h_1^t\\ h_2^t\\ .\\ .\\ h_n^t 
\end{bmatrix}
=
f\bigg(
\begin{bmatrix}
u_{11} &amp; u_{12} &amp; \cdots &amp; u_{1m}\\
u_{21} &amp; u_{22} &amp; \cdots &amp; u_{2m}\\
. &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; .\\
u_{n1} &amp; u_{n2} &amp; \cdots &amp; u_{nm}\\
\end{bmatrix}
\begin{bmatrix}
x_1\\ x_2\\ .\\ .\\ x_m 
\end{bmatrix}
+
\begin{bmatrix}
v_{11} &amp; v_{12} &amp; \cdots &amp; v_{1n}\\
v_{21} &amp; v_{22} &amp; \cdots &amp; v_{2n}\\
. &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; .\\
v_{n1} &amp; v_{n2} &amp; \cdots &amp; v_{nn}\\
\end{bmatrix}
\begin{bmatrix}
h_1^{t-1}\\ h_2^{t-1}\\ .\\ .\\ h_n^{t-1} 
\end{bmatrix}
\bigg)
\]</span></p>
<p>其中，<span class="math inline">\(h_j^t\)</span>表示向量<span class="math inline">\(h\)</span>的第<span class="math inline">\(j\)</span>个元素在<span class="math inline">\(t\)</span>时刻的值；<span class="math inline">\(u_ji\)</span>表示输入层第<span class="math inline">\(i\)</span>个神经元到循环层第<span class="math inline">\(j\)</span>个神经元的权重；<span class="math inline">\(w_ij\)</span>表示循环层第<span class="math inline">\(t-1\)</span>时刻的第<span class="math inline">\(i\)</span>个神经元到循环层第<span class="math inline">\(t\)</span>个时刻的第<span class="math inline">\(j\)</span>个神经元的权重。</p>
<p>我们用向量<span class="math inline">\(net_t\)</span>来表示神经元在<span class="math inline">\(t\)</span>时刻的<strong>加权输入</strong>： <span class="math display">\[
net_t = Ux_t + Vh_{t-1}\\
h_t = f(net_t)
\]</span> 同样有： <span class="math display">\[
h_{t-1} = f(net_{t-1})
\]</span> 因此： <span class="math display">\[
\frac{\partial net_t}{\partial net_{t-1}} = \frac{\partial net_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial net_{t-1}}
\]</span> 由<span class="math inline">\(net_t = Ux_t + Vh_{t-1}\)</span>式子，右边等式的第一部分可以推导得到： <span class="math display">\[
\begin{align}
\frac{\partial net_t}{\partial h_{t-1}} &amp; = \begin{bmatrix}
\frac{\partial net_1^t}{\partial h_1^{t-1}} &amp; \frac{\partial net_1^t}{\partial h_2^{t-1}} &amp; \cdots &amp; \frac{\partial net_1^t}{\partial h_n^{t-1}}\\
\frac{\partial net_2^t}{\partial h_1^{t-1}} &amp; \frac{\partial net_2^t}{\partial h_2^{t-1}} &amp; \cdots &amp; \frac{\partial net_2^t}{\partial h_n^{t-1}}\\
. &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; .\\
\frac{\partial net_n^t}{\partial h_1^{t-1}} &amp; \frac{\partial net_n^t}{\partial h_2^{t-1}} &amp; \cdots &amp; \frac{\partial net_n^t}{\partial h_n^{t-1}}\\
\end{bmatrix}\\
&amp;= \begin{bmatrix}
v_{11} &amp; v_{12} &amp; \cdots &amp; v_{1n}\\
v_{21} &amp; v_{22} &amp; \cdots &amp; v_{2n}\\
. &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; .\\
v_{n1} &amp; v_{n2} &amp; \cdots &amp; v_{nn}\\
\end{bmatrix}\\
&amp;= V
\end{align}
\]</span> 同理由<span class="math inline">\(h_{t-1} = f(net_{t-1})\)</span>式子，右边等式的第二部分可以推导得到： <span class="math display">\[
\begin{align}
\frac{\partial h_{t-1}}{\partial net_{t-1}} &amp;= \begin{bmatrix}
\frac{\partial h_1^{t-1}}{\partial net_1^{t-1}} &amp; \frac{\partial h_1^{t-1}}{\partial net_2^{t-1}} &amp; \cdots &amp; \frac{\partial h_1^{t-1}}{\partial net_n^{t-1}}\\
\frac{\partial h_2^{t-1}}{\partial net_1^{t-1}} &amp; \frac{\partial h_2^{t-1}}{\partial net_2^{t-1}} &amp; \cdots &amp; \frac{\partial h_2^{t-1}}{\partial net_n^{t-1}}\\
. &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; .\\
\frac{\partial h_n^{t-1}}{\partial net_1^{t-1}} &amp; \frac{\partial h_n^{t-1}}{\partial net_2^{t-1}} &amp; \cdots &amp; \frac{\partial h_n^{t-1}}{\partial net_n^{t-1}}\\
\end{bmatrix}\\
&amp;= \begin{bmatrix}
f&#39;(net_1^{t-1}) &amp; 0 &amp; \cdots &amp; 0\\
0 &amp; f&#39;(net_2^{t-1}) &amp; \cdots &amp; 0\\
. &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; .\\
0 &amp; 0 &amp; \cdots &amp; f&#39;(net_n^{t-1})\\
\end{bmatrix}\\
&amp;= diag[f&#39;(net_{t-1})]
\end{align}
\]</span> 因此我们可以得到： <span class="math display">\[
\begin{align}
\frac{\partial net_t}{\partial net_{t-1}} &amp;= \frac{\partial net_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial net_{t-1}}\\
&amp;= V diag[f&#39;(net_{t-1})]\\
&amp;= \begin{bmatrix}
v_{11}f&#39;(net_1^{t-1}) &amp; v_{12}f&#39;(net_2^{t-1}) &amp; \cdots &amp; v_{1n}f&#39;(net_n^{t-1})\\
v_{21}f&#39;(net_1^{t-1}) &amp; v_{22}f&#39;(net_2^{t-1}) &amp; \cdots &amp; v_{2n}f&#39;(net_n^{t-1})\\
. &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; .\\
v_{n1}f&#39;(net_1^{t-1}) &amp; v_{n2}f&#39;(net_2^{t-1}) &amp; \cdots &amp; v_{nn}f&#39;(net_n^{t-1})\\
\end{bmatrix}\\
\end{align}
\]</span></p>
<p>我们可以求得任意时刻<span class="math inline">\(k\)</span>的残差值<span class="math inline">\(\delta_k\)</span>： <span class="math display">\[
\begin{align}
\delta_k &amp;= \frac{\partial E}{\partial net_k}\\
&amp;= \frac{\partial E}{\partial net_t}\frac{\partial net_t}{\partial net_k}\\
&amp;= \frac{\partial E}{\partial net_t}\frac{\partial net_t}{\partial net_{t-1}}\frac{\partial net_{t-1}}{\partial net_{t-2}}\cdots\frac{\partial net_{k+1}}{\partial net_k}\\
&amp;= \delta_t V diag[f&#39;(net_{t-1})]V diag[f&#39;(net_{t-2})]\cdots V diag[f&#39;(net_k)]\\
&amp;= \delta_t \prod_{i=k}^{t-1}V diag[f&#39;(net_i)]
\end{align}
\]</span> 从上面式子我们可以得到： <span class="math display">\[
\begin{align}
&amp; \delta_k = \delta_t \prod_{i=k}^{t-1}V diag[f&#39;(net_i)]\\
\Rightarrow\ \ &amp; ||\delta_k|| \leq ||\delta_t|| \prod_{i=k}^{t-1} ||V|| \cdot ||diag[f&#39;(net_i)]||\\
\end{align}
\]</span> 其中模可以看作对<span class="math inline">\(\delta_k\)</span>中每一项值得大小的度量，假定<span class="math inline">\(\beta_f\)</span>和<span class="math inline">\(\beta_W\)</span>分别是<span class="math inline">\(||diag[f&#39;(net_i)]||\)</span>和<span class="math inline">\(||V||\)</span>的上界，那么 <span class="math display">\[
||\delta_k|| \leq ||\delta_t|| (\beta_f \beta_W)^{t-k}
\]</span> 显然，如果<span class="math inline">\(\beta_f \beta_W\)</span>的乘积比1小或者比1大时，由于指数的缘故，那么残差就可能会变得很小几乎为0或者变得很大。前者就是<strong>梯度消失</strong>，后者就是<strong>梯度爆炸</strong>。所以，普通的RNN是无法处理长距离依赖的原因。经过科学家们的努力，终于在1997年，由Sepp Hochreiter和Jürgen Schmidhuber两位科学家提出了长短时间记忆（LSTM），解决了RNN这样的问题。</p>
</div>
</div>
<div id="long-short-term-memory-lstm" class="section level2">
<h2>长短时间记忆（Long Short Term Memory, LSTM）</h2>
长短时间记忆网络在原来普通RNN的基础上再加了一个状态<span class="math inline">\(c\)</span>，用来保存长期的状态，这个状态也称为<strong>单元状态（cell state）</strong>。
<div align="center">
<img src="RNN3.PNG" width = "700" height = "500" alt="Figure3" />
<p>
图3. 普通RNN和LSTM
</div>
<p>LSTM中输入有三个：<span class="math inline">\(x_t\)</span>，<span class="math inline">\(h_{t-1}\)</span>和<span class="math inline">\(c_{t-1}\)</span>；而输出有两个：<span class="math inline">\(h_t\)</span>和<span class="math inline">\(c_t\)</span>；另外，LSTM引入了“门”或者“开关”这样的概念，用来控制不同阶段的数据输入和输出，分别是：</p>
<ul>
<li><strong>遗忘门（Forget gate）</strong>：控制单元状态<span class="math inline">\(c_{t-1}\)</span>保留多少到<span class="math inline">\(c_t\)</span>；它的计算公式为： <span class="math display">\[
f_t = \sigma(U^{(f)}x_t + V^{(f)}h_{t-1})
\]</span> 其中，<span class="math inline">\(\sigma\)</span>是激活函数（一般sigmoid函数）。
<div align="center">
<img src="RNN4.PNG" width = "700" height = "600" alt="Figure4" />
<p>
图4. 遗忘门示意图
</div>
结合上一时刻单元状态<span class="math inline">\(c_{t-1}\)</span>，<span class="math inline">\(f_t\)</span>决定了保留多少上一时刻单元状态，所以用乘积计算，计算公式为： <span class="math display">\[
c_t = f_t\circ c_{t-1}
\]</span>
<div align="center">
<img src="RNN5.PNG" width = "700" height = "600" alt="Figure5" />
<p>
图5. 遗忘门结合上一时刻单元状态示意图
</div>
<p>这里的<span class="math inline">\(c_t\)</span>只是计算过程中一个值，还不是输出时的单元状态。</p></li>
<li><strong>输入门（Input gate）</strong>：控制输入<span class="math inline">\(x_t\)</span>保存多少到<span class="math inline">\(c_t\)</span>；它的计算公式为： <span class="math display">\[
i_t = \sigma(U^{(i)}x_t + V^{(i)}h_{t-1})\\
\tilde{c}_t = \tanh (U^{(c)}x_t + V^{(c)}h_{t-1})
\]</span>
<div align="center">
<img src="RNN6.PNG" width = "700" height = "600" alt="Figure6" />
<p>
图6. 输入门示意图</li>
</ul>
</div>
其中，我们把<span class="math inline">\(\tilde{c}_t\)</span>称为“候选状态”，结合从遗忘门出来的<span class="math inline">\(c_t\)</span>，我们将三者结合，计算公式为： <span class="math display">\[
\begin{align}
c_t &amp;= c_t + i_t\circ \tilde{c}_t\\
\Rightarrow c_t &amp;= f_t\circ c_{t-1} + i_t\circ \tilde{c}_t
\end{align}
\]</span>
<div align="center">
<img src="RNN7.PNG" width = "700" height = "600" alt="Figure7" />
<p>
图7. 输入门结合候选状态和单元状态示意图
</div>
<ul>
<li><strong>输出门（Output gate）</strong>：控制单元状态<span class="math inline">\(c_t\)</span>输出多少到<span class="math inline">\(h_t\)</span>；它的计算公式为： <span class="math display">\[
e_t = \sigma(U^{(e)}x_t + V^{(e)}h_{t-1})\\
h_t = \tanh (c_t)\circ e_t
\]</span>
<div align="center">
<img src="RNN8.PNG" width = "700" height = "600" alt="Figure8" />
<p>
图8. 输出门结合单元状态示意图
</div></li>
</ul>
<p>上面提及的参数在初始化设置为接近0的较小值，对于长短时间记忆网络的训练，让我们慢慢叙述~</p>
<div id="lstm" class="section level3">
<h3>LSTM的训练</h3>
<p><strong>残差值沿时间的反向传播计算</strong></p>
<p>在推导之前，我们先假定每个gate的激活函数为sigmoid函数，而其中我们还涉及到了tanh函数，它们的公式及导数分别为： <span class="math display">\[
\sigma(x) = \frac{1}{1+e^{-x}}\\
\sigma&#39;(x) = \sigma(x)(1-\sigma(x))\\
\tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}\\
\tanh&#39;(x) = 1-\tanh(x)^2
\]</span> 显然，它们的导数均可以用它们本身进行表示，在计算过程中，我们只需要计算它们各自的值，就可以计算得到对应导数的值。</p>
<p>另外，对于符号<span class="math inline">\(\circ\)</span>，当作用于向量时，它的运算时对应元素的相乘，即： <span class="math display">\[
\text{a}\circ \text{b} = \begin{bmatrix}
a_1\\ a_2\\ a_3\\ \cdots\\ a_n
\end{bmatrix}
\circ
\begin{bmatrix}
b_1\\ b_2\\ b_3\\ \cdots\\ b_n
\end{bmatrix}
=
\begin{bmatrix}
a_1b_1\\ a_2b_2\\ a_3b_3\\ \cdots\\ a_nb_n
\end{bmatrix}
\]</span> 当作用于向量和矩阵时，它的计算为： <span class="math display">\[
\begin{align}
\text{a}\ \circ \text{X} &amp;= \begin{bmatrix}
a_1\\ a_2\\ a_3\\ \cdots\\ a_n
\end{bmatrix}
\circ
\begin{bmatrix}
x_{11} &amp; x_{12} &amp; x_{13} &amp; \cdots &amp; x_{1n}\\
x_{21} &amp; x_{22} &amp; x_{23} &amp; \cdots &amp; x_{2n}\\
x_{31} &amp; x_{32} &amp; x_{33} &amp; \cdots &amp; x_{3n}\\
 &amp; &amp; \cdots\\
x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; \cdots &amp; x_{nn}
\end{bmatrix}\\
&amp;=
\begin{bmatrix}
a_1x_{11} &amp; a_1x_{12} &amp; a_1x_{13} &amp; \cdots &amp; a_1x_{1n}\\
a_2x_{21} &amp; a_2x_{22} &amp; a_2x_{23} &amp; \cdots &amp; a_2x_{2n}\\
a_3x_{31} &amp; a_3x_{32} &amp; a_3x_{33} &amp; \cdots &amp; a_3x_{3n}\\
 &amp; &amp; \cdots\\
a_nx_{n1} &amp; a_nx_{n2} &amp; a_nx_{n3} &amp; \cdots &amp; a_nx_{nn}
\end{bmatrix}
\end{align}
\]</span> 当作用于矩阵和矩阵时，两个矩阵的对应位置的元素相乘。</p>
<p>同时，我们定义在<span class="math inline">\(t\)</span>时刻，LSTM中的输出值为<span class="math inline">\(h_t\)</span>，残差值<span class="math inline">\(\delta_t\)</span>为<span class="math inline">\(\delta_t = \frac{\partial E}{\partial h_t}\)</span>。</p>
<p>利用反向传播算法，我们需要得到的是前面时刻的残差值，假定现在我们先求上一时刻<span class="math inline">\(t-1\)</span>时刻的残差值<span class="math inline">\(\delta_{t-1}\)</span>。 <span class="math display">\[
\begin{align}
\delta_{t-1} &amp;= \frac{\partial E}{\partial h_{t-1}}\\
&amp;= \frac{\partial E}{\partial h_t}\frac{\partial h_t}{\partial h_{t-1}}\\
&amp;= \delta_t\frac{\partial h_t}{\partial h_{t-1}}
\end{align}
\]</span> 由前面的推导，我们知道 <span class="math display">\[
h_t = \tanh(c_t)\circ e_t\\
c_t = f_t\circ c_{t-1} + i_t\circ \tilde{c}_t\\
f_t = \sigma(U^{(f)}x_t + V^{(f)}h_{t-1})\\
i_t = \sigma(U^{(i)}x_t + V^{(i)}h_{t-1})\\
\tilde{c}_t = \tanh (U^{(c)}x_t + V^{(c)}h_{t-1})\\
e_t = \sigma(U^{(e)}x_t + V^{(e)}h_{t-1})
\]</span> 对于导数<span class="math inline">\(\frac{\partial h_t}{\partial h_{t-1}}\)</span>，我们得到： <span class="math display">\[
\frac{\partial h_t}{\partial h_{t-1}} = \frac{\partial h_t}{\partial e_t}\frac{\partial e_t}{\partial h_{t-1}} + \frac{\partial h_t}{\partial c_t}\frac{\partial c_t}{\partial f_t}\frac{\partial f_t}{\partial h_{t-1}} + \frac{\partial h_t}{\partial c_t}\frac{\partial c_t}{\partial i_t}\frac{\partial i_t}{\partial h_{t-1}} + \frac{\partial h_t}{\partial c_t}\frac{\partial c_t}{\partial \tilde{c}_t}\frac{\partial \tilde{c}_t}{\partial h_{t-1}}
\]</span> 由于 <span class="math display">\[
\begin{align}
\frac{\partial h_t}{\partial e_t} &amp;= diag[\tanh(c_t)]\\
\frac{\partial e_t}{\partial h_{t-1}} &amp;= diag[e_t\circ (1-e_t)]\ V^{(e)}\\
\frac{\partial h_t}{\partial c_t} &amp;= diag[e_t\circ (1-\tanh(c_t)^2)]\\
\frac{\partial c_t}{\partial f_t} &amp;= diag[c_{t-1}]\\
\frac{\partial c_t}{\partial i_t} &amp;= diag[\tilde{c}_t]\\
\frac{\partial c_t}{\partial \tilde{c}_t} &amp;= diag[i_t]\\
\frac{\partial f_t}{\partial h_{t-1}} &amp;= diag[f_t\circ (1-f_t)]\ V^{(f)}\\
\frac{\partial i_t}{\partial h_{t-1}} &amp;= diag[i_t\circ (1-i_t)]\ V^{(i)}\\
\frac{\partial \tilde{c}_t}{\partial h_{t-1}} &amp;= diag[1-\tilde{c}_t^2]\ V^{(c)}
\end{align}
\]</span> 因此， <span class="math display">\[
\begin{align}
\frac{\partial h_t}{\partial h_{t-1}} = &amp;\ diag[\tanh(c_t)] diag[e_t\circ (1-e_t)] V^{(e)}\\
&amp;\ + diag[e_t\circ (1-\tanh(c_t)^2)] diag[c_{t-1}] diag[f_t\circ (1-f_t)] V^{(f)}\\
&amp;\ + diag[e_t\circ (1-\tanh(c_t)^2)] diag[\tilde{c}_t] diag[i_t\circ (1-i_t)] V^{(i)}\\
&amp;\ + diag[e_t\circ (1-\tanh(c_t)^2)] diag[i_t] diag[1-\tilde{c}_t^2] V^{(c)}\\
= &amp;\ \tanh(c_t)\circ e_t\circ (1-e_t) V^{(e)}\\
&amp;\ + e_t\circ (1-\tanh(c_t)^2)\circ c_{t-1}\circ f_t\circ (1-f_t) V^{(f)}\\
&amp;\ + e_t\circ (1-\tanh(c_t)^2)\circ \tilde{c}_t\circ i_t\circ (1-i_t)\\
&amp;\ + e_t\circ (1-\tanh(c_t)^2)\circ i_t\circ (1-\tilde{c}_t^2)
\end{align}
\]</span> 可以求得<span class="math inline">\(t-1\)</span>时刻的残差值： <span class="math display">\[
\begin{align}
\delta_{t-1} &amp;= \delta_t\frac{\partial h_t}{\partial h_{t-1}}\\
&amp;= \delta_t\circ \tanh(c_t)\circ e_t\circ (1-e_t) V^{(e)}\\
&amp;\ + \delta_t\circ e_t\circ (1-\tanh(c_t)^2)\circ c_{t-1}\circ f_t\circ (1-f_t) V^{(f)}\\
&amp;\ + \delta_t\circ e_t\circ (1-\tanh(c_t)^2)\circ \tilde{c}_t\circ i_t\circ (1-i_t)\\
&amp;\ + \delta_t\circ e_t\circ (1-\tanh(c_t)^2)\circ i_t\circ (1-\tilde{c}_t^2)
\end{align}
\]</span> 同理，利用 <span class="math display">\[
\begin{align}
\delta_k &amp;= \delta_t\frac{\partial h_t}{\partial h_k}\\
&amp;= \delta_t\frac{\partial h_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial h_k}\\
&amp;= \delta_t\frac{\partial h_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial h_{t-2}} \cdots \frac{\partial h_{k+1}}{\partial h_k}\\
\end{align}
\]</span> 可以求得任意时刻<span class="math inline">\(k\)</span>的残差值。</p>
<p>对于残差值沿隐藏层的反向传播，其计算方法与之前提及的反向传播基本一致。</p>
</div>
</div>
<div class="section level2">
<h2>实践出真知</h2>
<pre class="python"><code>import tensorflow as tf
import pandas as pd
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(&quot;mnist/&quot;)
# 训练参数
n_epoches = 100
batch_size = 150
Learning_rate = 0.001
# 网络参数，把28x28的图片数据拆成28行的时序数据喂进RNN
n_inputs = 28
n_steps = 28
n_hiddens = 150
n_outputs = 10  # 10分类
# 输入tensors
X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.int32, [None])
# 构建RNN结构
basic_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_hiddens, state_is_tuple=True)
# basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_hiddens)
# basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_hiddens)  # 另一种创建基本单元的方式
outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)
# 前向传播，定义损失函数、优化器
logits = tf.layers.dense(states[-1], n_outputs)  # 与states tensor连接的全连接层，LSTM时为states[-1]，即h张量
cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)
loss = tf.reduce_mean(cross_entropy)
optimizer = tf.train.AdamOptimizer(learning_rate=Learning_rate)
train_op = optimizer.minimize(loss)
prediction = tf.nn.in_top_k(logits, y, 1)
accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))  # cast函数将tensor转换为指定类型
# 从MNIST中读取数据
X_test = mnist.test.images.reshape([-1, n_steps, n_inputs])
y_test = mnist.test.labels
# 训练阶段
init = tf.global_variables_initializer()
loss_list = []
accuracy_list = []
with tf.Session() as sess:
    sess.run(init)
    n_batches = mnist.train.num_examples // batch_size  # 整除返回整数部分
    # print(&quot;Batch_number: {}&quot;.format(n_batches))
    for epoch in range(n_epoches):
        for iteration in range(n_batches):
            X_batch, y_batch = mnist.train.next_batch(batch_size)
            X_batch = X_batch.reshape([-1, n_steps, n_inputs])
            sess.run(train_op, feed_dict={X: X_batch, y: y_batch})
        loss_train = loss.eval(feed_dict={X: X_batch, y: y_batch})
        loss_list.append(loss_train)
        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})
        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})
        accuracy_list.append(acc_test)
        print(epoch, &quot;Train accuracy: {:.3f}&quot;.format(acc_train), &quot;Test accuracy: {:.3f}&quot;.format(acc_test))
# 导出损失和准确率，方便绘图
loss_readout = pd.DataFrame(loss_list)
loss_readout.to_csv(&#39;csv/RNN_LSTM_loss.csv&#39;)
acc_readout = pd.DataFrame(accuracy_list)
acc_readout.to_csv(&#39;csv/RNN_LSTM_accuracy.csv&#39;)</code></pre>
<p>运行结果可以绘制出loss和accuracy两图： <img src="../../post/2019-02-01-recurrent-nerual-network-循环神经网络_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="reference" class="section level2">
<h2>Reference</h2>
<p>1.<a href="https://zybuluo.com/hanbingtao/note/541458">hanbingtao. 零基础入门深度学习(5) - 循环神经网络</a><br />
2.<a href="https://zybuluo.com/hanbingtao/note/581764">hanbingtao. 零基础入门深度学习(6) - 长短时记忆网络(LSTM)</a><br />
3.罗冬日. TensorFlow入门与实战<br />
4.<a href="https://zhuanlan.zhihu.com/p/55025354">郑思座. 循环神经网络（RNN）以及简单TensorFlow实例</a></p>
</div>

    </div>

    


<div class="article-tags">
  
  <a class="badge badge-light" href="../../tags/deep-learning/">Deep Learning</a>
  
</div>



    






<div class="media author-card" itemscope itemtype="http://schema.org/Person">
  
  <img class="portrait mr-3" src="../../img/portrait.jpg" itemprop="image" alt="Avatar">
  
  <div class="media-body">
    <h5 class="card-title" itemprop="name"><a href="../../">Thompson Hu</a></h5>
    
    
    <ul class="network-icon" aria-hidden="true">
      
      
      
      
        
      
      
      
      
      
      <li>
        <a itemprop="sameAs" href="mailto:ttxinlinhu@qq.com" >
          <i class="fas fa-envelope"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://github.com/thompsonhu" target="_blank" rel="noopener">
          <i class="fab fa-github"></i>
        </a>
      </li>
      
    </ul>
  </div>
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="../../post/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-convolutional-neural-networks/">卷积神经网络 Convolutional Neural Networks</a></li>
        
        <li><a href="../../post/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AC%AC%E4%BA%8Cpart/">生成对抗网络的第二Part</a></li>
        
        <li><a href="../../post/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AC%AC%E4%B8%80part/">生成对抗网络的第一Part</a></li>
        
        <li><a href="../../post/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/">深度神经网络基础</a></li>
        
      </ul>
    </div>
    

    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "thompsonhu" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<div class="container">
  <footer class="site-footer">
  
  <p class="powered-by">
    <a href="../../privacy/">Made by Thompson</a>
  </p>
  

  <p class="powered-by">
    &copy; 2018 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    
    <script src="../../js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//thompsonhu.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    <script src="../../js/academic.min.70f0041f5a24c6a675ac218c98d7ef71.js"></script>

    

  </body>
</html>

