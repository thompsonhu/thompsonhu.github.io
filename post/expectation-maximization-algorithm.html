<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.52" />


<title>Expectation-Maximization Algorithm - BONBON BLOG</title>
<meta property="og:title" content="Expectation-Maximization Algorithm - BONBON BLOG">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/avatar.jpg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about.html">About</a></li>
    
    <li><a href="mailto:ramakstt@gmail.com">Email</a></li>
    
    <li><a href="https://github.com/rstudio/blogdown">GitHub</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">4 min read</span>
    

    <h1 class="article-title">Expectation-Maximization Algorithm</h1>

    
    <span class="article-date">2018/12/24</span>
    

    <div class="article-content">
      


<p>From some on-line article, it is interesting that we can consider EM algorithm as some Chinese Kungfu manuals and it contains 9 levels of perspectives. With such metaphor, I can feel how this method powerful it is. Now, I want to share my view with you.</p>
<div id="introduction-to-the-em-algorithm" class="section level2">
<h2>Introduction to the EM algorithm</h2>
<p>Expectation-Maximization (EM) algorithm is an important method to solve the maximum likelihood problem with latent variables in statistics. It is widely used in Machine Learning because it can simplify many difficult problems. One of the famous and classical application is <strong>gaussian mixture model</strong>.</p>
<div id="basic-probability-theory" class="section level3">
<h3>Basic probability theory</h3>
<p>Let <span class="math inline">\(p(x|\theta)\)</span> be the probability density function of random variable <span class="math inline">\(x\)</span>, where <span class="math inline">\(\theta\)</span> is the parameter of the density function, then we know that the basic probability property is <span class="math display">\[
p(\text{x};\theta) \geq 0, \int_{-\infty}^{+\infty}p(\text{x};\theta) d\text{x} = 1.
\]</span></p>
<p>If we take the expectation of x, we get <span class="math display">\[
\mathbb{E}[\text{x}] = \int\text{x}p(\text{x};\theta) d\text{x}
\]</span> In the integral, we know that the <span class="math inline">\(\mathbb{E}[\text{x}]\)</span> involves <span class="math inline">\(\theta\)</span> but not <span class="math inline">\(\text{x}\)</span>.</p>
<p>If we generalize it to function and let <span class="math inline">\(f(\text{x})\)</span> be a function of <span class="math inline">\(\text{x}\)</span>. Similarly, the expectation of <span class="math inline">\(f(x)\)</span> is given by <span class="math display">\[
\mathbb{E}[f] = \int f(\text{x})p(\text{x};\theta) d\text{x}
\]</span></p>
<p>With the similar result, we know that the <span class="math inline">\(\mathbb{E}[f]\)</span> involves <span class="math inline">\(\theta\)</span> but not <span class="math inline">\(\text{x}\)</span>.</p>
</div>
<div id="motivation-of-the-em-algorithm" class="section level3">
<h3>Motivation of the EM algorithm</h3>
<p>At the beginning, we denote that</p>
<ul>
<li><span class="math inline">\(\text{X}\)</span>: Set of all observed data (incomplete-data)</li>
<li><span class="math inline">\(\text{Z}\)</span>: Set of all latent variables</li>
<li><span class="math inline">\(\theta\)</span>: Set of all model parameters</li>
<li><span class="math inline">\(\{ \text{X,Z} \}\)</span>: Each observation in <span class="math inline">\(\text{X}\)</span> is corresponding value of the latent variable <span class="math inline">\(\text{Z}\)</span> (complete-data)</li>
</ul>
<p>Then the log-likelihood function can be written as <span class="math display">\[
L(\text{X};\theta) = \ln p(\text{X};\theta) = \ln \{ \sum_{\text{Z}}p(\text{X}, \text{Z};\theta) \}
\]</span></p>
<p>It is too hard to straightly solve the problem with <span class="math inline">\(\ln\)</span> and <span class="math inline">\(\sum\)</span>. The likelihood function for the complete data set simply takes the form <span class="math inline">\(\ln p(\text{X,Z}|\theta)\)</span>, and we shall suppose that maximization of this complete-data log-likelihood function is straightforward.</p>
<p>In practice, we are not given the latent variable <span class="math inline">\(\text{Z}\)</span> but we know the posterior distribution <span class="math inline">\(p(\text{Z}|\text{X};\theta)\)</span>.</p>
</div>
<div id="the-level-1-of-the-em-algorithm" class="section level3">
<h3>The Level 1 of the EM algorithm</h3>
<p>In the level 1, we just need to know the basic knowledge of EM algorithm.</p>
<p>In the <span class="math inline">\(\mathbb{E}\)</span>-step, we use the current parameter values <span class="math inline">\(\theta_{\text{old}}\)</span> to find the posterior distribution of the latent variables given by <span class="math inline">\(p(\text{Z}|\text{X};\theta_{\text{old}})\)</span>.</p>
<p>Then, we would use this posterior distribution to find the expectation of the <strong>complete-data</strong> log-likelihood evaluated for some general parameter value <span class="math inline">\(\theta\)</span>. This expectation, denoted <span class="math inline">\(\mathcal{Q}(\theta,\theta_{\text{old}})\)</span>, is given by <span class="math display">\[
\mathcal{Q}(\theta,\theta_{\text{old}}) = \mathbb{E}_{\text{Z}|\text{X};\theta_{\text{old}}}[\ln p(\text{X,Z};\theta)] = \sum_{\text{Z}} \ln p(\text{X,Z};\theta)p(\text{Z}|\text{X};\theta_{\text{old}})
\]</span></p>
<p>In the <span class="math inline">\(\mathbb{M}\)</span> (Maximization) step, we determine the revised parameter estimate <span class="math inline">\(\theta_{\text{new}}\)</span> by maximizing the function <span class="math display">\[
\theta_{\text{new}} = \arg \max_\limits{\theta} \mathcal{Q}(\theta,\theta_{\text{old}})
\]</span></p>
<p>Since the <strong>complete-data</strong> log-likelihood involves unobversed data <span class="math inline">\(\text{Z}\)</span>, we use <strong>Expectation</strong> to eliminate the uncertainty, and the function <span class="math inline">\(\mathbb{E}_{\text{Z}|\text{X},\theta_{\text{old}}}[\ln p(\text{X,Z}|\theta)]\)</span> does not involve <span class="math inline">\(\text{Z}\)</span> but involve <span class="math inline">\((\theta,\theta_{\text{old}})\)</span>.</p>
<p>We can summarize the procedure as:</p>
<blockquote>
<p>While <span class="math inline">\(\theta_{\text{new}} - \theta_{old} &gt; \epsilon\)</span><br />
<span class="math inline">\(\quad\)</span> Expectation-Step on log likelihood function: <span class="math display">\[
\mathcal{Q}(\theta,\theta_{\text{old}}) = \mathbb{E}_{\text{Z}|\text{X};\theta_{\text{old}}}[\ln p(\text{X,Z};\theta)]= \sum_{\text{Z}} \ln p(\text{X,Z};\theta)p(\text{Z}|\text{X};\theta_{\text{old}})
\]</span> <span class="math inline">\(\quad\)</span> Maximization-Step on <span class="math inline">\(\mathcal{Q}(\theta,\theta_{\text{old}})\)</span>: <span class="math display">\[
\theta_{\text{new}} = \arg \max_\limits{\theta} \mathcal{Q}(\theta,\theta_{\text{old}})
\]</span></p>
</blockquote>
</div>
<div id="the-level-2-of-the-em-algorithm" class="section level3">
<h3>The Level 2 of the EM algorithm</h3>
<p>After writting down the pseudocode of EM algorithm, we want to know the reason that we can use expectation to approximate the maximum log-likelihood by repeating <strong>Expectation</strong> and <strong>Maximization</strong>. In other words, we want to prove that <span class="math display">\[
\arg \max_\theta \mathbb{E}_{\text{Z}|\text{X};\theta_{\text{old}}}[\ln p(\text{X};\theta)] \approx \arg \max_\theta \ln p(\text{X};\theta)
\]</span> where the joint distribution <span class="math inline">\(p(\text{X}, \text{Z};\theta)\)</span> is governed by a set of parameters <span class="math inline">\(\theta\)</span>.</p>
<p>Next we introduce a distribution <span class="math inline">\(q(\text{Z})\)</span> defined over the latent variables.</p>
<p>Since <span class="math display">\[
p(\text{X},\text{Z};\theta) = p(\text{Z}|\text{X};\theta)p(\text{X};\theta)
\]</span> and <span class="math display">\[
\sum_\text{Z}q(\text{Z}) = 1
\]</span> we can get decomposition by <span class="math display">\[
\begin{align}
\ln p(\text{X};\theta) =&amp;\ \ln \frac{p(\text{X},\text{Z};\theta)}{p(\text{Z}|\text{X};\theta)}\\
=&amp;\ \ln p(\text{X},\text{Z};\theta) - \ln p(\text{Z}|\text{X};\theta) + \ln q(\text{Z}) - \ln q(\text{Z})\\
=&amp;\ \ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})} - \ln \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}\\
=&amp; \sum_\text{Z}q(\text{Z}) \{ \ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})} - \ln \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})} \}\\
=&amp;\ \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})} - \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}\\
=&amp;\ \mathcal{L}(q,\theta) + KL(q||p)
\end{align}
\]</span> where <span class="math display">\[
\begin{align}
\mathcal{L}(q,\theta) = \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})}\\
KL(q||p) = - \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}
\end{align}
\]</span></p>
<p>We call the <span class="math inline">\(KL(q||p)\)</span> as the Kullback-Leibler divergence (KL divergence, also known as relative entropy).</p>
<p>Recall that <strong>Jensen’s inequality</strong> holds for convex function <span class="math inline">\(f(x)\)</span>. <span class="math display">\[
\mathbb{E}[f(x)] \geq f(\mathbb{E}[x])
\]</span></p>
<p>Applying Jensen’s inequality in KL divergence, we have <span class="math display">\[
\begin{align}
KL(q||p) =&amp; - \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})} = -\mathbb{E}_q[\ln\{\frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}\} ]\\
\geq&amp; -\ln \mathbb{\text{E}_q}[\frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}] = -\ln \sum_\text{Z} q(\text{Z}) \frac{p(\text{Z}|\text{X};\theta)}{q(\text{Z})}\\
=&amp; -\ln \sum_\text{Z} p(\text{Z}|\text{X};\theta) = -\ln 1\\ =&amp;\ 0
\end{align}
\]</span></p>
<p>If we let <span class="math inline">\(q(\text{Z}) = p(\text{Z}|\text{X};\theta)\)</span>, and <span class="math inline">\(p(\text{X},\text{Z};\theta) = p(\text{Z}|\text{X};\theta)p(\text{X};\theta)\)</span>, then <span class="math display">\[
\begin{align}
\mathcal{L}(q,\theta) =&amp; \sum_\text{Z}q(\text{Z}) \ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})} = \mathbb{E}_q[\ln \frac{p(\text{X},\text{Z};\theta)}{q(\text{Z})}]\\
=&amp;\ \mathbb{E}_{\text{Z}|\text{X};\theta}[\ln p(X;\theta)]
\end{align}
\]</span></p>
<p>Thus, we can get the result that <span class="math inline">\(\ln p(\text{X};\theta) \geq \mathcal{L}(q,\theta) = \mathbb{E}_{\text{Z}|\text{X};\theta}[\ln p(X;\theta)]\)</span>, so we can say that <span class="math inline">\(\mathcal{L}(q,\theta) = \mathbb{E}_{\text{Z}|\text{X};\theta}[\ln p(X;\theta)]\)</span> is the low bound of <span class="math inline">\(\ln p(\text{X};\theta)\)</span>.</p>
<div align="center">
<img src="EM1.PNG" width = "300" height = "300" alt="Figure1" />
<p>
Figure1. The EM algorithm involves alternatel computing a lower bound on the log likelihood for the current parameter values and then maximizing this bound to obtain the new parameter values.
</div>
<p>Actually, EM algorithm is one of the special case of Minorize-Maximization (MM) algorithm, and <span class="math inline">\(\mathcal{L}(q,\theta)\)</span> can be considered as the <strong>surrogate function</strong> in MM algorithm.</p>
</div>
</div>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//thompsonhu.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

