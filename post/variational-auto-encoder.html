<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.52" />


<title>Variational Auto-Encoder - BONBON BLOG</title>
<meta property="og:title" content="Variational Auto-Encoder - BONBON BLOG">



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/avatar.jpg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about.html">About</a></li>
    
    <li><a href="mailto:ramakstt@gmail.com">Email</a></li>
    
    <li><a href="https://github.com/thompsonhu/thompsonhu.github.io">GitHub</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">11 min read</span>
    

    <h1 class="article-title">Variational Auto-Encoder</h1>

    
    <span class="article-date">2018/12/31</span>
    

    <div class="article-content">
      


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>We assume the observed variable <span class="math inline">\(x\)</span> is a random sample from an <strong>unknown underlying process</strong>, whose <strong>true distribution</strong> <span class="math inline">\(p^*(x)\)</span> is <strong>unknown</strong>. We attempt to approximate this underlying process with a chosen model <span class="math inline">\(p_{\theta}(x)\)</span>, with parameters <span class="math inline">\(\theta\)</span>: <span class="math display">\[x\sim p_{\theta}(x)\]</span> We always talk about learning like <strong>Deep Learning</strong>, and actually the <strong>learning</strong> is the process of searching for a value of the parameters <span class="math inline">\(\theta\)</span> in model <span class="math inline">\(p_{\theta}(x)\)</span>, which can approximate the true distribution of the data, denoted by <span class="math inline">\(p^*(x)\)</span>. In other words, <span class="math display">\[p_{\theta}(x)\approx p^*(x)\]</span> Latent variables are variables that are part of the model, but which we don’t observe, and are therefore not part of the dataset. We typically use <span class="math inline">\(z\)</span> to denote such latent variables.</p>
<p>The marginal distribution over the observed variables <span class="math inline">\(p_{\theta}(x)\)</span>, is given by: <span class="math display">\[
p_{\theta}(x) = \int p_{\theta}(x,z) dz = \int p_{\theta}(z) p_{\theta}(x|z) dz
\]</span> We use the term <strong>deep latent variable model (DLVM)</strong> to denote a <strong>latent variable model</strong> <span class="math inline">\(p_{\theta}(x,z)\)</span> whose distributions are parameterized by neural networks.</p>
<div id="example-dlvm-for-multivariate-bernoulli-data" class="section level2">
<h2>Example DLVM for multivariate Bernoulli data</h2>
<p>A simple example DLVM for binary data <span class="math inline">\(x\)</span>, with a spherical Gaussian latent space, and a factorized Bernoulli obervation model <span class="math display">\[
p(z) = \mathcal{N}(0,\text{I})\\
\text{p} = \text{DecoderNeuralNet}_{\theta}(z)\\
\begin{align}
\log p(x|z) =&amp; \sum_{j=1}^J \log p(x_j|z) = \sum_{j=1}^J \text{Bernoulli}(x_j,p_j)\\
=&amp; \sum_{j=1}^Jx_j \log p_j + (1-x_j)\log (1-p_j)
\end{align}
\]</span> where <span class="math inline">\(0\leq p_j\leq 1\)</span>.</p>
<p>Therefore, we easily get <span class="math inline">\(p(x,z) = p(x|z)\times p(z)\)</span> by the term we described above.</p>
</div>
<div id="some-problem" class="section level2">
<h2>Some problem</h2>
<p>Note that <span class="math inline">\(p_{\theta}(x,z)\)</span> is efficient to compute. Since the intractability of <span class="math inline">\(p_{\theta}(x)\)</span> (<span class="math inline">\(p_{\theta}(x) = \int p_{\theta}(x,z) dz\)</span>), the posterior distribution <span class="math inline">\(p_{\theta}(z|x)\)</span> is also intractable, because their densities are related through the basic identity: <span class="math display">\[p_{\theta}(z|x) = \frac{p_{\theta}(x,z)}{p_{\theta}(x)}\]</span></p>
<p>How can we perform efficient approximate posterior inference and efficient approximate maximum likelihood estimation in deep latent variable models, in the presence of large datasets?</p>
</div>
<div id="similar-method-like-dlvm" class="section level2">
<h2>Similar method like DLVM</h2>
<p>We introduce a parametric inference model <span class="math inline">\(q_{\phi}(z|x)\)</span> (also called as <strong>encoder</strong>)in this part and we try to optimize the variational parameters <span class="math inline">\(\phi\)</span> such that: <span class="math display">\[q_{\phi}(z|x) \approx p_{\theta}(z|x)\]</span></p>
<p>Similar to <strong>DLVM</strong>, the distribution of <span class="math inline">\(q_{\phi}(z|x)\)</span> also can be parameterized using deep neural networks. In this case, the variational parameters <span class="math inline">\(\phi\)</span> include the weights and biases of the neural network. For example: <span class="math display">\[
(\mu,\sigma) = \text{EncoderNeuralNet}_{\phi}(x)\\
q_{\phi}(z|x) = \mathcal{N}(\mu,\text{diag}(\sigma^2))
\]</span></p>
</div>
</div>
<div id="evidence-lower-bound-elbo-and-kl-divergence" class="section level1">
<h1>Evidence lower bound (ELBO) and KL divergence</h1>
<p>The optimization objective of the variational autoencoder is the <strong>evidence lower bound</strong>, abbreviated as ELBO. An alternative term for this objective is <strong>variational lower bound</strong>. We can obtain the lower bound by: <span class="math display">\[
\begin{align}
\log p_{\theta}(x) =&amp;\ \mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x)] = \mathbb{E}_{q_{\phi}(z|x)} \Big[\log\Big[ \frac{p_{\theta}(x,z)}{p_{\theta}(z|x)}\Big]\Big]\\
=&amp;\ \mathbb{E}_{q_{\phi}(z|x)}\Big[\log\Big[\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}\frac{q_{\phi}(z|x)}{p_{\theta}(z|x)}\Big]\Big]\\
=&amp;\ \mathbb{E}_{q_{\phi}(z|x)}\Big[\log\Big[\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}\Big]\Big] + \mathbb{E}_{q_{\phi}(z|x)}\Big[\log\Big[\frac{q_{\phi}(z|x)}{p_{\theta}(z|x)}\Big]\Big]\\
=&amp;\ \mathcal{L}_{\theta,\phi}(x) + KL[q_{\phi}(z|x)||p_{\theta}(z|x)]\\
\geq&amp;\ \mathcal{L}_{\theta,\phi}(x)
\end{align}
\]</span></p>
<div id="kl-divergence" class="section level2">
<h2>KL divergence</h2>
<p>We want to find a good probability distribution <span class="math inline">\(q_{\phi}(z|x)\)</span> (‘good’ means the efficient computation) to approximate the true posterior probability <span class="math inline">\(p_{\theta}(z|x)\)</span>, where the <span class="math inline">\(z\)</span> is the latent variable. <strong>KL divergence</strong> can measure the distance well between these two distribution. For the discrete probability situation, the <strong>KL divergence</strong> can be written as <span class="math display">\[KL(q||p) = \sum q(x)\log \frac{q(x)}{p(x)}\]</span></p>
<div id="example-of-1-dimension-guassian-distribution" class="section level3">
<h3>Example of 1-dimension Guassian distribution</h3>
<p>Supposed that we have two random variables <span class="math inline">\(x_1, x_2\)</span> w.r.t the guassian distribution <span class="math inline">\(\mathcal{N}(\mu_1,\sigma_1^2),\mathcal{N}(\mu_2,\sigma_2^2)\)</span> respectively.</p>
<p>Recall that the density function of guassian distribution <span class="math display">\[
\mathcal{N}(\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\]</span> Then <span class="math display">\[
\begin{align}
KL(p_1,p_2) =&amp;\ \int p_1(x)\log \frac{p_1(x)}{p_2(x)}dx\\
=&amp;\ \int p_1(x)(\log p_1(x) - \log p_2(x))dx\\
=&amp;\ \int p_1(x)(\log \frac{1}{\sqrt{2\pi\sigma_1^2}}e^{-\frac{(x-\mu_1)^2}{2\sigma_1^2}} - \log \frac{1}{\sqrt{2\pi\sigma_2^2}}e^{-\frac{(x-\mu_2)^2}{2\sigma_2^2}})dx\\
=&amp;\ \int p_1(x)(-\log \sqrt{2\pi \sigma_1^2} - \frac{(x-\mu_1)^2}{2\sigma_1^2} + \log \sqrt{2\pi \sigma_2^2} + \frac{(x-\mu_2)^2}{2\sigma_2^2})dx\\
=&amp;\ \int p_1(x)(-\frac{1}{2}\log2\pi-\log\sigma_1+\frac{1}{2}\log2\pi+\log\sigma_2 - (\frac{(x-\mu_1)^2}{2\sigma_1^2}-\frac{(x-\mu_2)^2}{2\sigma_2^2}))dx\\
=&amp;\ \int p_1(x)(\log\frac{\sigma_2}{\sigma_1} - (\frac{(x-\mu_1)^2}{2\sigma_1^2}-\frac{(x-\mu_2)^2}{2\sigma_2^2}))dx\\
=&amp;\ \int p_1(x)(\log\frac{\sigma_2}{\sigma_1})dx + \int p_1(x)(\frac{(x-\mu_2)^2}{2\sigma_2^2})dx - \int p_1(x)(\frac{(x-\mu_1)^2}{2\sigma_1^2})dx\\
=&amp;\ \log\frac{\sigma_2}{\sigma_1} + \frac{1}{2\sigma_2^2}\int p_1(x)(x-\mu_2)^2dx - \frac{1}{2\sigma_1^2}\int p_1(x)(x-\mu_1)^2dx
\end{align}
\]</span> Since <span class="math inline">\(\sigma^2 = \int p_1(x)(x-\mu_1)^2dx\)</span>, then <span class="math display">\[
\begin{align}
KL(p_1,p_2) =&amp;\ \log\frac{\sigma_2}{\sigma_1} + \frac{1}{2\sigma_2^2}\int p_1(x)(x-\mu_2)^2dx - \frac{1}{2}\\
=&amp;\ \log\frac{\sigma_2}{\sigma_1} + \frac{1}{2\sigma_2^2}\int p_1(x)(x - \mu_1 + \mu_1 - \mu_2)^2dx - \frac{1}{2}\\
=&amp;\ \log\frac{\sigma_2}{\sigma_1} + \frac{1}{2\sigma_2^2}[\int p_1(x)(x-\mu_1)^2dx+\int p_1(x)(\mu_1-\mu_2)^2dx+2\int p_1(x)(x-\mu_1)(\mu_1-\mu_2)dx] - \frac{1}{2}\\
\end{align}
\]</span> We know that <span class="math inline">\(\mu_1 = \int x p_1(x)dx\)</span>, so <span class="math inline">\(2\int p_1(x)(x-\mu_1)(\mu_1-\mu_2)dx = 2(\mu_1-\mu_2)[\int xp_1(x)dx - \mu_1] = 0\)</span>, thus <span class="math display">\[
\begin{align}
KL(p_1,p_2) =&amp;\ \log\frac{\sigma_2}{\sigma_1} + \frac{1}{2\sigma_2^2}[\int p_1(x)(x-\mu_1)^2dx + (\mu_1-\mu_2)^2] - \frac{1}{2}\\
=&amp;\ \log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2+(\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac{1}{2}\\
\end{align}
\]</span> If we suppose that the <span class="math inline">\(\mathcal{N}(\mu_2,\sigma_2^2)\)</span> is standard guassian distribution, <span class="math inline">\(\mu_2 = 0, \sigma_2^2 = 1\)</span>, so <span class="math display">\[
\begin{align}
KL =&amp;\ \log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2+(\mu_1 - \mu_2)^2}{2\sigma_2^2} - \frac{1}{2}\\
=&amp;\ \log1 - \log\sigma_1 + \frac{\sigma_1^2+(\mu_1 - 0)^2}{2} - \frac{1}{2}\\
=&amp;\ -\log\sigma_1 + \frac{\sigma_1^2+\mu_1^2}{2} - \frac{1}{2}\\
\end{align}
\]</span> We expect that the <strong>KL</strong> can be as small as possible, we calculate its derivative, then we get <span class="math display">\[
\frac{\partial KL}{\partial \sigma_1} = -\frac{1}{\sigma_1} + \sigma_1\\
\frac{\partial KL}{\partial \mu_1} = \mu_1
\]</span> We let them equal to zero, then we get <span class="math display">\[
-\frac{1}{\sigma_1} + \sigma_1 = 0 \Rightarrow \sigma_1 = 1\\
\mu_1 = 0
\]</span> which means that the <strong>KL</strong> becomes the minimum when <span class="math inline">\(x_2 \sim \mathcal{N}(0,1)\)</span></p>
</div>
<div id="minimization-of-kl-divergence" class="section level3">
<h3>Minimization of KL divergence</h3>
<p>If we want to use the ELBO to approximate the log-likelihood, then we need to minimize the <span class="math inline">\(D_{KL}[q_{\phi}(z|x)||p_{\theta}(z|x)]\)</span>.</p>
<p>From <span class="math display">\[
\begin{align}
KL[q_{\phi}(z|x)||p_{\theta}(z|x)] =&amp;\ \int q_{\phi}(z|x) \log \frac{q_{\phi}(z|x)}{p_{\theta}(z|x)} dz\\
=&amp;\ \int q_{\phi}(z|x) [\log q_{\phi}(z|x) - \log  p_{\theta}(z|x)]dz
\end{align}
\]</span> and Bayesian formula <span class="math display">\[
p_{\theta}(z|x) = \frac{p_{\theta}(x|z)*p_{\theta}(z)}{p_{\theta}(x)}
\]</span> We can get <span class="math display">\[
\begin{align}
KL[q_{\phi}(z|x)||p_{\theta}(z|x)] =&amp;\ \int q_{\phi}(z|x) [\log q_{\phi}(z|x) - \log  \frac{p_{\theta}(x|z)*p_{\theta}(z)}{p_{\theta}(x)}]dz\\
=&amp;\ \int q_{\phi}(z|x) [\log q_{\phi}(z|x) -\log  p_{\theta}(x|z) - \log p_{\theta}(z) + \log p_{\theta}(x)]dz\\
=&amp;\  \int q_{\phi}(z|x) [\log q_{\phi}(z|x) -\log  p_{\theta}(x|z) - \log p_{\theta}(z)]dz + \log p_{\theta}(x)\\
=&amp;\ KL[q_{\phi}(z|x)||p_{\theta}(z)] - \int q_{\phi}(z|x) \log p_{\theta}(x|z)dz + \log p_{\theta}(x)
\end{align}
\]</span></p>
<p>When the data <span class="math inline">\(x\)</span> are provided, then last term in the right side <span class="math inline">\(\log p_{\theta}(x)\)</span> becomes constant, and we wish the <span class="math inline">\(D_{KL}[q_{\phi}(z|x)||p_{\theta}(z|x)]\)</span> can be as small as possible.</p>
<p>Thus, the optimization problem becomes</p>
<ul>
<li><p><span class="math inline">\(\min\limits_x D_{KL}[q_{\phi}(z|x)||p_{\theta}(z)]\)</span></p></li>
<li><p><span class="math inline">\(\max\limits_x \int q_{\phi}(z|x) \log p_{\theta}(x|z)dz\)</span></p></li>
</ul>
<p>It also can be written as <span class="math display">\[\min_x KL[q_{\phi}(z|x)||p_{\theta}(z)] - \mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)]\]</span></p>
<p>Although we can obtain our new optimization problem, the problem actually is difficult to solve, and thus we would like to straightly optimize the <strong>ELBO</strong>.</p>
</div>
</div>
</div>
<div id="variational-auto-encoder" class="section level1">
<h1>Variational Auto-Encoder</h1>
<div id="connection-with-em" class="section level2">
<h2>Connection with EM</h2>
<p>For standard EM algorithms, the posterior is often known, <span class="math inline">\(q_{\phi}(z|x) = q(z|x) = p_{\theta}(z|x)\)</span>, then the <strong>KL</strong> term becomes zero, so <span class="math display">\[
\begin{align}
\log p_{\theta}(x) = \mathcal{L}_{\theta}(x) =&amp;\ \mathbb{E}_{q(z|x)}\Big[\log\Big[\frac{p_{\theta}(x,z)}{q(z|x)}\Big]\Big]\\
=&amp;\ \mathbb{E}_{q(z|x)}[\log p_{\theta}(x,z)] - \mathbb{E}_{q(z|x)}[\log q(z|x)]
\end{align}
\]</span></p>
<p>The above step is indeed the E-step in the standard EM algorithm. The M-step would be <span class="math display">\[\theta_{\text{new}} = \arg \max_\theta L_{\theta}(x)\]</span></p>
</div>
<div id="stochastic-gradient-based-optimization-of-the-elbo" class="section level2">
<h2>Stochastic gradient-based optimization of the ELBO</h2>
<p>From the <strong>Evidence lower bound (ELBO)</strong> part, we obtain the inequality fomula as <span class="math inline">\(\log p_{\theta}(x) \geq \mathcal{L}_{\theta,\phi}(x)\)</span>. Recall that EM algorithm is one of the special case of Minorize-Maximization (MM) algorithm, and <span class="math inline">\(\mathcal{L}_{\theta,\phi}(x)\)</span> can be considered as the surrogate function in MM algorithm, so we would get the maximum of log-likelihood by maximizing the lower bound.</p>
<div align="center">
<img src="VAE1.PNG" width = "300" height = "300" alt="Figure1" />
<p>
Figure1. The EM algorithm involves alternatel computing a lower bound on the log likelihood for the current parameter values and then maximizing this bound to obtain the new parameter values.
</div>
<p>Given a dataset with i.i.d. data, the ELBO objective is the sum (or average) of individual-datapoint ELBO’s: <span class="math display">\[
\mathcal{L}_{\theta,\phi}(\mathcal{D})=\sum_{x\in\mathcal{D}}\mathcal{L}_{\theta,\phi}(x)
\]</span></p>
<p>Apparantly, the individual-datapoint ELBO and its gradient <span class="math inline">\(\nabla_{\theta,\phi}\mathcal{L}_{\theta,\phi}(x)\)</span> is intractable in general.</p>
<div id="the-sgvb-estimator-and-auto-encoding-vb-aevb-algorithm" class="section level3">
<h3>The SGVB estimator and Auto-Encoding VB (AEVB) algorithm</h3>
<p><strong>Reparamterization trick</strong></p>
<p>Let <span class="math inline">\(z\)</span> be a continuous random variable and <span class="math inline">\(z\sim q_{\phi}(z|x)\)</span> be some conditional distribution. It is often possible to express the random variable <span class="math inline">\(z\)</span> as a deterministic variable <span class="math inline">\(z=g_{\phi}(\epsilon,x)\)</span>, where <span class="math inline">\(\epsilon\)</span> is an auxiliary variable with independent marginal <span class="math inline">\(p(\epsilon)\)</span>.</p>
<p>We suppose that the recognition model <span class="math inline">\(q_{\phi}(z|x)\)</span> can be written as some differentiable transformation of another randome variable <span class="math inline">\(\epsilon\)</span>, <span class="math inline">\(g_{\phi}(\epsilon,x)\)</span>, and we can form a simple Monte Carlo estimator <span class="math inline">\(\tilde{\mathcal{L}}_{\theta,\phi}(x)\)</span> of the individual-datapoint ELBO: <span class="math display">\[
\epsilon \sim p(\epsilon)
\]</span></p>
<p>so we can get our generic Stochastic Gradient Variational Bayes (SGVB) estimator from the lower bound <span class="math display">\[
\tilde{\mathcal{L}}_{\theta,\phi}^{A}(x^{(i)}) = \frac{1}{L}\sum_{l=1}^L[\log p_{\theta}(x^{(i)},z^{(i,l)}) - \log q_{\phi}(z^{(i,l)}|x^{(i)})]
\]</span> where <span class="math inline">\(z^{(i,l)} = g_{\phi}(\epsilon^{(i,l)},x^{(i)}),\quad \epsilon^{(i,l)} \sim p(\epsilon)\)</span>.</p>
<p>We try to decompose the <span class="math inline">\(\mathcal{L}_{\theta,\phi}(x)\)</span>, and we get <span class="math display">\[
\begin{align}
\mathcal{L}_{\theta,\phi}(x^{(i)}) =&amp;\ \mathbb{E}_{q_{\phi}(z|x^{(i)})}\Big[\log\frac{p_{\theta}(x^{(i)},z)}{q_{\phi}(z|x^{(i)})}\Big]\\
=&amp;\ \mathbb{E}_{q_{\phi}(z|x^{(i)})}\Big[\log\frac{p_{\theta}(x^{(i)}|z)p_{\theta}(z)}{q_{\phi}(z|x^{(i)})}\Big]\\
=&amp;\ \mathbb{E}_{q_{\phi}(z|x^{(i)})}\Big[\log p_{\theta}(x^{(i)}|z)-\log\frac{q_{\phi}(z|x^{(i)})}{p_{\theta}(z)}\Big]\\
=&amp;\ \mathbb{E}_{q_{\phi}(z|x^{(i)})}[\log p_{\theta}(x^{(i)}|z)]-KL[q_{\phi}(z|x^{(i)})||p_{\theta}(z)]
\end{align}
\]</span> The final equality showed the same object result (In Minimization of KL divergence section).</p>
<p>With this equality, we also can obtain another estimator <span class="math display">\[
\tilde{\mathcal{L}}_{\theta,\phi}^{B}(x^{(i)}) = \mathbb{E}_{q_{\phi}(z|x^{(i)})}[\log p_{\theta}(x^{(i)}|z)]-KL[q_{\phi}(z|x^{(i)})||p_{\theta}(z)]\\
=\frac{1}{L}\sum_{l=1}^L\log p_{\theta}(x^{(i)}|z^{(i,l)})-KL[q_{\phi}(z|x^{(i)})||p_{\theta}(z)]
\]</span> where <span class="math inline">\(z^{(i,l)} = g_{\phi}(\epsilon^{(i,l)},x^{(i)}),\quad \epsilon^{(i,l)} \sim p(\epsilon)\)</span>. Given multiple datapoints from a dataset <span class="math inline">\(\text{X}\)</span> with <span class="math inline">\(N\)</span> datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches: <span class="math display">\[
\mathcal{L}_{\theta,\phi}(\text{X})\simeq\tilde{\mathcal{L}}_{\theta,\phi}^{M}(\text{X}^M)=\frac{N}{M}\sum_{i=1}^M\tilde{\mathcal{L}}_{\theta,\phi}(x^{(i)})
\]</span> where the minibatch <span class="math inline">\(\text{X}^M=\{x^{(i)}\}_{i=1}^M\)</span> is randomly drawn sample of <span class="math inline">\(M\)</span> datapoints from the full dataset <span class="math inline">\(\text{X}\)</span> with <span class="math inline">\(N\)</span> datapoints. In the paper <a href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a>, author set <span class="math inline">\(M = 100, L = 1\)</span> in their experiments.</p>
<div align="center">
<p><img src="VAE2.PNG" width = "700" height = "250" alt="Figure2" /></p>
</div>
</div>
</div>
<div id="variational-auto-encoder-with-specific-case" class="section level2">
<h2>Variational Auto-Encoder with specific case</h2>
<p>We know that we can not perform the algorithm that we describe above, because we don’t know the distributions of <span class="math inline">\(\epsilon, p_{\theta}(x|z), q_{\phi}(z|x), p_{\theta}(z)\)</span> and <span class="math inline">\(g_{\phi}(\epsilon,x)\)</span>. In reality, like author described in the paper, we firstly let the prior over the latent variables be the centered isotropic multivariate Guassian <span class="math inline">\(p_{\theta}(z) = \mathcal{N}(0,\text{I})\)</span>.</p>
<div id="variational-approxiamte-posterior-q_phizxi" class="section level3">
<h3>Variational approxiamte posterior <span class="math inline">\(q_{\phi}(z|x^{(i)})\)</span></h3>
<p>Let the variational approxiamte posterior be a multivariate Guassian with a diagonal covariance structure: <span class="math display">\[
q_{\phi}(z|x^{(i)}) = \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})
\]</span> where <span class="math inline">\(\mu^{(i)},\sigma^{(i)}\)</span> denote the variational mean and s.d. evaluated by datapoint <span class="math inline">\(i\)</span>.</p>
<p>Then a valid reparameterization is <span class="math inline">\(z=\mu+\sigma\epsilon\)</span>, where <span class="math inline">\(\epsilon\)</span> is an auxiliary noise variable <span class="math inline">\(\epsilon\sim \mathcal{N}(0,\text{I})\)</span>.</p>
<p>Let <span class="math inline">\(J\)</span> be the dimensionality of <span class="math inline">\(z\)</span> and <span class="math inline">\(\mu^{(i)}_j, \sigma^{(i)}_j\)</span> denote the <span class="math inline">\(j\)</span>-th element. Recall that <span class="math display">\[
\mathbb{E}[z] = \int z p(z) dz\\
\mathbb{E}[z^2] = \int z^2 p(z) dz\\
\text{Var}[z] = \mathbb{E}[z^2] - \mathbb{E}^2[z]
\]</span> Then, <span class="math display">\[
\begin{align}
\int q_{\phi}(z|x^{(i)})\log p_{\theta}(z)dz =&amp;\ \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})\log \mathcal{N}(0,\text{I})dz\\
=&amp;\ \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})(\log \frac{1}{\sqrt{2\pi}})dz - \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I}) \frac{z^2}{2}dz\\
=&amp;\ -\frac{J}{2}\log(2\pi)\int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})dz - \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I}) \frac{z^2}{2}dz\\
=&amp;\ -\frac{J}{2}\log(2\pi) - \frac{1}{2}\int z^2 \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})dz\\
=&amp;\ -\frac{J}{2}\log(2\pi) - \frac{1}{2}\sum_{j=1}^J\mathbb{E}_{ q_{\phi}(z_j|x^{(i)})}[z_j^2]\\
=&amp;\ -\frac{J}{2}\log(2\pi) - \frac{1}{2}\sum_{j=1}^J\Big[\mathbb{E}_{ q_{\phi}(z_j|x^{(i)})}^2[z_j]+\text{Var}_{ q_{\phi}(z_j|x^{(i)})}[z_j]\Big]\\
=&amp;\ -\frac{J}{2}\log(2\pi) - \frac{1}{2}\sum_{j=1}^J(\mu_j^2+\sigma_j^2)
\end{align}
\]</span> and <span class="math display">\[
\begin{align}
\int q_{\phi}(z|x^{(i)})\log q_{\phi}(z|x^{(i)})dz =&amp;\ \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})\log \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})dz\\
=&amp;\ \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})\log \Big[\frac{1}{\sqrt{2\pi\sigma^{(i)^2}}}\exp(\frac{-(z-\mu^{(i)})^2}{2\sigma^{(i)^2}})\Big]dz\\
=&amp;\ \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})\Big[-\frac{1}{2}\log 2\pi - \frac{1}{2}\log \sigma^{(i)^2} - \frac{(z-\mu^{(i)})^2}{2\sigma^{(i)^2}}\Big]dz\\
=&amp;\ -\frac{J}{2}\log(2\pi) - \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})\Big[\frac{\log \sigma^{(i)^2}}{2} - \frac{(z-\mu^{(i)})^2}{2\sigma^{(i)^2}}\Big]dz\\
=&amp;\ -\frac{J}{2}\log(2\pi) - \int \mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I}) \Big[\frac{1}{2}\log \sigma^{(i)^2} - \frac{z^2-2\mu^{(i)}z+\mu^{(i)^2}}{2\sigma^{(i)^2}}\Big]dz\\
=&amp;\ -\frac{J}{2}\log(2\pi) - \frac{1}{2}\sum_{J=1}^J\log \sigma_j^{(i)^2} + \int \frac{1}{2\sigma^{(i)^2}}\mathcal{N}(\mu^{(i)},\sigma^{(i)^2}\text{I})(z^2-2\mu^{(i)}z+\mu^{(i)^2})dz\\
=&amp;\ -\frac{J}{2}\log(2\pi) - \frac{1}{2}\sum_{j=1}^J\log \sigma_j^{(i)^2} + \frac{1}{2}\sum_{j=1}^J\frac{\mu^{(i)^2}+\sigma^{(i)^2}-2\mu^{(i)^2}+\mu^{(i)^2}}{\sigma^{(i)^2}}\\
=&amp;\ -\frac{J}{2}\log(2\pi) - \frac{1}{2}\sum_{j=1}^J(1+\log \sigma_j^{(i)^2})
\end{align}
\]</span> Therefore, <span class="math display">\[
\begin{align}
-KL[q_{\phi}(z|x^{(i)})||p_{\theta}(z)] =&amp; - \int q_{\phi}(z|x^{(i)})\log \frac{q_{\phi}(z|x^{(i)})}{p_{\theta}(z)}dz\\
=&amp;\ - \int q_{\phi}(z|x^{(i)})[\log q_{\phi}(z|x^{(i)}) - \log p_{\theta}(z)]dz\\
=&amp;\ \int q_{\phi}(z|x^{(i)})[\log p_{\theta}(z) - \log q_{\phi}(z|x^{(i)})]dz\\
=&amp;\ -\frac{J}{2}\log(2\pi) - \frac{1}{2}\sum_{j=1}^J(\mu_j^2+\sigma_j^2) - \Big[-\frac{J}{2}\log(2\pi) - \frac{1}{2}\sum_{j=1}^J(1+\log \sigma_j^{(i)^2})\Big]\\
=&amp;\ \frac{1}{2}\sum_{j=1}^J(1+\log \sigma_j^{(i)^2}-\mu_j^2-\sigma_j^2)
\end{align}
\]</span></p>
</div>
<div id="true-posterior-p_thetaxz" class="section level3">
<h3>True posterior <span class="math inline">\(p_{\theta}(x|z)\)</span></h3>
<p>We supposed that the true posterior <span class="math inline">\(p_{\theta}(x|z)\)</span> be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution parameters are computed from <span class="math inline">\(z\)</span> with a MLP (Multi-Layer Perceptron).</p>
<p><strong>Bernoulli MLP as decoder</strong><br />
If the data are binary data, then we would choose <span class="math display">\[
\log p_{\theta}(x|z) = \sum_{j=1}^Dx_j \log y_j + (1-x_j)\log (1-y_j)
\]</span> where <span class="math inline">\(y = f_\sigma(W_2\tanh(W_1z+b_1)+b_2)\)</span>, <span class="math inline">\(f_\sigma(\cdot)\)</span> is the elementwise sigmoid activation function and <span class="math inline">\(\theta=\{W_1,W_2,b_1,b_2\}\)</span> are the weights and biases of the MLP.</p>
<p><strong>Gaussian MLP as decoder</strong><br />
Let decoder be a mutivariate Guassian with a diagonal covariance structure: <span class="math display">\[
\log p_{\theta}(x|z) = \log \mathcal{N}(\mu,\sigma^2\text{I})
\]</span> where <span class="math inline">\(\mu = W_4h+b_4,\ \log\sigma^2 = W_5h+b_5,\ h = \tanh(W_3Z+b_3)\)</span> and <span class="math inline">\(\{W_3,W_4,W_5,b_3,b_4,b_5\}\)</span> are the weights and biases of the MLP and part of <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Analysis in case of binary data</strong><br />
Recall the second estimator we describe above <span class="math display">\[
\begin{align}
\mathcal{L}_{\theta,\phi}(\text{X})\simeq&amp;\ \tilde{\mathcal{L}}_{\theta,\phi}^{B}(x^{(i)})\\
=&amp;\ \frac{1}{L}\sum_{l=1}^L\log p_{\theta}(x^{(i)}|z^{(i,l)}) - KL[q_{\phi}(z|x^{(i)})||p_{\theta}(z)]\\
=&amp;\ \frac{1}{L}\sum_{l=1}^L\log p_{\theta}(x^{(i)}|z^{(i,l)}) + \frac{1}{2}\sum_{j=1}^J(1+\log \sigma_j^{(i)^2}-\mu_j^2-\sigma_j^2)
\end{align}
\]</span> where <span class="math inline">\(z^{(i,l)} = \mu^{(i)} + \sigma^{(i)}\epsilon^{(l)}, \epsilon^{l}\sim p(\epsilon)\)</span> and <span class="math inline">\(\log p_{\theta}(x|z) = \sum_{j=1}^Dx_j \log y_j + (1-x_j)\log (1-y_j)\)</span>.</p>
</div>
</div>
</div>
<div id="using-variational-auto-encoder-in-python" class="section level1">
<h1>Using Variational Auto-Encoder in python</h1>
<div id="import-packages" class="section level2">
<h2>Import packages</h2>
<pre class="python"><code>import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data</code></pre>
</div>
<div id="function-for-visualizing-batch-images" class="section level2">
<h2>Function for visualizing batch images</h2>
<pre class="python"><code>def VisConcatImg(batch_images, title):
    batch_size = np.shape(batch_images)[0]
    sqrt_size = int(batch_size ** 0.5)
    batch_images = batch_images.reshape(batch_size, 28, 28)
    row_concatenated = [np.concatenate(batch_images[i*sqrt_size : (i+1)*sqrt_size], axis=1) for i in range(sqrt_size)]
    concatenated = np.concatenate(row_concatenated, axis=0)
    plt.imshow(concatenated, cmap=&#39;gray&#39;)
    plt.title(title)
    plt.axis(&#39;off&#39;)
    plt.show()</code></pre>
</div>
<div id="mnist-dataset" class="section level2">
<h2>MNIST Dataset</h2>
<p>The MNIST includes 60000 training samples and 10000 testing samples. Each sample is a 784-dimensional vector (28??28), with pixel values in [0, 1], which can be assumed as multivariate Bernoulli variables.</p>
<pre class="python"><code># Downloading MNIST dataset
mnist = input_data.read_data_sets(&#39;./mnist&#39;, one_hot=False)
# VAE for MNIST
class VAE(object):
    def __init__(self, x_size=28*28, hidden1_size=100, hidden2_size=400, hidden3_size=100, hidden4_size=400, z_size=20, learning_rate=1e-4):
        self.x_size = x_size
        self.hidden1_size = hidden1_size
        self.hidden2_size = hidden2_size
        self.hidden3_size = hidden3_size
        self.hidden4_size = hidden4_size
        self.z_size = z_size
        self.learning_rate = learning_rate
        self.x = tf.placeholder(tf.float32, [None, x_size])
        self.epsilon = tf.placeholder(tf.float32, [None, z_size]) # sample from N(0,1) for every step
        with tf.variable_scope(&#39;encoder&#39;):
            self.encoder()
        with tf.variable_scope(&#39;decoder&#39;):
            self.decoder()
        with tf.variable_scope(&#39;loss&#39;):
            self.compute_loss()
        with tf.variable_scope(&#39;train&#39;):
            self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.total_loss)
    def encoder(self):
        self.hidden1 = tf.layers.dense(self.x, units=self.hidden1_size, activation=tf.nn.relu)
        self.hidden2 = tf.layers.dense(self.hidden1, units=self.hidden2_size, activation=tf.nn.relu)
        self.mu = tf.layers.dense(self.hidden2, units=self.z_size)
        self.sigma = tf.layers.dense(self.hidden2, units=self.z_size, activation=tf.exp)
        self.z = tf.add(self.mu, tf.multiply(self.epsilon, self.sigma))
        
    def decoder(self):
        self.hidden3 = tf.layers.dense(self.z, units=self.hidden3_size, activation=tf.nn.relu)
        self.hidden4 = tf.layers.dense(self.hidden3, units=self.hidden4_size, activation=tf.nn.relu)
        self.y = tf.layers.dense(self.hidden4, units=self.x_size, activation=tf.nn.sigmoid)
        
    # adding 1e-8 before taking the logarithm to avoid numerical instability.
    def compute_loss(self):
        self.recons_loss = tf.reduce_mean(tf.reduce_sum(-(self.x * tf.log(self.y + 1e-8) + (1 - self.x) * tf.log(1 - self.y + 1e-8)), 1))
        self.KL_loss = tf.reduce_mean(-0.5 * tf.reduce_sum(1 + 2 * tf.log(self.sigma + 1e-8) - tf.square(self.mu) - tf.square(self.sigma), 1))
        self.total_loss = self.recons_loss + self.KL_loss
        
# Training VAE
model = VAE()
BATCH_SIZE = 100
EPOCHS = 50
STEPS = int(60000 / BATCH_SIZE)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
for e in range(EPOCHS):
    for i in range(STEPS):
        train_data, _ = mnist.train.next_batch(batch_size=BATCH_SIZE)
        ep = np.random.multivariate_normal(np.zeros(model.z_size), np.eye(model.z_size), size=BATCH_SIZE)
        sess.run(model.train_op, feed_dict={model.x: train_data, model.epsilon: ep})
    REloss, KLloss, Tloss = sess.run([model.recons_loss, model.KL_loss, model.total_loss], feed_dict={model.x: train_data, model.epsilon: ep})
    print(&#39;Epoch: &#39;, e, &#39;| reconstruction loss: &#39;, REloss, &#39;| KL loss:&#39;, KLloss, &#39;| total loss: &#39;, Tloss)
# Visualizing results
test_data, _ = mnist.test.next_batch(batch_size=50)
VisConcatImg(test_data, &#39;raw images&#39;)
ep = np.random.multivariate_normal(np.zeros(model.z_size), np.eye(model.z_size), size=50)
latent, recons_x = sess.run([model.mu, model.y], feed_dict={model.x: test_data, model.epsilon: ep})
VisConcatImg(recons_x, &#39;reconstructed images&#39;)
randoms = np.random.multivariate_normal(np.zeros(model.z_size), np.eye(model.z_size), size=50)
generated_x = sess.run(model.y, feed_dict={model.z: randoms})
VisConcatImg(generated_x, &#39;generated images&#39;)
sess.close()</code></pre>
</div>
<div id="result" class="section level2">
<h2>Result</h2>
<div align="center">
<img src="VAE3.PNG" width = "300" height = "300" alt="Figure3" />
<p>
Figure3. Raw Images
</div>
<div align="center">
<img src="VAE4.PNG" width = "300" height = "300" alt="Figure4" />
<p>
Figure4. Reconstructed Images
</div>
<div align="center">
<img src="VAE5.PNG" width = "300" height = "300" alt="Figure5" />
<p>
Figure5. Generated Images
</div>
</div>
</div>
<div id="reference" class="section level1">
<h1>Reference</h1>
<ol style="list-style-type: decimal">
<li><a href="https://arxiv.org/abs/1312.6114">D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2014. 5, 1</a></li>
<li><a href="https://github.com/eeyangc/Statistical-Machine-Learning">Yang Can. VAE_demo in python. 2018,12,31</a></li>
</ol>
</div>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  var disqus_config = function () {
  
  };
  (function() {
    var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//thompsonhu.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

