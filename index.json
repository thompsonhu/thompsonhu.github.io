[{"authors":null,"categories":["Deep Learning"],"content":" 我们认为每张图片对应的是一个高维向量，我们希望能找出这一类图片所在的图像空间的分布\\(P\\)，GAN的目的其实就是在寻找这个分布\\(P\\)。\n传统的方法我们会想到用最大似然估计：\n 首先给定一些样本数据，可以得到它的分布\\(P_{data}(x)\\)；接着我们假定有一个由参数\\(\\theta\\)决定的分布\\(P_G(x;\\theta)\\)；我们希望能够找到\\(\\theta\\)，满足分布\\(P_G(x;\\theta)\\)尽可能靠近分布\\(P_{data}(x)\\)；假设\\(P_G(x;\\theta)\\)是高斯分布（正态分布），那\\(\\theta\\)就是均值和方差。\n 从分布\\(P_{data}(x)\\)中进行抽样获得一组样本数据\\(\\{x_1,x_2,\\dots,x_m\\}\\)，我们可以得到似然值的表达式 \\[L = \\prod_{i=1}^{m} P_G(x^i;\\theta)\\]我们希望能够找到\\(\\theta^*\\)能够使得似然值\\(L\\)最大。\n  最大似然估计与最小KL散度的等价性 通过对\\(L\\)取对数，我们可以得到对数似然值，同时\\(\\theta^*\\)可以通过下式得到： \\[ \\begin{align} \\theta^* \u0026amp;=\\ \\arg \\max_{\\theta} \\log L\\\\ \u0026amp;=\\ \\arg \\max_{\\theta} \\log \\prod_{i=1}^{m} P_G(x^i;\\theta)\\\\ \u0026amp;=\\ \\arg \\max_{\\theta} \\sum_{i=1}^m \\log P_G(x^i;\\theta) \\end{align} \\]\n由于样本\\(\\{x^1,x^2,\\dots,x^m\\}\\)是随机抽取来自于分布\\(P_{data}(x)\\)，上式可以近似求\\(\\log P_G(x;\\theta)\\)的期望的最大点，可以表示为 \\[ \\begin{align} \\theta^* \u0026amp;\\approx\\ \\arg \\max_{\\theta} E_{x\\sim P_{data}}[\\log P_G(x;\\theta)]\\\\ \u0026amp;=\\ \\arg \\max_{\\theta} \\int\\limits_x P_{data}(x) \\log P_G(x;\\theta) dx \\end{align} \\]\n从式子中知道，我们所求的\\(\\theta^*\\)只跟分布\\(P_G(x;\\theta)\\)相关，我们可以在后面加上一项\\(-\\int\\limits_x P_{data}(x) \\log P_{data}(x)dx\\)，这不影响求最大化下对应得\\(\\theta^*\\)，那么由上式就可以得到 \\[ \\begin{align} \\theta^* \u0026amp;\\approx\\ \\arg \\max_{\\theta} \\int\\limits_x P_{data}(x) \\log P_G(x;\\theta) dx - \\int\\limits_x P_{data}(x) \\log P_{data}(x)dx\\\\ \u0026amp;=\\ \\arg \\max_{\\theta} \\int\\limits_x P_{data}(x) \\Big[\\log P_G(x;\\theta) - \\log P_{data}(x)\\Big] dx\\\\ \u0026amp;=\\ \\arg \\min_{\\theta} \\int\\limits_x P_{data}(x) \\Big[\\log P_{data}(x) - \\log P_G(x;\\theta)\\Big] dx\\\\ \u0026amp;=\\ \\arg \\min_{\\theta} \\int\\limits_x P_{data}(x) \\log \\frac{P_{data}(x)}{P_G(x;\\theta)} dx\\\\ \u0026amp;=\\ \\arg \\min_{\\theta} KL(P_{data}||P_G) \\end{align} \\]\n从上述推导可以发现最大似然估计实际上等价于最小化KL散度。\n 用生成器Generator定义概率分布\\(P_G\\) 如果\\(P_G\\)是很复杂的形式，那我们很难去直接计算我们的目标值。虽然我们可以假定一些简单的已知的分布来确定\\(P_G\\)，比如高斯分布；但是很多情况下，高斯分布的实验结果并不是很好；所以，我们希望用一个更generalized的函数来定义\\(P_G\\)。\n我们用生成器Generator（一个网络结构）来定义概率分布\\(P_G\\)，我们希望输入某个已知分布（比如高斯分布）随机产生的一个数\\(z\\)，通过Generator之后可以得到\\(x=G(z)\\)，那么这些\\(x\\)可以构成某个复杂的分布，也可以说这个复杂的分布是我们希望得到的分布\\(P_G\\)，我们希望这个分布\\(P_G\\)能够和分布\\(P_{data}\\)越接近越好；从散度（Divergence）的角度来说，我们希望分布\\(P_G\\)和分布\\(P_{data}\\)的散度能够越小越好，即 \\[ G^* = \\arg \\min_G \\text{Div}(P_G,P_{data}) \\tag{1} \\]\n如果是最大似然估计的话，我们会希望KL散度越小越好，即 \\[G^* = \\arg \\min_G KL(P_G,P_{data})\\]\n 图1. 生成器Generator流程示意图   如何获得我们需要的散度Divergence？ 由于我们无法确定\\(P_{data}(x)\\)和\\(P_G(x)\\)的分布，所以我们也无法直接计算得到他们的Divergence，当然就没办法直接用梯度下降去找到最小的Divergence小对应的Generator。\n虽然我们无法知道\\(P_{data}(x)\\)和\\(P_G(x)\\)的分布，但是利用重抽样（Resample）的想法，我们可以对它们这两个分布进行抽样；对于\\(P_{data}(x)\\)而言，我们从原数据抽取样本；对于\\(P_G(x)\\)而言，我们从高斯分布抽样再通过前面确定的Generator得到样本。\n我们引入上一Part讲到的监督器Discriminator（我们说到Discriminator也是一个network），对于训练Discriminator的时候，假设我们的目标函数是 \\[ V(G,D) = E_{x\\sim P_{data}}[\\log D(x)] + E_{x\\sim P_{G}}[\\log (1-D(x))] \\tag{2} \\]\n其中\\(G\\)代表Generator，它在这一步是固定不变的；我们希望最大化\\(V(G,D)\\)，式（2）第一项\\(E_{x\\sim P_{data}}[\\log D(x)]\\)表示从\\(P_{data}(x)\\)得到的\\(x\\)，我们希望\\(D(x)\\)能够尽可能大；而对于第二项\\(E_{x\\sim P_{G}}[\\log (1-D(x))]\\)，表示从\\(P_G(x)\\)得到的\\(x\\)，我们希望\\(D(x)\\)能够尽可能小，因为这样\\(1-D(x)\\)才能尽可能大；整体两部分加起来才能够做到尽可能大，实现\\(V(G,D)\\)的最大化。由于抽取的样本\\(x\\)在这个环节是固定的，相当于我们需要找到一个Discriminator满足最大化\\(V(G,D)\\)，即 \\[ D^* = \\arg \\max_D V(G,D) \\]\n【注：使用式（2）作为目标函数相当于训练一个二分类器】\n当抽取的样本分散的很开的时候，即它们的散度很大的时候，我们很容易训练得到一个Discriminator能够容易区分来自两个不同分布的样本（很容易使我们的目标函数变大）；但如果抽取的样本分散的不开的时候，即它们的散度比较小时，我们训练得到的Discriminator是很难区分来自两个不同分布的样本的（很难使我们的目标函数变大）。\n 图2. 生成器Discriminator流程示意图  最大化目标函数\\(V(D,G)\\)与散度Divergence的联系 在给定Generator情况下，我们想要找到一个Discriminator使得我们的目标函数\\(V(G,D)\\)最大，依据上面式（2），我们可以得到： \\[ \\begin{align} V(G,D) \u0026amp;=\\ E_{x\\sim P_{data}}[\\log D(x)] + E_{x\\sim P_G}[\\log (1-D(x))]\\\\ \u0026amp;=\\ \\int\\limits_x P_{data}(x) \\log D(x) dx + \\int\\limits_x P_G(x) \\log (1-D(x)) dx\\\\ \u0026amp;=\\ \\int\\limits_x \\Big[P_{data}(x) \\log D(x) + P_G(x) \\log (1-D(x))\\Big] dx \\end{align} \\tag{3} \\]\n给定\\(x\\)的情况下，最大化式（3）等价于最大化 \\[ P_{data}(x) \\log D(x) + P_G(x) \\log (1-D(x)) \\tag{4} \\]\n那么，原优化问题等价于找一个最优Discriminator使得式（4）最大。由于\\(x\\)固定了，那么\\(P_{data}(x)\\)和\\(P_G(x)\\)是确定的；我们将\\(P_{data}(x)\\)记作\\(a\\)，\\(P_G(x)\\)记作\\(b\\)，\\(D(x)\\)记作\\(D\\)，那式（4）可以变化为： \\[ f(D) = a\\log(D) + b\\log (1-D) \\tag{5} \\]\n对式（5）求导并赋值为0，可以得到： \\[ \\begin{align} \u0026amp; \\ \\frac{df(D)}{dD} = \\frac{a}{D} - \\frac{b}{1-D} = 0\\\\ \\Rightarrow\u0026amp; \\ \\frac{a}{D} = \\frac{b}{1-D}\\\\ \\Rightarrow\u0026amp; \\ a\\times (1-D) = b\\times D\\\\ \\Rightarrow\u0026amp; \\ D = \\frac{a}{a+b} \\end{align} \\]\n将符号复原，我们知道满足式（4）最大的\\(D^*\\)满足 \\[ D^* = \\frac{P_{data}(x)}{P_{data}(x)+P_G(x)} \\]\n将\\(D^*\\)带入\\(V(G,D) = E_{x\\sim P_{data}}[\\log D(x)] + E_{x\\sim P_{G}}[\\log (1-D(x))]\\)，我们可以得到 \\[ \\begin{align} \\max_D V(G,D) \u0026amp;=\\ V(G,D^*)\\\\ \u0026amp;=\\ E_{x\\sim P_{data}}[\\log D(x)] + E_{x\\sim P_G}[\\log (1-D(x))]\\\\ \u0026amp;=\\ E_{x\\sim P_{data}}[\\log \\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}] + E_{x\\sim P_G}[\\log (1-\\frac{P_{data}(x)}{P_{data}(x)+P_G(x)})]\\\\ \u0026amp;=\\ E_{x\\sim P_{data}}[\\log \\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}] + E_{x\\sim P_G}[\\log \\frac{P_G(x)}{P_{data}(x)+P_G(x)}]\\\\ \u0026amp;=\\ \\int\\limits_x P_{data}(x)\\log \\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}dx + \\int\\limits_x P_G(x)\\log \\frac{P_G(x)}{P_{data}(x)+P_G(x)}dx\\\\ \u0026amp;=\\ \\int\\limits_x P_{data}(x)\\log \\frac{\\frac{1}{2}P_{data}(x)}{\\frac{1}{2} [P_{data}(x)+P_G(x)]}dx + \\int\\limits_x P_G(x)\\log \\frac{\\frac{1}{2}P_G(x)}{\\frac{1}{2} [P_{data}(x)+P_G(x)]}dx\\\\ \u0026amp;=\\ \\int\\limits_x P_{data}(x)\\log \\frac{P_{data}(x)}{\\frac{1}{2} [P_{data}(x)+P_G(x)]}dx + \\int\\limits_x P_G(x)\\log \\frac{P_G(x)}{\\frac{1}{2} [P_{data}(x)+P_G(x)]}dx - 2\\log2\\\\ \u0026amp;=\\ KL(P_{data} || \\frac{P_{data}+P_G}{2}) + KL(P_G || \\frac{P_{data}+P_G}{2}) - 2\\log2 \\end{align} \\]\nJS散度（Jensen-Shannon Divergence）是这样定义的： \\[ JS(P_1||P_2)=\\frac{1}{2}KL(P_1||\\frac{P_1+P_2}{2})+\\frac{1}{2}KL(P_2||\\frac{P_1+P_2}{2}) \\]\n因此，我们可以推得 \\[ \\max_D V(G,D) = V(G,D^*) = -2\\log 2 + 2JS(P_{data}||P_G) \\]\n从上面可以看出，我们在固定Generator之后，对\\(V(G,D)\\)的最大化相当于计算\\(P_{data}\\)和\\(P_G\\)之间的JS散度再减去2倍\\(\\log2\\)。\n在式（1）中散度Divergence是无法直接计算的，但是从刚刚的推导中，我们知道最大化目标函数\\(V(G,D)\\)可以得到散度的值，我们进一步用最大化\\(V(G,D)\\)替代\\(P_{data}\\)和\\(P_G\\)之间的散度，可以得到一个Min-Max的优化问题。 \\[ \\begin{align} G^* \u0026amp;=\\ \\arg \\min_G \\text{Div}(P_G,P_{data})\\\\ \u0026amp;=\\ \\arg \\min_G \\max_D V(G,D) \\end{align} \\]\n 求解Min-Max问题 上面我们最终得到了一个Min-Max最优化问题 \\[G^* = \\arg \\min_G \\max_D V(G,D)\\]\n如果我们手中没有Generator的话，我们无法得到\\(V(G,D)\\)的最大值，在解决这个优化问题之时，我们可以按一下步骤进行：\n 初始化一个带随机参数的Generator \\(G_0\\)；  对于\\(i = 0,1,\\dots,n\\)，重复进行下列步骤：\n 通过梯度上升（Gradient Ascent）方法找到满足最大化\\(V(G_i,D)\\)的\\(D_i\\)；（这一步等价于获得了\\(P_{data}\\)和\\(P_{G_i}\\)之间的JS散度）\n 通过梯度下降（Gradient Descent）方法得到\\(G_{i+1}\\)：\n  \\[\\theta_{G_{i+1}} = \\theta_{G_i} - \\eta\\frac{\\partial V(G,D_i)}{\\partial \\theta_G}\\]\n算法的大致框架得到了，但是对于细节\\(V(G,D)\\)的形式我们只有式（2）；对于式（2）中的期望，实际上我们是很难得到的，在实践中，我们用样本均值来代替期望值；因此最大化\\(V(G,D)\\)可以转换为最大化 \\[ \\tilde{V} = \\frac{1}{m}\\sum_{i=1}^m\\log D(x^i) + \\frac{1}{m}\\sum_{i=1}^m\\log (1-D(\\tilde{x}^i)) \\] 其中样本\\(\\{x^1,x^2,\\dots,x^m\\}\\)来自分布\\(P_{data}(x)\\)，样本\\(\\{\\tilde{x}^1,\\tilde{x}^2,\\dots,\\tilde{x}^m\\}\\)来自分布\\(P_G(x)\\)。\n GAN的算法 通过上面的推导介绍，我们可以总结出GAN的算法框架出来：  图3. GAN的算法框架  可以注意到，在训练Generator的时候，其中\\(\\tilde{V}\\)的第一项实际上是与Generator是无关的，去掉第一项不影响我们最小化目标函数\\(\\tilde{V}\\)。显然，整个GAN算法分为两部分，我们可以这么理解：第一步训练Discriminator实际上是度量两个分布间JS散度，而训练Generator是在最小化JS散度。\n从数学推导可以帮助我们更好理解GAN，这一Part就结束啦๑乛◡乛๑\n下一Part我们尝试用Tensorflow实现GAN！！！\n  Reference 李宏毅个人主页   ","date":1547510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547510400,"objectID":"d03491a09024157e17f22f6dd9546953","permalink":"/post/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AC%AC%E4%BA%8Cpart/","publishdate":"2019-01-15T00:00:00Z","relpermalink":"/post/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AC%AC%E4%BA%8Cpart/","section":"post","summary":"我们认为每张图片对应的是一个高维向量，我们希望能找出这一类图片所在的图像空间的分布\\(P\\)，GAN的目的其实就是在寻找这个分布\\(P\\)。\n传统的方法我们会想到用最大似然估计：\n 首先给定一些样本数据，可以得到它的分布\\(P_{data}(x)\\)；接着我们假定有一个由参数\\(\\theta\\)决定的分布\\(P_G(x;\\theta)\\)；我们希望能够找到\\(\\theta\\)，满足分布\\(P_G(x;\\theta)\\)尽可能靠近分布\\(P_{data}(x)\\)；假设\\(P_G(x;\\theta)\\)是高斯分布（正态分布），那\\(\\theta\\)就是均值和方差。\n 从分布\\(P_{data}(x)\\)中进行抽样获得一组样本数据\\(\\{x_1,x_2,\\dots,x_m\\}\\)，我们可以得到似然值的表达式 \\[L = \\prod_{i=1}^{m} P_G(x^i;\\theta)\\]我们希望能够找到\\(\\theta^*\\)能够使得似然值\\(L\\)最大。\n  最大似然估计与最小KL散度的等价性 通过对\\(L\\)取对数，我们可以得到对数似然值，同时\\(\\theta^*\\)可以通过下式得到： \\[ \\begin{align} \\theta^* \u0026amp;=\\ \\arg \\max_{\\theta} \\log L\\\\ \u0026amp;=\\ \\arg \\max_{\\theta} \\log \\prod_{i=1}^{m} P_G(x^i;\\theta)\\\\ \u0026amp;=\\ \\arg \\max_{\\theta} \\sum_{i=1}^m \\log P_G(x^i;\\theta) \\end{align} \\]\n由于样本\\(\\{x^1,x^2,\\dots,x^m\\}\\)是随机抽取来自于分布\\(P_{data}(x)\\)，上式可以近似求\\(\\log P_G(x;\\theta)\\)的期望的最大点，可以表示为 \\[ \\begin{align} \\theta^* \u0026amp;\\approx\\ \\arg \\max_{\\theta} E_{x\\sim P_{data}}[\\log P_G(x;\\theta)]\\\\ \u0026amp;=\\ \\arg \\max_{\\theta} \\int\\limits_x P_{data}(x) \\log P_G(x;\\theta) dx \\end{align} \\]\n从式子中知道，我们所求的\\(\\theta^*\\)只跟分布\\(P_G(x;\\theta)\\)相关，我们可以在后面加上一项\\(-\\int\\limits_x P_{data}(x) \\log P_{data}(x)dx\\)，这不影响求最大化下对应得\\(\\theta^*\\)，那么由上式就可以得到 \\[ \\begin{align} \\theta^* \u0026amp;\\approx\\ \\arg \\max_{\\theta} \\int\\limits_x P_{data}(x) \\log P_G(x;\\theta) dx - \\int\\limits_x P_{data}(x) \\log P_{data}(x)dx\\\\ \u0026amp;=\\ \\arg \\max_{\\theta} \\int\\limits_x P_{data}(x) \\Big[\\log P_G(x;\\theta) - \\log P_{data}(x)\\Big] dx\\\\ \u0026amp;=\\ \\arg \\min_{\\theta} \\int\\limits_x P_{data}(x) \\Big[\\log P_{data}(x) - \\log P_G(x;\\theta)\\Big] dx\\\\ \u0026amp;=\\ \\arg \\min_{\\theta} \\int\\limits_x P_{data}(x) \\log \\frac{P_{data}(x)}{P_G(x;\\theta)} dx\\\\ \u0026amp;=\\ \\arg \\min_{\\theta} KL(P_{data}||P_G) \\end{align} \\]","tags":["Deep Learning","Algorithm"],"title":"生成对抗网络的第二Part","type":"post"},{"authors":null,"categories":["Deep Learning"],"content":"\r从知乎上了解到台大有位著名的教授李宏毅超级会讲Generative Adversarial Networks, GAN技术，所以慕名而到Youtube找到他的上课视频成为他的“课外学生”。李教授真的厉害，形象生动地讲解GAN的各个知识点。那么，我把我学到的整理为一篇博客，尝试作为一名“GAN路上的导游”。\nYann LeCun是Facebook的AI研究部门的Director，同时也是NYU（New York University）的一位教授，维基百科上是这么介绍他：\n\rHe is the Chief Artificial Intelligence Scientist at Facebook AI Research, and he is well known for his work on optical character recognition and computer vision using convolutional neural networks (CNN), and is a founding father of convolutional nets.\n\r做Deep Learning的人多多少少会听过这个名字，他曾经这样回答了Quora论坛上的一个问题（What are some recent and potentially upcoming breakthroughs in unsupervised learning?）：\n\rAdversarial training is the coolest thing since sliced bread.\n\r这里sliced bread的中文意思是切片面包，但其实这里是表示了有一个好东西问世，给某个领域带来了巨大发展，维基百科这么说：\n\rThe phrase “the greatest thing since sliced bread” is a common hyperbole used to praise an invention or development.\n\r这也说明了GAN推进了整个领域的发展。Yann LeCun对GAN也有过这样极高的评价：\n\rThis, and the variations that are now being proposed is the most interesting idea in the last 10 years in ML, in my opinion.\n\r李教授统计了ICASSP（International Conference on Acoustics, Speech and Signal Processing）的文章题目涵盖关键词的数量：\r\r图1. ICASSP文章题目涵盖关键词的数量变化图\r\r很明显，从17年的2篇Adversarial到18年的42篇Adversarial，同年增长了21倍！相当惊人！既然GAN这么Popular又是这么酷炫，那么我们就开始正文吧！(๑•̀ㅂ•́)و✧\n什么是GAN？说GAN就干！\rGAN里面主要分为Generator和Discriminator这两部分，其实原理很简单，Generator是负责训练样本并能生成对的Output，而Discriminator像是一位老师，看看Generator这位学生交的作业质量怎么样，会给Generator的作业一个分数。下面我们更详细地介绍他们这两部分~\n一无所知的Generator生成器\rGenerator其实就是neural network (NN)，它的输入是向量，那我们如果丢一个向量到Generator里面，它就能产生某个Output（可能是一张相片或者一句话等等）\r\r图2. Generator生成器的示例图\r\r李教授酷爱二次元，所以他举了这样的例子，丢一些向量给生成图片的Generator就生成了一些二次元的图片；另外，对应句子生成的例子，丢一些向量给生成句子的Generator就生成了一些句子。简单来说，我们给Generator这个function（NN其实就是一个复杂的function）赋值，它就会产生对应的结果。\n我们以图片为例，输入的向量中的每一个元素，可能对应着图片中不同的特征。假设第一位是改变头发的长短特征，那从下图中可以看到0.1改为3后，图片中的女孩从短头发变为长头发；假设向量倒数第二位是改变头发的颜色特征，那从下图可以看到5.4改为2.4后，图片中的女孩从紫色变为了蓝色；假设向量倒数第一位是改变嘴巴特征，那从下图可以看到0.9改为3.5后，图片中的女孩从小嘴巴变为了张开嘴巴。\r\r图3. 输入向量对结果输出的影响\r\r在实际操作中，如果我们通过大量的样本让Generator训练，使其能够输出与图片尽可能相似的结果，那这样就只是普通的NN（神经网络），但我们想要更高级！想要输出的图片质量更好！而GAN满足了我们的需求，它的idea妙就妙在搞多了一个Discriminator（监督器），审判Generator输出的结果是不是真的“好”。\n\r超严厉的Discriminator监督器\rDiscriminator也是一个NN，但是它的输入是一张图片或是一句话（Generator的输出）。而它的输出是一个数值，这个数值代表了这张图片的质量如何，数值越大，那图片的质量就越好，越像是真实的图片；相反，数值越小，图片质量越差。下面可以看到不同质量的二次元图片对应着不同的得分数值~\r\r图4. 不同二次元图片对应不同的得分\r\r前面大致讲了一下Generator和Discriminator的关系，但是不算很生动详细！下面我来举个栗子！\n我们可以把Generator和Discriminator当做捕食者和被捕食者，那Pokemon里面鸟系对虫系就有着威慑能力，天生虫系会怕鸟系嘛，这很合理╮(๑•́ ₃•̀๑)╭\r\r图5. 小智的比比鸟和绿毛虫的初次相遇\r\r其实一开始绿毛虫（一代Generator）就很怕波波（一代Discriminator）嘛，所以它就会进化成铁甲蛹（二代Generator），那波波个子小就拿它没办法了，吃也吃不了╮(๑•́ ₃•̀๑)╭所以波波也进化了变成了比比鸟（二代Discriminator）。那现在比比鸟翅膀大了，爪子一夹，把铁甲蛹抓到空中再丢下来（极其残忍！），铁甲蛹也会痛！所以铁甲蛹不甘示弱，它接着进化成巴大蝴（三代Generator）！这下比比鸟没办法它也进化成比雕（三代Discriminator），它以为比雕可能会搞得定巴大蝴！奈何巴大蝴也有翅膀了，还有催眠粉！比雕觉得没办法了，就这样吧，两人实力相当了！\n\r图6. Generator和Discriminator的相互关系\r\r回到现实！这里的巴大蝴就是我们的最终Generator（这里以三代为例，实际需要N代），那它所生成的图片可以让最终的Discriminator认为是真实的，这就达到了我们的目的(*•̀ㅂ•́)و\n\rGAN的算法框架\r我们了解了Generator和Discriminator的基本知识后，我们来看看算法~\n首先随机产生了两个NN作为Generator和Discriminator，然后不断循环：固定Generator调Discriminator的参数，再固定Discriminator调Generator的参数。具体过程如下所示：\n在第一步中固定Generator调Discriminator的参数，对于从Database出来的真实案例，我们希望Discriminator给高分；对于Generator生成的案例，我们希望Discriminator给低分。Discriminator通过训练会在这个过程中学会给真实的案例高分，给假的案例低分。\r\r图7. 固定Generator下训练Discriminator\r\r在下一步中我们固定Discriminator调Generator的参数，我们希望调整Generator后，我们给一个向量生成一个案例，这个案例通过Discriminator得到的分数能够尽量高。\r\r图8. 固定Discriminator下训练Generator\r\r如果我们写成Pseudocode（伪代码），就变成下面这样：\r\r图9. 关于GAN算法的伪代码\r\r\r\rUnsupervised Conditional Generation非监督的条件生成器\r通常在普通的神经网络方法下，但是我们需要对应的样本来帮助我们生成我们需要的图片；我们给一个输入，再给一个Label告诉机器，你看到这个输入就需要有这样的Label的输出，但如果对于某些样本我们刚好没有对应Label的话，但我们手中有其他类似的，那我们可以通过非监督的条件生成器来生成我们想要的结果。\n我们希望能够创造一个Generator，输入一个来自Domain X的样本，可以输出一个对应在Domain Y的样本。这相当于一种风格转化的案例。举个例子，现在许多人喜欢拍照用滤镜，通常我们拍的普通相片是没有带滤镜的，那现在我希望能够得到水彩画形式的相片，如下所示：\r\r图10. 变成水彩画形式的相片\r\r样本的直接转化问题（Direct Transformation）\r像上面叙述的例子，其实普通相片到油画形式的相片，只是色彩质地有所区别，但是总体的框架基本不变，相当于说只是小部分修改了我们输入的相片，那Direct Transformation就足够帮我们处理这个问题了。\n通常我们会用GAN的技术来实现，GAN其实也是可以解决这个问题，如果Generator的层数很少，不是那么复杂，在Discrimiantor的监督下生成油画型的图片跟输入的图片差距不会太大。但是如果Generator很复杂的情况下，Generator是存在可能会生成在Domain Y的相片与在Domain X的相片差距很大。即使在Discriminator中拿到了很高的分数，但是与原始输入的样本差距甚远，如下图所示：\r\r图11. 普通GAN的弱点\r\r在Domain X的河道图输入之后，尽管训练得到的Generator产生了艺术型的油画图，但是明显上图中的梵高自画像不是我们希望得到的输出，但它确实是属于Domain Y类型的图片。\nCycle GAN\r存在上述的问题，那么我们想在生成Domain Y的图片之后，我们可以做一个逆函数（NN）来返回去检验这个图片，看看能否恢复成我们输入图片的样子；当然，我们需要Discriminator帮助我们将输入的图片训练生成在Domain Y的图片。\r\r图12. Cycle GAN处理过程示意图\r\r\rStarGAN\r如果现在我们不仅仅希望普通相片可以变成一张油画照，我们还希望可以变成黑白照或者素描照，那我们的案例就变得更复杂，需要多个Domain，而且我们希望能够在多个Domain里面互转。假设我们有普通照片，油画照，黑白照和素描照这四种类型照片，那其实我们需要创建\\(C_4^2\\)个Cycle GAN来解决这样一个大问题，使得在这四个Domain之间实现互转（如下图(a)所示）。\n2017年arXiv上Yunjey Choi等人发表了文章StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation，StarGAN的方便之处是在于只学习了一个Generator，就可以在多个Domain之间实现互转（如下图(b)所示）。\n\r图13. Cross-domain models和StarGAN的示意图\r\r下面我们通过原文提供的案例来进一步了解StarGAN：\n\r首先，我们先训练Discriminator。这个Discriminator的输入有真实的照片和假的照片，我们希望Discriminator可以输出判断输入的照片是真的还是假的；同时希望可以输出判断真实的照片属于哪一个Domain。在下图案例可以看到，我们真实判断的Domain是\\(0\\ 0\\ 1\\ 0\\ 1\\)相当于真实照片是一个棕色头发年轻的女性角色。在这个地方可以注意到，我们只在考虑CelebA label，但是没有考虑RaFD label，而这个是由Mask vector所控制的（先设定为\\(1\\ 0\\)）。\n\r接着我们以真实照片（棕色头发年轻的女性角色）和我们希望得到的Domain（\\(1\\ 0\\ 0\\ 1\\ 1\\)）作为我们训练Generator的输入，我们希望输出得到目标Domain的照片（转化案例是希望得到一个黑色头发年轻的男性角色）。\n\r利用Cycle GAN的想法，我们用输出的照片（黑色头发年轻的男性角色）和我们原始的Domain（\\(0\\ 0\\ 1\\ 0\\ 1\\)）作为另外一个Generator的输入，希望得到和原始一模一样的照片（棕色头发年轻的女性角色）。\n\r最后呢~将第二步训练得到的照片作为Discriminator的输入，训练Discriminator判断能不能认为输入的照片是真实的照片，并且是属于我们希望得到的Domain（\\(1\\ 0\\ 0\\ 1\\ 1\\)）。\n\r循环前面第二到第四步，直到第四步中Discriminator认为第二步所输出的照片是真实的且属于Domain（\\(1\\ 0\\ 0\\ 1\\ 1\\)）。\n\r\r同理，我们可以让Mask vector为\\(0\\ 1\\)，不考虑CelebA label了而是考虑RaFD label，即考虑情绪的变换。\r\r图14. 来自原Paper的案例\r\r\r\r投影到共有的空间（Projection to Common Space）\r那如果想要转化的结果跟你原来输入的样本结果差别很大，对于图片而言，可能画风突变！比如从真人头像到二次元的漫画头像，这时没办法简单地进行Direct Transformation了，需要利用Projection这样的技术。如果我们先通过编码器获得latent variables，再通过解码器获得我们想要的输出，这样就有可能做到比较大的变换。\n问题\n对于两个Domain的大变化的互转，我们希望可以先分别对两个不同Domain的样本做Auto-Encoder，同时最小化重建的误差（Minimizing reconstruction error）。我们假设Domain X是漫画版本闪电侠，我们希望直接通过图片转化成真人版本的闪电侠（Domain Y），我们按刚刚说的流程，实现的流程如下图所示。\r\r图15. 漫画闪电侠与真人版的互换\r\r但是这样做存在问题就是Domain X做好的Auto-Encoder跟Domain Y做好的Auto-Encoder是完全没有联系的，这种情况下可能我输入一张闪电侠的漫画图片，通过Y的解码器（Decoder）之后，没办法得到闪电侠的真人图片，而是得到了绿箭侠的真人图片(๑•́ ₃ •̀)\r\r图16. 漫画闪电侠的变换结果出错的可能性\r\r【注：美剧闪电侠第五季的第九集和美剧绿箭第七季的第九集中，Oliver Queen变成了闪电侠。。。Barry Allen变成了绿箭侠。。。源自异世界的剧情_(:з」∠)_ 】\r\r回到正题！！！\n方法1\n那有一种方法呢，就是在获得中间的latent variables之前时，两个encoder最后有几层是共享相同参数的；同样在进入decoder部分时，前面几层也是共享相同参数的。这样在获得latent variables的时候，能够尽可能落在相同的latent space。这样的方法出现在文章Coupled Generative Adversarial Networks和文章Unsupervised Image-to-Image Translation Networks。\r\r图17. 共享参数的方法示意图\r\r方法2\n另外一种方法就是我们加一个Domain Discriminator，对中间产生的latent variables进行判定，判定产生的latent variables是来自\\(EN_X\\)还是来自\\(EN_Y\\)，那么两个Encoder就会训练使得产生的latent variables能够骗过Discriminator。这样相当于这个Domain Discriminator促使两个Encoder产生的latent variables来自相同的分布。\r\r图18. 加入Domain Discriminator的示意图\r\r方法3\n利用Cycle GAN的想法，我们还可以输入一张我们想要转换的图片（比如输入是漫画闪电侠），通过Domain X的encoder和Domain Y的decoder得到目标输出结果；然后，将这个结果作为Domain Y的encoder的输入，再通过Domain X的decoder得到我们想要转换的图片，当然我们会希望重建误差尽可能小。\n另外，我们可以加入两个Discriminator来判断两个decoder生成的图片是否属于各自相对应的Domain，这样的想法就有用在ComboGAN上，ComboGAN的文章全名叫做ComboGAN: Unrestrained Scalability for Image Domain Translation。\r\r图19. Cycle GAN想法下的流程示意图\r\r方法4\n除了最小化重建误差，我们也可以最小化从Domain X和Domain Y分别编码得到的latent variables。跟方法3有点相似，先输入一张我们想要转换的图片（比如输入是漫画闪电侠），通过Domain X的encoder得到latent variables，再通过Domain Y的decoder得到目标输出结果；然后，将这个结果作为Domain Y的encoder的输入，可以再次得到latent variables。其中这两次获得的latent variables，我们希望它们能够越接近越好。那这样的方法就有用在DTN和XGAN上，它们的文章全名分别为Unsupervised Cross-Domain Image Generation和XGAN: Unsupervised Image-to-Image Translation for Many-to-Many Mappings。\n更多的应用！语音变换！\r其实这项技术还可以用在语音变换上，可能生活中我们希望做一个变声器，我们说一句话但是扬声器出来的是另外一个人的声音。其实阿笠博士在20年前就已经给柯南一个变声器了，每次柯南都用来假装毛利小五郎的声音(..•˘_˘•..)\r\r图20. 柯南的蝴蝶结变声器\r\r结束基础普及知识的第一Part！！！第二Part再从基础理论来看GAN！\n\r\r\rReference\r李宏毅个人主页\r李宏毅，Youtube：GAN Lecture 1 (2018): Introduction\rI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial networks,” in Advances in Neural Information Processing Systems (NIPS), 2014. 1, 3\rY. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo, “Stargan: Unified generative adversarial networks for multi-domain image-toimage translation,” in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 1, 3, 6, 7, 8, 9, 10\r\r\r","date":1547424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547424000,"objectID":"fe7467601e4b4a7a071f5b06deb13dc2","permalink":"/post/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AC%AC%E4%B8%80part/","publishdate":"2019-01-14T00:00:00Z","relpermalink":"/post/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AC%AC%E4%B8%80part/","section":"post","summary":"从知乎上了解到台大有位著名的教授李宏毅超级会讲Generative Adversarial Networks, GAN技术，所以慕名而到Youtube找到他的上课视频成为他的“课外学生”。李教授真的厉害，形象生动地讲解GAN的各个知识点。那么，我把我学到的整理为一篇博客，尝试作为一名“GAN路上的导游”。\nYann LeCun是Facebook的AI研究部门的Director，同时也是NYU（New York University）的一位教授，维基百科上是这么介绍他：\n\rHe is the Chief Artificial Intelligence Scientist at Facebook AI Research, and he is well known for his work on optical character recognition and computer vision using convolutional neural networks (CNN), and is a founding father of convolutional nets.\n\r做Deep Learning的人多多少少会听过这个名字，他曾经这样回答了Quora论坛上的一个问题（What are some recent and potentially upcoming breakthroughs in unsupervised learning?）：\n\rAdversarial training is the coolest thing since sliced bread.","tags":["Deep Learning"],"title":"生成对抗网络的第一Part","type":"post"},{"authors":null,"categories":["Deep Learning"],"content":" 神经元及神经网络基础结构  图1. 神经元的组成（源自维基百科）  神经元这个图大多数理科生在高中生物课本都学过~神经网络则由许许多多的神经元所组成，通常一个神经元具有多个树突，主要用来接收消息；轴突只有一条，相当于我们定义的一个计算过程；而轴突尾部的许许多多轴突末梢，将传递信息给其他神经元。\n 图2. 神经网络基础结构  通常这里的非线性函数会用上各式各样的激活函数，比如Sigmoid函数，tanh函数和ReLu函数。\nSigmoid函数\n\\[f(z) = \\frac{1}{1+e^{-z}}\\] tanh函数\n\\[f(z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}\\] ReLu函数\n\\[f(z) = \\max(0,z)\\]\n 神经网络基础认知 我们把许多神经元组合起来就可以得到一个神经网络，由于有输入的数据和我们想得到的输出数据，便会有“输入层”（Input layer）和“输出层”（Output layer）；中间的神经元则组成了“隐藏层”（Hidden layer）。在下面图3中，输入层有3个神经元，隐藏层有4个神经元，输出层有2个神经元。在实际情况中，输入层和输出层通常是固定的，而隐藏层的层数和节点数则可以自由调节。  图3. 神经网络基础层级结构  我们假设一个全连接的网络结构，其中隐藏层只有一层。另外，假设输入层和隐藏层之间的边的权值构成的矩阵为 \\[ \\left [ \\begin{matrix} w_{11} \u0026amp; w_{12} \u0026amp; w_{13} \\\\ w_{21} \u0026amp; w_{22} \u0026amp; w_{23} \\\\ w_{31} \u0026amp; w_{32} \u0026amp; w_{33} \\end{matrix} \\right ] \\] 其中，第一列的\\(w_{11}, w_{21}, w_{31}\\)代表的是输入层的点\\(x_1\\)分别连接隐藏层的三个节点的边的权值；第二列的\\(w_{12}, w_{22}, w_{32}\\)代表的是输入层的点\\(x_2\\)分别连接隐藏层的三个节点的边的权值；第三列的\\(w_{13}, w_{23}, w_{33}\\)代表的是输入层的点\\(x_3\\)分别连接隐藏层的三个节点的边的权值。\n图中的“+1”点代表我们添加了一个值b，称其为偏置项。那么，隐藏层的节点可以由下计算得到： \\[ \\begin{align} a_1 = w_{11}\\times x_1 + w_{12}\\times x_2 + w_{13}\\times x_3 + b_1\\\\ a_2 = w_{21}\\times x_1 + w_{22}\\times x_2 + w_{23}\\times x_3 + b_2\\\\ a_3 = w_{31}\\times x_1 + w_{32}\\times x_2 + w_{33}\\times x_3 + b_3 \\end{align} \\tag{1} \\] 由于线性计算的表现能力比较差，所以考虑用非线性函数进行计算，即使用激活函数\\(f(\\cdot)\\)（前面已提及）。（1）式可以变换为（2）式： \\[ \\begin{align} a_1 = f(w_{11}\\times x_1 + w_{12}\\times x_2 + w_{13}\\times x_3 + b_1)\\\\ a_2 = f(w_{21}\\times x_1 + w_{22}\\times x_2 + w_{23}\\times x_3 + b_2)\\\\ a_3 = f(w_{31}\\times x_1 + w_{32}\\times x_2 + w_{33}\\times x_3 + b_3) \\end{align} \\tag{2} \\] 将（2）式改写为矩阵运算形式（3）式： \\[ \\begin{align} \\boldsymbol{a} = f \\begin{pmatrix} \\begin{pmatrix} w_{11},w_{12},w_{13}\\\\w_{21},w_{22},w_{23}\\\\w_{31},w_{32},w_{33} \\end{pmatrix} \\begin{pmatrix} x_1\\\\x_2\\\\x_3 \\end{pmatrix} + \\begin{pmatrix} b_1\\\\b_2\\\\b_3 \\end{pmatrix} \\end{pmatrix} = f(\\boldsymbol{W}x+\\boldsymbol{B}) \\end{align} \\tag{3} \\]\n 图4. 简单全连接网络中层之间的计算方式  当我们增加隐藏层的层数，便可以构成更复杂的网络，即深度神经网络。  图5. 深度神经网络示意图   损失函数（Loss Function） 训练数据通常是一系列“输入-输出”数据对组成的集合，我们希望输入一个数据，尽可能与配对的输出数据相同。那么网络的输出结果和实际的真实结果差多少，我们需要一定数学形式进行量化，所以引入了损失函数（Loss Function）。常见的损失函数有以下几种：\n0-1损失函数\n如果预测值和真实值一样，则损失值为0；若不等，则为1；公式表达为： \\[ L(y,f(x)) = \\begin{cases} 1, \u0026amp; y = f(x)\\\\ 0, \u0026amp; y \\neq f(x) \\end{cases} \\]\n绝对值损失函数（1-范数形式）\n通过预测值和真实值之差的绝对值进行衡量，公式表达为： \\[ L(y,f(x)) = |y-f(x)| \\]\n均方误差损失函数（2-范数形式）\n通过计算预测值和真实值之差的平方再求均值，可得到均方误差，公式表达为： \\[ L(y,f(x)) = \\frac{1}{n}\\sum_{i=1}^n(y_i-f(x_i))^2 \\]\n 优化算法 梯度下降法（Gradient Descent Method） 传统的梯度下降法是通过计算损失函数的一阶导数作为方向进行下降计算，计算方法可表示为： \\[ W_{ij} = W_{ij} - \\alpha\\frac{\\partial}{\\partial W_{ij}}L(w,b)\\\\ b_i = b_i - \\alpha\\frac{\\partial}{\\partial b_i}L(w,b) \\] 其中\\(W_{ij}\\)和\\(b_i\\)是需要优化的参数，\\(L(w,b)\\)是损失函数，\\(\\alpha\\)在深度学习中通常称为学习率（learning rate），在机器学习或最优化计算领域中我们通常称为步长（stepsize）。\n传统的梯度下降法需要计算\\(n\\)个梯度，即样本数量的梯度个数，在数据越来越大的时代，这会大大降低我们需要的计算速度，因此也产生了随机梯度下降（Stochastic gradient descent）这一类的方法。随机梯度下降通常随机选取某个样本并计算其相应导数，作为所有样本相同的导数进行计算，这种方法在实践上有不错的效果。当然，我们可以随机选取一小批样本，样本数量记为batch size，将batch size个样本的导数进行累加后求均值作为所有样本相同的导数，再进一步计算；这种方法我们称为小批量随机梯度下降法（mini-batch stochastic gradient descent）。\n虽然梯度下降直接快速，但是也有一定的不足，由于我们需要选取stepsize，若stepsize太大，那可能无法达到优化问题的最优点；若stepsize太小，则收敛速度太慢，大大降低了模型训练速度。同时，不变的stepsize可能会使结果无法收敛到全局最优解，并可能停在局部最小值（局部最优解），当然很容易陷入到“鞍点”。\n 图6. “鞍点”示意图   Momentum优化器（Momentum Optimizer） Momentum优化器也可称为基于动量的优化算法，其中参数的更新会根据梯度的变化而变化：动量再梯度连续指向同一方向上时会增加，而在梯度方向变化时会减小；这样可以更快地收敛并减少震荡。公式表示为： \\[ v_t^{W} = \\gamma \\times v_{t-1}^{W} + \\alpha \\times \\frac{\\partial}{\\partial W_{ij}}L(w,b)\\\\ W_{ij} = W_{ij} - v_t^{W}\\\\ v_t^{b} = \\gamma \\times v_{t-1}^{b} + \\alpha \\times \\frac{\\partial}{\\partial b_i}L(w,b)\\\\ b_i = b_i - v_t^{b} \\] 其中，\\(\\gamma\\)是动量更新值，通常取0.9。这样，基于Momentum的随机梯度下降可以更快地收敛，并减少陷入局部最优点的概率。\n Adagrad优化器（Adaptive Gradient Optimizer） Momentum优化器虽然加速了参数的更新并加速收敛，但存在缺点是没有对不同的参数进行区别对待。Adagrad优化器则基于这样的梯度优化思想：根据参数自适应地更新学习率（也为步长stepsize），对于不频繁更新的参数做较大更新，而对于频繁更新的参数做较小的更新。\nAdagrad对于每个参数\\(\\theta_{t,i}\\)，在每个时间点\\(t\\)使用不同的学习率。首先我们先考虑Adagrad的单参数情况，为了公式形式的整洁，我们记各个时间点\\(t\\)的参数\\(\\theta_{t,i}\\)下的目标函数梯度为\\(g_{t,i}\\)： \\[g_{t,i} = \\frac{\\partial}{\\partial \\theta_{t,i}}L(\\theta_{t,i})\\] 在Adagrad的更新规则中，我们会根据每个时间点\\(t\\)对每个参数\\(\\theta_{t+1,i}\\)基于上次已经计算过的梯度\\(\\theta_{t,i}\\)来修改步长： \\[\\theta_{t+1,i} = \\theta_{t,i} - \\frac{\\alpha}{\\sqrt{G_{t,ii}+\\epsilon}}\\times g_{t,i}\\] 其中，\\(G_{t,ii}\\in R^{d\\times d}\\)，\\(G_{t,ii}\\)是一个对角矩阵，其对角元素\\(t\\)时刻参数\\(\\theta_{t,i}\\)的梯度平方和，\\(\\epsilon\\)是一个光滑项，防止分母为0，通常取1e-8级别的数。另外，\\(\\alpha\\)使用默认值0.01。Adagrad有个缺点就是其分母实际上累积了梯度的平方，会使得步长（学习率）越来越小。\n Adadelta优化器 Adadelta是对Adagrad的改进，通过用过去计算的梯度平方的均值代替单纯的累加梯度平方，可以避免一味地降低步长。\n\\(t\\)时刻的梯度平方均值表示为： \\[ E[g^2]_{t,i} = \\gamma\\times E[g^2]_{t-1,i} + (1-\\gamma)\\times g_{t,i}^2 \\] 其中，\\(\\gamma\\)和前面提到的Momentum优化器中的\\(\\gamma\\)类似，通常取0.9。将累积梯度平方更改为梯度平方均值，可得到： \\[\\theta_{t+1,i} = \\theta_{t,i} - \\frac{\\alpha}{\\sqrt{E_t+\\epsilon}}\\times g_{t,i}\\] 另外，我们还想要变换分子的\\(\\gamma\\)，将\\(\\gamma\\)改为\\(\\sqrt{E[\\Delta\\theta^2]_t+\\epsilon}\\)，便得到Adadelta的计算形式，以上内容可以总结为： \\[ E[g^2]_{t,i} = \\gamma\\times E[g^2]_{t-1,i} + (1-\\gamma)\\times g_{t,i}^2\\\\ E[\\Delta\\theta^2]_{t,i} = \\gamma\\times E[\\Delta\\theta^2]_{t-1,i} + (1-\\gamma)\\times \\Delta\\theta_{t,i}^2\\\\ \\theta_{t+1,i} = \\theta_{t,i} - \\frac{\\sqrt{E[\\Delta\\theta^2]_{t-1,i}+\\epsilon}}{\\sqrt{E[g^2]_{t,i}+\\epsilon}}\\times g_{t,i} \\] 显然，我们不再需要提前设定步长了。\n Adam优化器（Adaptive Moment Estimation Optimizer, Adam Optimizer） Adam也是个人名，圣经中说他是世上的第一个人类也是第一个男人，接着和夏娃结为夫妻，过上了幸福的生活…跑远了！回正题！其实Adam的全称中文是自适应矩估计，它不仅像Adadelta一样存储过去梯度平方\\(v_t\\)的平均值之外，还保留了像Momentum一样的保留了过去梯度\\(m_t\\)，其计算公式为： \\[ m_t = \\beta_1m_{t-1} + (1-\\beta_1)g_t\\\\ v_t = \\beta_2v_{t-1} + (1-\\beta_2)g_t^2 \\] 由于\\(m_t\\)和\\(v_t\\)在计算上会存在偏差，所以进行了偏差校正： \\[ \\hat{m_t} = \\frac{m_t}{1-\\beta_1^t}\\\\ \\hat{v_t} = \\frac{v_t}{1-\\beta_2^t} \\] Adam的更新规则： \\[\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{v_t}+\\epsilon}}\\times \\hat{m_t}\\] 其中，\\(\\beta_1\\)通常取0.9，\\(\\beta_2\\)通常取0.999，\\(\\epsilon\\)通常取1e-8。大多实验表明，Adam比其他自适应学习算法表现更优。\n 算法表现效果  图7. 不同优化器的随机梯度下降法在鞍点处的不同表现  注：动图来源于Sebastian Ruder的文章，由Alec Radford制作。\n 题外话（跳过这段吧~）\n在整理学习优化算法的时候，发现一件有趣的事。我边看着《Tensorflow入门与实战》的第四章边学习优化算法，同时网上边找找资料帮助理解。然而有趣的是我找到了一位业界大神Sebastian Ruder的主页，并看到了他在2016年1月6日写下了An overview of gradient descent optimization algorithms。看着看着我发现手中拿的书竟然是电脑屏幕上显示的文章的中文版，我便好奇地寻找手中这本实战书的出版时间 —— 2018年2月17日。怪哉怪哉~再翻翻书，发现并无任何引用。算了，回归主题！（Reference选了日期比较前的S.R.大佬的文章作为引用）    反向传播算法（Backpropagation） 反向传播算法是目前用来训练人工神经网络（Artificial Neural Network，ANN）的最常用且最有效的算法。首先我们先定义变量：\n \\(v_i^{(l)}\\)：第\\(l\\)层的第\\(i\\)个节点的输入值，\\(v_i^{(l)} = \\sum_{j=0}^n w_{ij}^{(l)}a_j^{(l-1)} + b_i^{(l)}\\)； \\(a_i^{(l)}\\)：第\\(l\\)层的第\\(i\\)个节点的输出值，\\(a_i^{(l)} = f(v_i^{(l)})\\)，其中\\(f(\\cdot)\\)是激活函数； \\(w_{ij}^{(l)}\\)：第\\(l-1\\)层的第\\(j\\)个节点到第\\(l\\)层的第\\(i\\)个节点的权值； \\(b_i^{(l)}\\)：第\\(l\\)层计算第\\(i\\)个节点的输入值时的偏置项的值； \\(K\\)：神经网络的总层数； \\(f(\\cdot)\\)：激活函数，例如sigmoid函数，tanh函数或者ReLu函数； \\(L(w,b)\\)：整体损失函数，常用的损失函数为\\(\\frac{1}{2n}\\sum_{i=1}^n(y_i-f(x_i))^2\\)，其中n是样本的个数。  反向传播计算过程的细节如下所示：\n 参数初始化\n随机初始化网络中的各层的参数\\(w_{ij}^{(l)}\\)和\\(b_i^{(l)}\\)，且满足\\(N(0,\\ 0.01)\\)分布的随机数；\n 前向传播\n  以图3中隐藏层的第一个节点（从上往下数第一个）为例，对于这个节点而言，其输入信号为： \\[v_1^{(2)} = a_1^{(1)}\\times w_{11}^{(2)} + a_2^{(1)}\\times w_{12}^{(2)} + a_3^{(1)}\\times w_{13}^{(2)} + b_1^{(2)}\\]\n同理，我们可以得到该层的其他节点的计算： \\[ v_2^{(2)} = a_1^{(1)}\\times w_{21}^{(2)} + a_2^{(1)}\\times w_{22}^{(2)} + a_3^{(1)}\\times w_{23}^{(2)} + b_2^{(2)}\\\\ v_3^{(2)} = a_1^{(1)}\\times w_{31}^{(2)} + a_2^{(1)}\\times w_{32}^{(2)} + a_3^{(1)}\\times w_{33}^{(2)} + b_3^{(2)}\\\\ v_4^{(2)} = a_1^{(1)}\\times w_{41}^{(2)} + a_2^{(1)}\\times w_{42}^{(2)} + a_3^{(1)}\\times w_{43}^{(2)} + b_4^{(2)}\\\\ \\]\n若用矩阵形式进行表达： \\[V^{(2)} = A^{(1)}\\times W^{(2)} + B^{(2)}\\] 其中， \\[ V^{(2)} = (v_1^{(2)}, v_2^{(2)}, v_3^{(2)}, v_4^{(2)})\\\\ A^{(1)} = (a_1^{(1)}, a_2^{(1)}, a_3^{(1)})\\\\ W^{(2)} = \\begin{pmatrix} w_{11}^{(2)} \u0026amp; w_{21}^{(2)} \u0026amp; w_{31}^{(2)} \u0026amp; w_{41}^{(2)}\\\\ w_{12}^{(2)} \u0026amp; w_{22}^{(2)} \u0026amp; w_{32}^{(2)} \u0026amp; w_{42}^{(2)}\\\\ w_{13}^{(2)} \u0026amp; w_{23}^{(2)} \u0026amp; w_{33}^{(2)} \u0026amp; w_{43}^{(2)} \\end{pmatrix}\\\\ B^{(2)} = (b_1^{(2)}, b_2^{(2)}, b_3^{(2)}, b_4^{(2)}) \\]\n再经过激活函数（非线性函数）变换后得到： \\[A^{(2)} = f(V^{(2)})\\]\n同理，经由 \\[ V^{(3)} = A^{(2)}\\times W^{(3)} + B^{(3)}\\\\ A^{(3)} = f(V^{(3)}) \\] 可以得到最终输出。\n 反向传播\n首先对于最后一层节点的偏导数，其实我们很容易得到，我们定义神经网络总共有\\(K\\)层，对于最后一层即第\\(K\\)层（输出层），根据偏导数的定义：  \\[ \\begin{align} \\delta_i^{(K)} =\u0026amp;\\ \\frac{\\partial}{\\partial v_i^{(K)}}L(w,b)\\\\ =\u0026amp;\\ \\frac{\\partial L(w,b)}{\\partial a_i^{(K)}}\\times \\frac{\\partial a_i^{(K)}}{\\partial v_i^{(K)}}\\\\ =\u0026amp;\\ \\frac{\\partial L(w,b)}{\\partial a_i^{(K)}}\\times f\u0026#39;(v_i^{(K)}) \\end{align} \\tag{4} \\] 明显的是，\\(a_i^{(K)}\\)是最后一层（即输出层）的输出值，\\(f\u0026#39;(v_i^{(K)})\\)则是激活函数对\\(v_i^{(K)}\\)的导数。\n对（4）式进一步推导可以得到： \\[ \\begin{align} \\delta_i^{(K)} =\u0026amp;\\ \\frac{\\partial}{\\partial a_i^{(K)}}\\Big[\\frac{1}{2n_K}\\sum_{j=1}^{n_K}\\Big(y_j-a_j^{(K)}\\Big)^2\\Big]\\times f\u0026#39;(v_i^{(K)})\\\\ =\u0026amp;\\ -\\frac{1}{n_k}(y_i-a_i^{(K)})\\times f\u0026#39;(v_i^{(K)}) \\end{align} \\] 其中，\\(y_i\\)是样本对应的正确值，\\(n_K\\)是第K层节点个数。\n因此，可得到最后一层（第K层）的计算公式： \\[ \\delta_i^{(K)} = -\\frac{1}{n_k}(y_i-a_i^{(K)})\\times f\u0026#39;(v_i^{(K)}) \\tag{5} \\]\n那么对于第\\(K-1\\)层的偏导数，可以根据第\\(K\\)层的计算出来： \\[ \\begin{align} \\delta_i^{(K-1)} =\u0026amp;\\ \\frac{\\partial}{\\partial v_i^{(K-1)}}L(w,b)\\\\ =\u0026amp;\\ \\frac{\\partial}{\\partial v_i^{(K-1)}}\\Big[\\frac{1}{2n_K}\\sum_{j=1}^{n_K}\\Big(y_j-a_j^{(K)}\\Big)^2\\Big]\\\\ =\u0026amp;\\ \\frac{1}{2n_K}\\Big[\\frac{\\partial}{\\partial v_i^{(K-1)}} \\sum_{j=1}^{n_K}\\Big(y_j-f(v_j^{(K)})\\Big)^2\\Big] \\end{align} \\tag{6} \\]\n利用连续函数的求导和求和顺序可互换，（6）式可以推得： \\[ \\begin{align} \\delta_i^{(K-1)} =\u0026amp;\\ -\\frac{1}{n_K} \\sum_{j=1}^{n_K} \\Big[(y_j-f(v_j^{(K)}))\\times \\frac{\\partial}{\\partial v_i^{(K-1)}}f(v_j^{(K)})\\Big]\\\\ =\u0026amp;\\ -\\frac{1}{n_K} \\sum_{j=1}^{n_K} \\Big[(y_j-f(v_j^{(K)}))\\times \\frac{\\partial f(v_j^{(K)})}{\\partial v_i^{(K)}}\\times \\frac{\\partial v_i^{(K)}}{\\partial v_i^{(K-1)}} \\Big]\\\\ =\u0026amp;\\ \\sum_{j=1}^{n_K} \\Big[-\\frac{1}{n_K} (y_j-f(v_j^{(K)}))\\times f\u0026#39;(v_j^{(K)})\\times \\frac{\\partial v_i^{(K)}}{\\partial v_i^{(K-1)}} \\Big] \\end{align} \\]\n联合（5）式，由（6）式可以得到： \\[ \\begin{align} \\delta_i^{(K-1)} =\u0026amp;\\ \\sum_{j=1}^{n_K} \\Big[ \\delta_i^{(K)}\\times \\frac{\\partial v_i^{(K)}}{\\partial v_i^{(K-1)}} \\Big]\\\\ =\u0026amp;\\ \\sum_{j=1}^{n_K}\\Bigg[ \\delta_i^{(K)}\\times \\frac{\\partial }{\\partial v_i^{(K-1)}}\\Big[ \\sum_{m=0}^{n_{K-1}}a_m^{(K-1)}\\times w_{jm}^{(K)}+b_j^{(K)} \\Big]\\Bigg]\\\\ =\u0026amp;\\ \\sum_{j=1}^{n_K}\\Bigg[ \\delta_i^{(K)}\\times \\frac{\\partial }{\\partial v_i^{(K-1)}}\\Big[ \\sum_{m=0}^{n_{K-1}}f(v_m^{(K-1)})\\times w_{jm}^{(K)}+b_j^{(K)} \\Big]\\Bigg]\\\\ =\u0026amp;\\ \\sum_{j=1}^{n_K}\\Bigg[ \\delta_i^{(K)}\\times f\u0026#39;(v_i^{(K-1)})\\times w_{ji}^{(K)} \\Bigg]\\\\ =\u0026amp;\\ \\Bigg[\\sum_{j=1}^{n_K}\\Big[ \\delta_i^{(K)}\\times w_{ji}^{(K)} \\Big]\\Bigg] \\times f\u0026#39;(v_i^{(K-1)}) \\end{align} \\]\n因此，可得到第K-1层的计算公式： \\[ \\delta_i^{(K-1)} = \\Bigg[\\sum_{j=1}^{n_K}\\Big[ \\delta_i^{(K)}\\times w_{ji}^{(K)} \\Big]\\Bigg] \\times f\u0026#39;(v_i^{(K-1)}) \\tag{7} \\]\n同理，用\\(K-2\\)替换\\(K-1\\)，用\\(K-1\\)替换\\(K\\)，则可计算第\\(K-2\\)层的偏导数。 \\[ \\delta_i^{(K-2)} = \\Bigg[\\sum_{j=1}^{n_{K-1}}\\Big[ \\delta_i^{(K-1)}\\times w_{ji}^{(K-1)} \\Big]\\Bigg] \\times f\u0026#39;(v_i^{(K-2)}) \\tag{7} \\]\n同样的，可以根据（7）式计算得到网络中所有节点的偏导数。\n回归我们的参数迭代公式： \\[ \\begin{align} w_{ij}^{(l)} =\u0026amp;\\ w_{ij}^{(l)} - \\alpha\\times \\frac{\\partial}{\\partial w_{ij}^{(l)}}L(w,b)\\\\ b_i^{(l)} =\u0026amp;\\ b_i^{(l)} - \\alpha\\times \\frac{\\partial}{\\partial b_i^{(l)}}L(w,b) \\end{align} \\tag{8} \\]\n对于后面的偏导数部分，我们可以加以处理，对于参数\\(w_{ij}^{(l)}\\)部分： \\[ \\begin{align} \\frac{\\partial L(w,b)}{\\partial w_{ij}^{(l)}} =\u0026amp;\\ \\frac{\\partial L(w,b)}{\\partial v_i^{(l)}}\\times \\frac{\\partial v_i^{(l)}}{\\partial w_{ij}^{(l)}}\\\\ =\u0026amp;\\ \\delta_i^{(l)}\\times \\frac{\\partial v_i^{(l)}}{\\partial w_{ij}^{(l)}}\\\\ =\u0026amp;\\ \\delta_i^{(l)}\\times \\frac{\\partial }{\\partial w_{ij}^{(l)}}\\Big[\\sum_{j=0}^{n_{l-1}}a_j^{(l-1)}\\times w_{ij}^{(l)}+b_i^{(l)}\\Big]\\\\ =\u0026amp;\\ \\delta_i^{(l)}\\times a_j^{(l-1)} \\end{align} \\]\n对于参数\\(b_i^{(l)}\\)部分： \\[ \\begin{align} \\frac{\\partial L(w,b)}{\\partial b_i^{(l)}} =\u0026amp;\\ \\frac{\\partial L(w,b)}{\\partial v_i^{(l)}}\\times \\frac{\\partial v_i^{(l)}}{\\partial b_i^{(l)}}\\\\ =\u0026amp;\\ \\delta_i^{(l)}\\times \\frac{\\partial}{\\partial b_i^{(l)}}\\Big[\\sum_{j=0}^{n_{l-1}}a_j^{(l-1)}\\times w_{ij}^{(l)}+b_i^{(l)}\\Big]\\\\ =\u0026amp;\\ \\delta_i^{(l)} \\end{align} \\]\n因此，由（8）式可以推得 \\[ \\begin{align} w_{ij}^{(l)} =\u0026amp;\\ w_{ij}^{(l)} - \\alpha\\times \\delta_i^{(l)}\\times a_j^{(l-1)}\\\\ b_i^{(l)} =\u0026amp;\\ b_i^{(l)} - \\alpha\\times \\delta_i^{(l)} \\end{align} \\tag{8} \\]\nOver！反向传播算法到此结束！\n Reference Sebastian Ruder. An overview of gradient descent optimization algorithms\n ","date":1546646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546646400,"objectID":"a02ffc69823c43b02495a4e0f043803c","permalink":"/post/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/","publishdate":"2019-01-05T00:00:00Z","relpermalink":"/post/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/","section":"post","summary":"神经元及神经网络基础结构  图1. 神经元的组成（源自维基百科）  神经元这个图大多数理科生在高中生物课本都学过~神经网络则由许许多多的神经元所组成，通常一个神经元具有多个树突，主要用来接收消息；轴突只有一条，相当于我们定义的一个计算过程；而轴突尾部的许许多多轴突末梢，将传递信息给其他神经元。\n 图2. 神经网络基础结构  通常这里的非线性函数会用上各式各样的激活函数，比如Sigmoid函数，tanh函数和ReLu函数。\nSigmoid函数\n\\[f(z) = \\frac{1}{1+e^{-z}}\\] tanh函数\n\\[f(z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}\\] ReLu函数\n\\[f(z) = \\max(0,z)\\]\n 神经网络基础认知 我们把许多神经元组合起来就可以得到一个神经网络，由于有输入的数据和我们想得到的输出数据，便会有“输入层”（Input layer）和“输出层”（Output layer）；中间的神经元则组成了“隐藏层”（Hidden layer）。在下面图3中，输入层有3个神经元，隐藏层有4个神经元，输出层有2个神经元。在实际情况中，输入层和输出层通常是固定的，而隐藏层的层数和节点数则可以自由调节。  图3. 神经网络基础层级结构  我们假设一个全连接的网络结构，其中隐藏层只有一层。另外，假设输入层和隐藏层之间的边的权值构成的矩阵为 \\[ \\left [ \\begin{matrix} w_{11} \u0026amp; w_{12} \u0026amp; w_{13} \\\\ w_{21} \u0026amp; w_{22} \u0026amp; w_{23} \\\\ w_{31} \u0026amp; w_{32} \u0026amp; w_{33} \\end{matrix} \\right ] \\] 其中，第一列的\\(w_{11}, w_{21}, w_{31}\\)代表的是输入层的点\\(x_1\\)分别连接隐藏层的三个节点的边的权值；第二列的\\(w_{12}, w_{22}, w_{32}\\)代表的是输入层的点\\(x_2\\)分别连接隐藏层的三个节点的边的权值；第三列的\\(w_{13}, w_{23}, w_{33}\\)代表的是输入层的点\\(x_3\\)分别连接隐藏层的三个节点的边的权值。\n图中的“+1”点代表我们添加了一个值b，称其为偏置项。那么，隐藏层的节点可以由下计算得到： \\[ \\begin{align} a_1 = w_{11}\\times x_1 + w_{12}\\times x_2 + w_{13}\\times x_3 + b_1\\\\ a_2 = w_{21}\\times x_1 + w_{22}\\times x_2 + w_{23}\\times x_3 + b_2\\\\ a_3 = w_{31}\\times x_1 + w_{32}\\times x_2 + w_{33}\\times x_3 + b_3 \\end{align} \\tag{1} \\] 由于线性计算的表现能力比较差，所以考虑用非线性函数进行计算，即使用激活函数\\(f(\\cdot)\\)（前面已提及）。（1）式可以变换为（2）式： \\[ \\begin{align} a_1 = f(w_{11}\\times x_1 + w_{12}\\times x_2 + w_{13}\\times x_3 + b_1)\\\\ a_2 = f(w_{21}\\times x_1 + w_{22}\\times x_2 + w_{23}\\times x_3 + b_2)\\\\ a_3 = f(w_{31}\\times x_1 + w_{32}\\times x_2 + w_{33}\\times x_3 + b_3) \\end{align} \\tag{2} \\] 将（2）式改写为矩阵运算形式（3）式： \\[ \\begin{align} \\boldsymbol{a} = f \\begin{pmatrix} \\begin{pmatrix} w_{11},w_{12},w_{13}\\\\w_{21},w_{22},w_{23}\\\\w_{31},w_{32},w_{33} \\end{pmatrix} \\begin{pmatrix} x_1\\\\x_2\\\\x_3 \\end{pmatrix} + \\begin{pmatrix} b_1\\\\b_2\\\\b_3 \\end{pmatrix} \\end{pmatrix} = f(\\boldsymbol{W}x+\\boldsymbol{B}) \\end{align} \\tag{3} \\]","tags":["Deep Learning"],"title":"深度神经网络基础","type":"post"},{"authors":null,"categories":["Rmarkdown"],"content":" 缘于某人用Rmarkdown搞不出中文内容的pdf而引发一场激战之下，TT只能忍气吞声继续走上帮人帮到底的道路，于是网上搜出一大堆关于Rmarkdown生成中文pdf的麻烦事。无奈，众里寻它千百度，最终发现解决问题的YAML模板及相关的解决方案，怕在接下来的日子可能遭受同样的折磨，并以扩充Blog文章为前提，书写此文。\n首先，让我们先在RStudio菜单栏选择Tools并点击Global Options。选择Sweaver并按图勾选，最后点OK~  Figure1. 可爱的Global Options窗口  然后.Rmd文件中的YAML模板如下设置：\n--- title: \u0026quot;我是一个Test文档的标题\u0026quot; author: \u0026quot;我是一个Test文档的作者名称\u0026quot; date: \u0026quot;我是一个Test文档的写作日期\u0026quot; CJKmainfont: Microsoft YaHei output: pdf_document: includes: header-includes: - \\usepackage{xeCJK} keep_tex: yes latex_engine: xelatex --- 注：介个模板用上了大微软的雅黑字体，如若想修改，那请继续摸索摸索。（T.T累了不想改了~）\n搞定！Over！愿你的探索之路不与我一样艰辛((٩(//̀Д/́/)۶))\n课外补充：\n 关于Rmarkdown to pdf的美好世界\n如果你也被这样的问题所困扰，那么你会发现R界的大佬谢益辉搞了个包叫rticles，直接提供template给你写中文文档。然而，无奈Tex世界的混乱，还是遇到奇奇怪怪的乱七八糟的问题，但是大佬说大家用TinyTex吧，那将提供Rmarkdown to pdf的一片美好世界。  ","date":1546387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546387200,"objectID":"787c464bca8e3faac12f75c940ece8da","permalink":"/post/%E5%85%B3%E4%BA%8Ermarkdown%E7%94%9F%E6%88%90%E4%B8%AD%E6%96%87%E5%86%85%E5%AE%B9pdf%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B/","publishdate":"2019-01-02T00:00:00Z","relpermalink":"/post/%E5%85%B3%E4%BA%8Ermarkdown%E7%94%9F%E6%88%90%E4%B8%AD%E6%96%87%E5%86%85%E5%AE%B9pdf%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B/","section":"post","summary":" 缘于某人用Rmarkdown搞不出中文内容的pdf而引发一场激战之下，TT只能忍气吞声继续走上帮人帮到底的道路，于是网上搜出一大堆关于Rmarkdown生成中文pdf的麻烦事。无奈，众里寻它千百度，最终发现解决问题的YAML模板及相关的解决方案，怕在接下来的日子可能遭受同样的折磨，并以扩充Blog文章为前提，书写此文。\n首先，让我们先在RStudio菜单栏选择Tools并点击Global Options。选择Sweaver并按图勾选，最后点OK~  Figure1. 可爱的Global Options窗口  然后.Rmd文件中的YAML模板如下设置：\n--- title: \u0026quot;我是一个Test文档的标题\u0026quot; author: \u0026quot;我是一个Test文档的作者名称\u0026quot; date: \u0026quot;我是一个Test文档的写作日期\u0026quot; CJKmainfont: Microsoft YaHei output: pdf_document: includes: header-includes: - \\usepackage{xeCJK} keep_tex: yes latex_engine: xelatex --- 注：介个模板用上了大微软的雅黑字体，如若想修改，那请继续摸索摸索。（T.T累了不想改了~）\n搞定！Over！愿你的探索之路不与我一样艰辛((٩(//̀Д/́/)۶))\n课外补充：\n 关于Rmarkdown to pdf的美好世界\n如果你也被这样的问题所困扰，那么你会发现R界的大佬谢益辉搞了个包叫rticles，直接提供template给你写中文文档。然而，无奈Tex世界的混乱，还是遇到奇奇怪怪的乱七八糟的问题，但是大佬说大家用TinyTex吧，那将提供Rmarkdown to pdf的一片美好世界。  ","tags":["Rmarkdown"],"title":"关于Rmarkdown生成中文内容pdf的那些事","type":"post"},{"authors":null,"categories":["Deep Learning"],"content":"\rIntroduction\rWe assume the observed variable \\(x\\) is a random sample from an unknown underlying process, whose true distribution \\(p^*(x)\\) is unknown. We attempt to approximate this underlying process with a chosen model \\(p_{\\theta}(x)\\), with parameters \\(\\theta\\): \\[x\\sim p_{\\theta}(x)\\] We always talk about learning like Deep Learning, and actually the learning is the process of searching for a value of the parameters \\(\\theta\\) in model \\(p_{\\theta}(x)\\), which can approximate the true distribution of the data, denoted by \\(p^*(x)\\). In other words, \\[p_{\\theta}(x)\\approx p^*(x)\\] Latent variables are variables that are part of the model, but which we don’t observe, and are therefore not part of the dataset. We typically use \\(z\\) to denote such latent variables.\nThe marginal distribution over the observed variables \\(p_{\\theta}(x)\\), is given by: \\[\rp_{\\theta}(x) = \\int p_{\\theta}(x,z) dz = \\int p_{\\theta}(z) p_{\\theta}(x|z) dz\r\\] We use the term deep latent variable model (DLVM) to denote a latent variable model \\(p_{\\theta}(x,z)\\) whose distributions are parameterized by neural networks.\nExample DLVM for multivariate Bernoulli data\rA simple example DLVM for binary data \\(x\\), with a spherical Gaussian latent space, and a factorized Bernoulli obervation model \\[\rp(z) = \\mathcal{N}(0,\\text{I})\\\\\r\\text{p} = \\text{DecoderNeuralNet}_{\\theta}(z)\\\\\r\\begin{align}\r\\log p(x|z) =\u0026amp; \\sum_{j=1}^J \\log p(x_j|z) = \\sum_{j=1}^J \\text{Bernoulli}(x_j,p_j)\\\\\r=\u0026amp; \\sum_{j=1}^Jx_j \\log p_j + (1-x_j)\\log (1-p_j)\r\\end{align}\r\\] where \\(0\\leq p_j\\leq 1\\).\nTherefore, we easily get \\(p(x,z) = p(x|z)\\times p(z)\\) by the term we described above.\n\rSome problem\rNote that \\(p_{\\theta}(x,z)\\) is efficient to compute. Since the intractability of \\(p_{\\theta}(x)\\) (\\(p_{\\theta}(x) = \\int p_{\\theta}(x,z) dz\\)), the posterior distribution \\(p_{\\theta}(z|x)\\) is also intractable, because their densities are related through the basic identity: \\[p_{\\theta}(z|x) = \\frac{p_{\\theta}(x,z)}{p_{\\theta}(x)}\\]\nHow can we perform efficient approximate posterior inference and efficient approximate maximum likelihood estimation in deep latent variable models, in the presence of large datasets?\n\rSimilar method like DLVM\rWe introduce a parametric inference model \\(q_{\\phi}(z|x)\\) (also called as encoder)in this part and we try to optimize the variational parameters \\(\\phi\\) such that: \\[q_{\\phi}(z|x) \\approx p_{\\theta}(z|x)\\]\nSimilar to DLVM, the distribution of \\(q_{\\phi}(z|x)\\) also can be parameterized using deep neural networks. In this case, the variational parameters \\(\\phi\\) include the weights and biases of the neural network. For example: \\[\r(\\mu,\\sigma) = \\text{EncoderNeuralNet}_{\\phi}(x)\\\\\rq_{\\phi}(z|x) = \\mathcal{N}(\\mu,\\text{diag}(\\sigma^2))\r\\]\n\r\rEvidence lower bound (ELBO) and KL divergence\rThe optimization objective of the variational autoencoder is the evidence lower bound, abbreviated as ELBO. An alternative term for this objective is variational lower bound. We can obtain the lower bound by: \\[\r\\begin{align}\r\\log p_{\\theta}(x) =\u0026amp;\\ \\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x)] = \\mathbb{E}_{q_{\\phi}(z|x)} \\Big[\\log\\Big[ \\frac{p_{\\theta}(x,z)}{p_{\\theta}(z|x)}\\Big]\\Big]\\\\\r=\u0026amp;\\ \\mathbb{E}_{q_{\\phi}(z|x)}\\Big[\\log\\Big[\\frac{p_{\\theta}(x,z)}{q_{\\phi}(z|x)}\\frac{q_{\\phi}(z|x)}{p_{\\theta}(z|x)}\\Big]\\Big]\\\\\r=\u0026amp;\\ \\mathbb{E}_{q_{\\phi}(z|x)}\\Big[\\log\\Big[\\frac{p_{\\theta}(x,z)}{q_{\\phi}(z|x)}\\Big]\\Big] + \\mathbb{E}_{q_{\\phi}(z|x)}\\Big[\\log\\Big[\\frac{q_{\\phi}(z|x)}{p_{\\theta}(z|x)}\\Big]\\Big]\\\\\r=\u0026amp;\\ \\mathcal{L}_{\\theta,\\phi}(x) + KL[q_{\\phi}(z|x)||p_{\\theta}(z|x)]\\\\\r\\geq\u0026amp;\\ \\mathcal{L}_{\\theta,\\phi}(x)\r\\end{align}\r\\]\nKL divergence\rWe want to find a good probability distribution \\(q_{\\phi}(z|x)\\) (‘good’ means the efficient computation) to approximate the true posterior probability \\(p_{\\theta}(z|x)\\), where the \\(z\\) is the latent variable. KL divergence can measure the distance well between these two distribution. For the discrete probability situation, the KL divergence can be written as \\[KL(q||p) = \\sum q(x)\\log \\frac{q(x)}{p(x)}\\]\nExample of 1-dimension Guassian distribution\rSupposed that we have two random variables \\(x_1, x_2\\) w.r.t the guassian distribution \\(\\mathcal{N}(\\mu_1,\\sigma_1^2),\\mathcal{N}(\\mu_2,\\sigma_2^2)\\) respectively.\nRecall that the density function of guassian distribution \\[\r\\mathcal{N}(\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\r\\] Then \\[\r\\begin{align}\rKL(p_1,p_2) =\u0026amp;\\ \\int p_1(x)\\log \\frac{p_1(x)}{p_2(x)}dx\\\\\r=\u0026amp;\\ \\int p_1(x)(\\log p_1(x) - \\log p_2(x))dx\\\\\r=\u0026amp;\\ \\int p_1(x)(\\log \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}}e^{-\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}} - \\log \\frac{1}{\\sqrt{2\\pi\\sigma_2^2}}e^{-\\frac{(x-\\mu_2)^2}{2\\sigma_2^2}})dx\\\\\r=\u0026amp;\\ \\int p_1(x)(-\\log \\sqrt{2\\pi \\sigma_1^2} - \\frac{(x-\\mu_1)^2}{2\\sigma_1^2} + \\log \\sqrt{2\\pi \\sigma_2^2} + \\frac{(x-\\mu_2)^2}{2\\sigma_2^2})dx\\\\\r=\u0026amp;\\ \\int p_1(x)(-\\frac{1}{2}\\log2\\pi-\\log\\sigma_1+\\frac{1}{2}\\log2\\pi+\\log\\sigma_2 - (\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}-\\frac{(x-\\mu_2)^2}{2\\sigma_2^2}))dx\\\\\r=\u0026amp;\\ \\int p_1(x)(\\log\\frac{\\sigma_2}{\\sigma_1} - (\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}-\\frac{(x-\\mu_2)^2}{2\\sigma_2^2}))dx\\\\\r=\u0026amp;\\ \\int p_1(x)(\\log\\frac{\\sigma_2}{\\sigma_1})dx + \\int p_1(x)(\\frac{(x-\\mu_2)^2}{2\\sigma_2^2})dx - \\int p_1(x)(\\frac{(x-\\mu_1)^2}{2\\sigma_1^2})dx\\\\\r=\u0026amp;\\ \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{1}{2\\sigma_2^2}\\int p_1(x)(x-\\mu_2)^2dx - \\frac{1}{2\\sigma_1^2}\\int p_1(x)(x-\\mu_1)^2dx\r\\end{align}\r\\] Since \\(\\sigma^2 = \\int p_1(x)(x-\\mu_1)^2dx\\), then \\[\r\\begin{align}\rKL(p_1,p_2) =\u0026amp;\\ \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{1}{2\\sigma_2^2}\\int p_1(x)(x-\\mu_2)^2dx - \\frac{1}{2}\\\\\r=\u0026amp;\\ \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{1}{2\\sigma_2^2}\\int p_1(x)(x - \\mu_1 + \\mu_1 - \\mu_2)^2dx - \\frac{1}{2}\\\\\r=\u0026amp;\\ \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{1}{2\\sigma_2^2}[\\int p_1(x)(x-\\mu_1)^2dx+\\int p_1(x)(\\mu_1-\\mu_2)^2dx+2\\int p_1(x)(x-\\mu_1)(\\mu_1-\\mu_2)dx] - \\frac{1}{2}\\\\\r\\end{align}\r\\] We know that \\(\\mu_1 = \\int x p_1(x)dx\\), so \\(2\\int p_1(x)(x-\\mu_1)(\\mu_1-\\mu_2)dx = 2(\\mu_1-\\mu_2)[\\int xp_1(x)dx - \\mu_1] = 0\\), thus \\[\r\\begin{align}\rKL(p_1,p_2) =\u0026amp;\\ \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{1}{2\\sigma_2^2}[\\int p_1(x)(x-\\mu_1)^2dx + (\\mu_1-\\mu_2)^2] - \\frac{1}{2}\\\\\r=\u0026amp;\\ \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2+(\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\\\\\r\\end{align}\r\\] If we suppose that the \\(\\mathcal{N}(\\mu_2,\\sigma_2^2)\\) is standard guassian distribution, \\(\\mu_2 = 0, \\sigma_2^2 = 1\\), so \\[\r\\begin{align}\rKL =\u0026amp;\\ \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2+(\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\\\\\r=\u0026amp;\\ \\log1 - \\log\\sigma_1 + \\frac{\\sigma_1^2+(\\mu_1 - 0)^2}{2} - \\frac{1}{2}\\\\\r=\u0026amp;\\ -\\log\\sigma_1 + \\frac{\\sigma_1^2+\\mu_1^2}{2} - \\frac{1}{2}\\\\\r\\end{align}\r\\] We expect that the KL can be as small as possible, we calculate its derivative, then we get \\[\r\\frac{\\partial KL}{\\partial \\sigma_1} = -\\frac{1}{\\sigma_1} + \\sigma_1\\\\\r\\frac{\\partial KL}{\\partial \\mu_1} = \\mu_1\r\\] We let them equal to zero, then we get \\[\r-\\frac{1}{\\sigma_1} + \\sigma_1 = 0 \\Rightarrow \\sigma_1 = 1\\\\\r\\mu_1 = 0\r\\] which means that the KL becomes the minimum when \\(x_2 \\sim \\mathcal{N}(0,1)\\)\n\rMinimization of KL divergence\rIf we want to use the ELBO to approximate the log-likelihood, then we need to minimize the \\(D_{KL}[q_{\\phi}(z|x)||p_{\\theta}(z|x)]\\).\nFrom \\[\r\\begin{align}\rKL[q_{\\phi}(z|x)||p_{\\theta}(z|x)] =\u0026amp;\\ \\int q_{\\phi}(z|x) \\log \\frac{q_{\\phi}(z|x)}{p_{\\theta}(z|x)} dz\\\\\r=\u0026amp;\\ \\int q_{\\phi}(z|x) [\\log q_{\\phi}(z|x) - \\log p_{\\theta}(z|x)]dz\r\\end{align}\r\\] and Bayesian formula \\[\rp_{\\theta}(z|x) = \\frac{p_{\\theta}(x|z)*p_{\\theta}(z)}{p_{\\theta}(x)}\r\\] We can get \\[\r\\begin{align}\rKL[q_{\\phi}(z|x)||p_{\\theta}(z|x)] =\u0026amp;\\ \\int q_{\\phi}(z|x) [\\log q_{\\phi}(z|x) - \\log \\frac{p_{\\theta}(x|z)*p_{\\theta}(z)}{p_{\\theta}(x)}]dz\\\\\r=\u0026amp;\\ \\int q_{\\phi}(z|x) [\\log q_{\\phi}(z|x) -\\log p_{\\theta}(x|z) - \\log p_{\\theta}(z) + \\log p_{\\theta}(x)]dz\\\\\r=\u0026amp;\\ \\int q_{\\phi}(z|x) [\\log q_{\\phi}(z|x) -\\log p_{\\theta}(x|z) - \\log p_{\\theta}(z)]dz + \\log p_{\\theta}(x)\\\\\r=\u0026amp;\\ KL[q_{\\phi}(z|x)||p_{\\theta}(z)] - \\int q_{\\phi}(z|x) \\log p_{\\theta}(x|z)dz + \\log p_{\\theta}(x)\r\\end{align}\r\\]\nWhen the data \\(x\\) are provided, then last term in the right side \\(\\log p_{\\theta}(x)\\) becomes constant, and we wish the \\(D_{KL}[q_{\\phi}(z|x)||p_{\\theta}(z|x)]\\) can be as small as possible.\nThus, the optimization problem becomes\n\r\\(\\min\\limits_x D_{KL}[q_{\\phi}(z|x)||p_{\\theta}(z)]\\)\n\r\\(\\max\\limits_x \\int q_{\\phi}(z|x) \\log p_{\\theta}(x|z)dz\\)\n\r\rIt also can be written as \\[\\min_x KL[q_{\\phi}(z|x)||p_{\\theta}(z)] - \\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)]\\]\nAlthough we can obtain our new optimization problem, the problem actually is difficult to solve, and thus we would like to straightly optimize the ELBO.\n\r\r\rVariational Auto-Encoder\rConnection with EM\rFor standard EM algorithms, the posterior is often known, \\(q_{\\phi}(z|x) = q(z|x) = p_{\\theta}(z|x)\\), then the KL term becomes zero, so \\[\r\\begin{align}\r\\log p_{\\theta}(x) = \\mathcal{L}_{\\theta}(x) =\u0026amp;\\ \\mathbb{E}_{q(z|x)}\\Big[\\log\\Big[\\frac{p_{\\theta}(x,z)}{q(z|x)}\\Big]\\Big]\\\\\r=\u0026amp;\\ \\mathbb{E}_{q(z|x)}[\\log p_{\\theta}(x,z)] - \\mathbb{E}_{q(z|x)}[\\log q(z|x)]\r\\end{align}\r\\]\nThe above step is indeed the E-step in the standard EM algorithm. The M-step would be \\[\\theta_{\\text{new}} = \\arg \\max_\\theta L_{\\theta}(x)\\]\n\rStochastic gradient-based optimization of the ELBO\rFrom the Evidence lower bound (ELBO) part, we obtain the inequality fomula as \\(\\log p_{\\theta}(x) \\geq \\mathcal{L}_{\\theta,\\phi}(x)\\). Recall that EM algorithm is one of the special case of Minorize-Maximization (MM) algorithm, and \\(\\mathcal{L}_{\\theta,\\phi}(x)\\) can be considered as the surrogate function in MM algorithm, so we would get the maximum of log-likelihood by maximizing the lower bound.\n\rFigure1. The EM algorithm involves alternatel computing a lower bound on the log likelihood for the current parameter values and then maximizing this bound to obtain the new parameter values.\r\rGiven a dataset with i.i.d. data, the ELBO objective is the sum (or average) of individual-datapoint ELBO’s: \\[\r\\mathcal{L}_{\\theta,\\phi}(\\mathcal{D})=\\sum_{x\\in\\mathcal{D}}\\mathcal{L}_{\\theta,\\phi}(x)\r\\]\nApparantly, the individual-datapoint ELBO and its gradient \\(\\nabla_{\\theta,\\phi}\\mathcal{L}_{\\theta,\\phi}(x)\\) is intractable in general.\nThe SGVB estimator and Auto-Encoding VB (AEVB) algorithm\rReparamterization trick\nLet \\(z\\) be a continuous random variable and \\(z\\sim q_{\\phi}(z|x)\\) be some conditional distribution. It is often possible to express the random variable \\(z\\) as a deterministic variable \\(z=g_{\\phi}(\\epsilon,x)\\), where \\(\\epsilon\\) is an auxiliary variable with independent marginal \\(p(\\epsilon)\\).\nWe suppose that the recognition model \\(q_{\\phi}(z|x)\\) can be written as some differentiable transformation of another randome variable \\(\\epsilon\\), \\(g_{\\phi}(\\epsilon,x)\\), and we can form a simple Monte Carlo estimator \\(\\tilde{\\mathcal{L}}_{\\theta,\\phi}(x)\\) of the individual-datapoint ELBO: \\[\r\\epsilon \\sim p(\\epsilon)\r\\]\nso we can get our generic Stochastic Gradient Variational Bayes (SGVB) estimator from the lower bound \\[\r\\tilde{\\mathcal{L}}_{\\theta,\\phi}^{A}(x^{(i)}) = \\frac{1}{L}\\sum_{l=1}^L[\\log p_{\\theta}(x^{(i)},z^{(i,l)}) - \\log q_{\\phi}(z^{(i,l)}|x^{(i)})]\r\\] where \\(z^{(i,l)} = g_{\\phi}(\\epsilon^{(i,l)},x^{(i)}),\\quad \\epsilon^{(i,l)} \\sim p(\\epsilon)\\).\nWe try to decompose the \\(\\mathcal{L}_{\\theta,\\phi}(x)\\), and we get \\[\r\\begin{align}\r\\mathcal{L}_{\\theta,\\phi}(x^{(i)}) =\u0026amp;\\ \\mathbb{E}_{q_{\\phi}(z|x^{(i)})}\\Big[\\log\\frac{p_{\\theta}(x^{(i)},z)}{q_{\\phi}(z|x^{(i)})}\\Big]\\\\\r=\u0026amp;\\ \\mathbb{E}_{q_{\\phi}(z|x^{(i)})}\\Big[\\log\\frac{p_{\\theta}(x^{(i)}|z)p_{\\theta}(z)}{q_{\\phi}(z|x^{(i)})}\\Big]\\\\\r=\u0026amp;\\ \\mathbb{E}_{q_{\\phi}(z|x^{(i)})}\\Big[\\log p_{\\theta}(x^{(i)}|z)-\\log\\frac{q_{\\phi}(z|x^{(i)})}{p_{\\theta}(z)}\\Big]\\\\\r=\u0026amp;\\ \\mathbb{E}_{q_{\\phi}(z|x^{(i)})}[\\log p_{\\theta}(x^{(i)}|z)]-KL[q_{\\phi}(z|x^{(i)})||p_{\\theta}(z)]\r\\end{align}\r\\] The final equality showed the same object result (In Minimization of KL divergence section).\nWith this equality, we also can obtain another estimator \\[\r\\tilde{\\mathcal{L}}_{\\theta,\\phi}^{B}(x^{(i)}) = \\mathbb{E}_{q_{\\phi}(z|x^{(i)})}[\\log p_{\\theta}(x^{(i)}|z)]-KL[q_{\\phi}(z|x^{(i)})||p_{\\theta}(z)]\\\\\r=\\frac{1}{L}\\sum_{l=1}^L\\log p_{\\theta}(x^{(i)}|z^{(i,l)})-KL[q_{\\phi}(z|x^{(i)})||p_{\\theta}(z)]\r\\] where \\(z^{(i,l)} = g_{\\phi}(\\epsilon^{(i,l)},x^{(i)}),\\quad \\epsilon^{(i,l)} \\sim p(\\epsilon)\\). Given multiple datapoints from a dataset \\(\\text{X}\\) with \\(N\\) datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches: \\[\r\\mathcal{L}_{\\theta,\\phi}(\\text{X})\\simeq\\tilde{\\mathcal{L}}_{\\theta,\\phi}^{M}(\\text{X}^M)=\\frac{N}{M}\\sum_{i=1}^M\\tilde{\\mathcal{L}}_{\\theta,\\phi}(x^{(i)})\r\\] where the minibatch \\(\\text{X}^M=\\{x^{(i)}\\}_{i=1}^M\\) is randomly drawn sample of \\(M\\) datapoints from the full dataset \\(\\text{X}\\) with \\(N\\) datapoints. In the paper Auto-Encoding Variational Bayes, author set \\(M = 100, L = 1\\) in their experiments.\n\r\r\rVariational Auto-Encoder with specific case\rWe know that we can not perform the algorithm that we describe above, because we don’t know the distributions of \\(\\epsilon, p_{\\theta}(x|z), q_{\\phi}(z|x), p_{\\theta}(z)\\) and \\(g_{\\phi}(\\epsilon,x)\\). In reality, like author described in the paper, we firstly let the prior over the latent variables be the centered isotropic multivariate Guassian \\(p_{\\theta}(z) = \\mathcal{N}(0,\\text{I})\\).\nVariational approxiamte posterior \\(q_{\\phi}(z|x^{(i)})\\)\rLet the variational approxiamte posterior be a multivariate Guassian with a diagonal covariance structure: \\[\rq_{\\phi}(z|x^{(i)}) = \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})\r\\] where \\(\\mu^{(i)},\\sigma^{(i)}\\) denote the variational mean and s.d. evaluated by datapoint \\(i\\).\nThen a valid reparameterization is \\(z=\\mu+\\sigma\\epsilon\\), where \\(\\epsilon\\) is an auxiliary noise variable \\(\\epsilon\\sim \\mathcal{N}(0,\\text{I})\\).\nLet \\(J\\) be the dimensionality of \\(z\\) and \\(\\mu^{(i)}_j, \\sigma^{(i)}_j\\) denote the \\(j\\)-th element. Recall that \\[\r\\mathbb{E}[z] = \\int z p(z) dz\\\\\r\\mathbb{E}[z^2] = \\int z^2 p(z) dz\\\\\r\\text{Var}[z] = \\mathbb{E}[z^2] - \\mathbb{E}^2[z]\r\\] Then, \\[\r\\begin{align}\r\\int q_{\\phi}(z|x^{(i)})\\log p_{\\theta}(z)dz =\u0026amp;\\ \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})\\log \\mathcal{N}(0,\\text{I})dz\\\\\r=\u0026amp;\\ \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})(\\log \\frac{1}{\\sqrt{2\\pi}})dz - \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I}) \\frac{z^2}{2}dz\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi)\\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})dz - \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I}) \\frac{z^2}{2}dz\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\int z^2 \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})dz\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{j=1}^J\\mathbb{E}_{ q_{\\phi}(z_j|x^{(i)})}[z_j^2]\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{j=1}^J\\Big[\\mathbb{E}_{ q_{\\phi}(z_j|x^{(i)})}^2[z_j]+\\text{Var}_{ q_{\\phi}(z_j|x^{(i)})}[z_j]\\Big]\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{j=1}^J(\\mu_j^2+\\sigma_j^2)\r\\end{align}\r\\] and \\[\r\\begin{align}\r\\int q_{\\phi}(z|x^{(i)})\\log q_{\\phi}(z|x^{(i)})dz =\u0026amp;\\ \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})\\log \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})dz\\\\\r=\u0026amp;\\ \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})\\log \\Big[\\frac{1}{\\sqrt{2\\pi\\sigma^{(i)^2}}}\\exp(\\frac{-(z-\\mu^{(i)})^2}{2\\sigma^{(i)^2}})\\Big]dz\\\\\r=\u0026amp;\\ \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})\\Big[-\\frac{1}{2}\\log 2\\pi - \\frac{1}{2}\\log \\sigma^{(i)^2} - \\frac{(z-\\mu^{(i)})^2}{2\\sigma^{(i)^2}}\\Big]dz\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})\\Big[\\frac{\\log \\sigma^{(i)^2}}{2} - \\frac{(z-\\mu^{(i)})^2}{2\\sigma^{(i)^2}}\\Big]dz\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I}) \\Big[\\frac{1}{2}\\log \\sigma^{(i)^2} - \\frac{z^2-2\\mu^{(i)}z+\\mu^{(i)^2}}{2\\sigma^{(i)^2}}\\Big]dz\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{J=1}^J\\log \\sigma_j^{(i)^2} + \\int \\frac{1}{2\\sigma^{(i)^2}}\\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})(z^2-2\\mu^{(i)}z+\\mu^{(i)^2})dz\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{j=1}^J\\log \\sigma_j^{(i)^2} + \\frac{1}{2}\\sum_{j=1}^J\\frac{\\mu^{(i)^2}+\\sigma^{(i)^2}-2\\mu^{(i)^2}+\\mu^{(i)^2}}{\\sigma^{(i)^2}}\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{j=1}^J(1+\\log \\sigma_j^{(i)^2})\r\\end{align}\r\\] Therefore, \\[\r\\begin{align}\r-KL[q_{\\phi}(z|x^{(i)})||p_{\\theta}(z)] =\u0026amp; - \\int q_{\\phi}(z|x^{(i)})\\log \\frac{q_{\\phi}(z|x^{(i)})}{p_{\\theta}(z)}dz\\\\\r=\u0026amp;\\ - \\int q_{\\phi}(z|x^{(i)})[\\log q_{\\phi}(z|x^{(i)}) - \\log p_{\\theta}(z)]dz\\\\\r=\u0026amp;\\ \\int q_{\\phi}(z|x^{(i)})[\\log p_{\\theta}(z) - \\log q_{\\phi}(z|x^{(i)})]dz\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{j=1}^J(\\mu_j^2+\\sigma_j^2) - \\Big[-\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{j=1}^J(1+\\log \\sigma_j^{(i)^2})\\Big]\\\\\r=\u0026amp;\\ \\frac{1}{2}\\sum_{j=1}^J(1+\\log \\sigma_j^{(i)^2}-\\mu_j^2-\\sigma_j^2)\r\\end{align}\r\\]\n\rTrue posterior \\(p_{\\theta}(x|z)\\)\rWe supposed that the true posterior \\(p_{\\theta}(x|z)\\) be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution parameters are computed from \\(z\\) with a MLP (Multi-Layer Perceptron).\nBernoulli MLP as decoder\nIf the data are binary data, then we would choose \\[\r\\log p_{\\theta}(x|z) = \\sum_{j=1}^Dx_j \\log y_j + (1-x_j)\\log (1-y_j)\r\\] where \\(y = f_\\sigma(W_2\\tanh(W_1z+b_1)+b_2)\\), \\(f_\\sigma(\\cdot)\\) is the elementwise sigmoid activation function and \\(\\theta=\\{W_1,W_2,b_1,b_2\\}\\) are the weights and biases of the MLP.\nGaussian MLP as decoder\nLet decoder be a mutivariate Guassian with a diagonal covariance structure: \\[\r\\log p_{\\theta}(x|z) = \\log \\mathcal{N}(\\mu,\\sigma^2\\text{I})\r\\] where \\(\\mu = W_4h+b_4,\\ \\log\\sigma^2 = W_5h+b_5,\\ h = \\tanh(W_3Z+b_3)\\) and \\(\\{W_3,W_4,W_5,b_3,b_4,b_5\\}\\) are the weights and biases of the MLP and part of \\(\\theta\\).\nAnalysis in case of binary data\nRecall the second estimator we describe above \\[\r\\begin{align}\r\\mathcal{L}_{\\theta,\\phi}(\\text{X})\\simeq\u0026amp;\\ \\tilde{\\mathcal{L}}_{\\theta,\\phi}^{B}(x^{(i)})\\\\\r=\u0026amp;\\ \\frac{1}{L}\\sum_{l=1}^L\\log p_{\\theta}(x^{(i)}|z^{(i,l)}) - KL[q_{\\phi}(z|x^{(i)})||p_{\\theta}(z)]\\\\\r=\u0026amp;\\ \\frac{1}{L}\\sum_{l=1}^L\\log p_{\\theta}(x^{(i)}|z^{(i,l)}) + \\frac{1}{2}\\sum_{j=1}^J(1+\\log \\sigma_j^{(i)^2}-\\mu_j^2-\\sigma_j^2)\r\\end{align}\r\\] where \\(z^{(i,l)} = \\mu^{(i)} + \\sigma^{(i)}\\epsilon^{(l)}, \\epsilon^{l}\\sim p(\\epsilon)\\) and \\(\\log p_{\\theta}(x|z) = \\sum_{j=1}^Dx_j \\log y_j + (1-x_j)\\log (1-y_j)\\).\n\r\r\rUsing Variational Auto-Encoder in python\rImport packages\rimport numpy as np\rimport matplotlib.pyplot as plt\rimport tensorflow as tf\rfrom tensorflow.examples.tutorials.mnist import input_data\r\rFunction for visualizing batch images\rdef VisConcatImg(batch_images, title):\rbatch_size = np.shape(batch_images)[0]\rsqrt_size = int(batch_size ** 0.5)\rbatch_images = batch_images.reshape(batch_size, 28, 28)\rrow_concatenated = [np.concatenate(batch_images[i*sqrt_size : (i+1)*sqrt_size], axis=1) for i in range(sqrt_size)]\rconcatenated = np.concatenate(row_concatenated, axis=0)\rplt.imshow(concatenated, cmap=\u0026#39;gray\u0026#39;)\rplt.title(title)\rplt.axis(\u0026#39;off\u0026#39;)\rplt.show()\r\rMNIST Dataset\rThe MNIST includes 60000 training samples and 10000 testing samples. Each sample is a 784-dimensional vector (28??28), with pixel values in [0, 1], which can be assumed as multivariate Bernoulli variables.\n# Downloading MNIST dataset\rmnist = input_data.read_data_sets(\u0026#39;./mnist\u0026#39;, one_hot=False)\r# VAE for MNIST\rclass VAE(object):\rdef __init__(self, x_size=28*28, hidden1_size=100, hidden2_size=400, hidden3_size=100, hidden4_size=400, z_size=20, learning_rate=1e-4):\rself.x_size = x_size\rself.hidden1_size = hidden1_size\rself.hidden2_size = hidden2_size\rself.hidden3_size = hidden3_size\rself.hidden4_size = hidden4_size\rself.z_size = z_size\rself.learning_rate = learning_rate\rself.x = tf.placeholder(tf.float32, [None, x_size])\rself.epsilon = tf.placeholder(tf.float32, [None, z_size]) # sample from N(0,1) for every step\rwith tf.variable_scope(\u0026#39;encoder\u0026#39;):\rself.encoder()\rwith tf.variable_scope(\u0026#39;decoder\u0026#39;):\rself.decoder()\rwith tf.variable_scope(\u0026#39;loss\u0026#39;):\rself.compute_loss()\rwith tf.variable_scope(\u0026#39;train\u0026#39;):\rself.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.total_loss)\rdef encoder(self):\rself.hidden1 = tf.layers.dense(self.x, units=self.hidden1_size, activation=tf.nn.relu)\rself.hidden2 = tf.layers.dense(self.hidden1, units=self.hidden2_size, activation=tf.nn.relu)\rself.mu = tf.layers.dense(self.hidden2, units=self.z_size)\rself.sigma = tf.layers.dense(self.hidden2, units=self.z_size, activation=tf.exp)\rself.z = tf.add(self.mu, tf.multiply(self.epsilon, self.sigma))\rdef decoder(self):\rself.hidden3 = tf.layers.dense(self.z, units=self.hidden3_size, activation=tf.nn.relu)\rself.hidden4 = tf.layers.dense(self.hidden3, units=self.hidden4_size, activation=tf.nn.relu)\rself.y = tf.layers.dense(self.hidden4, units=self.x_size, activation=tf.nn.sigmoid)\r# adding 1e-8 before taking the logarithm to avoid numerical instability.\rdef compute_loss(self):\rself.recons_loss = tf.reduce_mean(tf.reduce_sum(-(self.x * tf.log(self.y + 1e-8) + (1 - self.x) * tf.log(1 - self.y + 1e-8)), 1))\rself.KL_loss = tf.reduce_mean(-0.5 * tf.reduce_sum(1 + 2 * tf.log(self.sigma + 1e-8) - tf.square(self.mu) - tf.square(self.sigma), 1))\rself.total_loss = self.recons_loss + self.KL_loss\r# Training VAE\rmodel = VAE()\rBATCH_SIZE = 100\rEPOCHS = 50\rSTEPS = int(60000 / BATCH_SIZE)\rsess = tf.Session()\rsess.run(tf.global_variables_initializer())\rfor e in range(EPOCHS):\rfor i in range(STEPS):\rtrain_data, _ = mnist.train.next_batch(batch_size=BATCH_SIZE)\rep = np.random.multivariate_normal(np.zeros(model.z_size), np.eye(model.z_size), size=BATCH_SIZE)\rsess.run(model.train_op, feed_dict={model.x: train_data, model.epsilon: ep})\rREloss, KLloss, Tloss = sess.run([model.recons_loss, model.KL_loss, model.total_loss], feed_dict={model.x: train_data, model.epsilon: ep})\rprint(\u0026#39;Epoch: \u0026#39;, e, \u0026#39;| reconstruction loss: \u0026#39;, REloss, \u0026#39;| KL loss:\u0026#39;, KLloss, \u0026#39;| total loss: \u0026#39;, Tloss)\r# Visualizing results\rtest_data, _ = mnist.test.next_batch(batch_size=50)\rVisConcatImg(test_data, \u0026#39;raw images\u0026#39;)\rep = np.random.multivariate_normal(np.zeros(model.z_size), np.eye(model.z_size), size=50)\rlatent, recons_x = sess.run([model.mu, model.y], feed_dict={model.x: test_data, model.epsilon: ep})\rVisConcatImg(recons_x, \u0026#39;reconstructed images\u0026#39;)\rrandoms = np.random.multivariate_normal(np.zeros(model.z_size), np.eye(model.z_size), size=50)\rgenerated_x = sess.run(model.y, feed_dict={model.z: randoms})\rVisConcatImg(generated_x, \u0026#39;generated images\u0026#39;)\rsess.close()\r\rResult\r\rFigure3. Raw Images\r\r\rFigure4. Reconstructed Images\r\r\rFigure5. Generated Images\r\r\r\rReference\rD. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2014. 5, 1\rYang Can. VAE_demo in python. 2018,12,31\r\r\r","date":1546214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546214400,"objectID":"364d7cb43b7ce5560d8689197bd08d27","permalink":"/post/variational-auto-encoder/","publishdate":"2018-12-31T00:00:00Z","relpermalink":"/post/variational-auto-encoder/","section":"post","summary":"Introduction\rWe assume the observed variable \\(x\\) is a random sample from an unknown underlying process, whose true distribution \\(p^*(x)\\) is unknown. We attempt to approximate this underlying process with a chosen model \\(p_{\\theta}(x)\\), with parameters \\(\\theta\\): \\[x\\sim p_{\\theta}(x)\\] We always talk about learning like Deep Learning, and actually the learning is the process of searching for a value of the parameters \\(\\theta\\) in model \\(p_{\\theta}(x)\\), which can approximate the true distribution of the data, denoted by \\(p^*(x)\\).","tags":["TensorFlow","VAE"],"title":"Variational Auto-Encoder","type":"post"},{"authors":null,"categories":["Deep Learning"],"content":" 迈出TensorFlow世界的第一步 安装TensorFlow 鉴于python3更高级（传闻python2最终会被淘汰，所以希望选择能陪伴更久的工具:)），本文中会以python3为基准，屏幕前的读者你！也建议你跟我用上python3！另外，如果你刚起步使用python的话，那建议你直接下载 Anaconda，快捷高效，下载引导详见图1。Anaconda会帮助我们省去下载很多库的时间，把时间留给TensorFlow吧！\n 图1. 下载Anaconda  设置水土不服的Anaconda 首先，在安装Anaconda后，先打开Anaconda Prompt（一般在你的应用目录可以找到），打开后是一个跟CMD一样黑乎乎的界面 ╮(๑•́ ₃•̀๑)╭ 打开后呢~通过conda --version看看是否成功安装了Anaconda。一般会得到版本信息，如下所示\nconda 4.5.12 那么，Anaconda安装成功！\n下一步我们设置下Anaconda的仓库(Repository)镜像，因为默认连接的是境外镜像地址，会超慢（我记得我当时只有10kb的网速T_T），我们把镜像地址改为境内的清华大学开源软件镜像站，所以通过下面指令就可以提高你的下载速度(´•灬•‘)。\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --set show_channel_urls yes  创建python3.5环境 首先，由于目前TensoFlow官方Only支持python3.5版本，而且在文章写的这个时候，Anaconda官方最新版本中的python是3.7版本，所以我们需要创建一个python3.5的新环境。\n首先在系统菜单栏找到并点击Anaconda Navigator，然后选择Enviroments（如图2所示），然后点击Create创建新环境：\n 图2. Environments界面  我们命名为tensorflow，并选择python3.5的版本：\n 图3. 创建新环境窗口  安装成功后，Environments界面多了一个我们创建的命名为tensorflow的python3.5的环境，并自动预先安装一些基础的库。\n 图4. 安装成功后的python3.5环境  搞定python3.5的环境后，我们顺便在python3.5环境下安装常用的jupyter notebook和spyder。先选择Home界面，然后可以看到jupyter notebook和spyder下方均显示Install，当然就点击Install，就等着Anaconda Navigator帮我们搞定啦！安装成功后，它们下方会变成Launch（如图5所示）。\n 图5. 安装jupyter和spyder成功后的Home界面  如果要在Anaconda prompt界面启动python3.5环境，很简单，一行命令！\nconda activate tensorflow 这里的tensorflow其实是我们的python3.5环境的命名~\n 通过pip安装我们的主角TensorFlow ٩(๑\u0026gt; ₃ \u0026lt;)۶з 搞定一切基础的部分后，接下来就开始用pip安装我们TensorFlow的CPU版本了~我们先激活python3.5的环境并用pip安装tensorflow\nconda activate python35 pip install tensorflow 旋转跳跃~闭着眼~睁开眼后就搞定了你要的TensorFlow (●´▽｀●) 下面我们先测试一下，在激活python3.5之后，输入python运行python，然后输入下面的命令。\nimport tensorflow as tf hello = tf.constant(\u0026#39;Hello, TensorFlow!\u0026#39;) sess = tf.Session() print(sess.run(hello)) a = tf.constant(1) b = tf.constant(2) c = sess.run(a+b) print(\u0026quot;1 + 2 = %d\u0026quot; % c) 如果输出了\n‘Hello, TensorFlow!’\n1 + 2 = 3\n那么恭喜你，TensorFlow安装成功！！\nAttention!\n CPU有个利器(๑•̀_•́๑)\n当我使用sess = tf.Session()的时候，对话框告诉我：\n2018-12-26 15:03:57.708274: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n然后一脸懵，杰是个啥？\n感恩必应搜索，感恩维基百科！\n 高级矢量扩展（AVX）是英特尔在2008年3月提出的英特尔和AMD微处理器的x86指令集体系结构的扩展，英特尔首先通过Sandy Bridge处理器在2011年第一季度推出，随后由AMD推出Bulldozer处理器在2011年第三季度.AVX提供了新功能，新指令和新编码方案。 特别是，AVX引入了融合乘法累加（FMA）操作，加速了线性代数计算，即点积，矩阵乘法，卷积等。几乎所有机器学习训练都涉及大量这些操作，因此将会支持AVX和FMA的CPU（最高达300％）更快。\n 而对话框想告诉我，我的CPU支持AVX，让我赶紧用上它！\nimport tensorflow as tf import os os.environ[\u0026#39;TF_CPP_MIN_LOG_LEVEL\u0026#39;] = \u0026#39;2\u0026#39; sess = tf.Session() 这样就不会出现警告了~\n 目前觉得不重要的part(๑•̀_•́๑)\n查看Windows系统下机器的GPU信息\n首先打开“运行”对话框并在“运行”对话框中输入“dxdiag”（如图6），此时会打开“DirextX诊断工具”窗口，再通过选择“显示”标签便可查到机器的GPU信息（如图7）。\n   图6. 输入dxdiag命令   图7. 查看机器GPU信息  简单提及安装GPU版本TensorFlow\n如果你觉得运行的速度不满足你的需求，那么你可以选择用上GPU版本的TensorFlow，那将帮助你火箭般的速度运行！下面简单带过如何安装TensorFlow的GPU版本~由于具体操作复杂，暂且跳过~\npip install tensorflow-gpu    ","date":1545782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545782400,"objectID":"0194b36e492a9de3af5ffa26d2f117a7","permalink":"/post/%E5%B0%8F%E8%8F%9C%E9%B8%9F%E7%9A%84%E5%85%A5%E9%97%A8tensorflow/","publishdate":"2018-12-26T00:00:00Z","relpermalink":"/post/%E5%B0%8F%E8%8F%9C%E9%B8%9F%E7%9A%84%E5%85%A5%E9%97%A8tensorflow/","section":"post","summary":"迈出TensorFlow世界的第一步 安装TensorFlow 鉴于python3更高级（传闻python2最终会被淘汰，所以希望选择能陪伴更久的工具:)），本文中会以python3为基准，屏幕前的读者你！也建议你跟我用上python3！另外，如果你刚起步使用python的话，那建议你直接下载 Anaconda，快捷高效，下载引导详见图1。Anaconda会帮助我们省去下载很多库的时间，把时间留给TensorFlow吧！\n 图1. 下载Anaconda  设置水土不服的Anaconda 首先，在安装Anaconda后，先打开Anaconda Prompt（一般在你的应用目录可以找到），打开后是一个跟CMD一样黑乎乎的界面 ╮(๑•́ ₃•̀๑)╭ 打开后呢~通过conda --version看看是否成功安装了Anaconda。一般会得到版本信息，如下所示\nconda 4.5.12 那么，Anaconda安装成功！\n下一步我们设置下Anaconda的仓库(Repository)镜像，因为默认连接的是境外镜像地址，会超慢（我记得我当时只有10kb的网速T_T），我们把镜像地址改为境内的清华大学开源软件镜像站，所以通过下面指令就可以提高你的下载速度(´•灬•‘)。\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --set show_channel_urls yes  创建python3.5环境 首先，由于目前TensoFlow官方Only支持python3.5版本，而且在文章写的这个时候，Anaconda官方最新版本中的python是3.7版本，所以我们需要创建一个python3.5的新环境。\n首先在系统菜单栏找到并点击Anaconda Navigator，然后选择Enviroments（如图2所示），然后点击Create创建新环境：\n 图2. Environments界面  我们命名为tensorflow，并选择python3.5的版本：\n 图3. 创建新环境窗口  安装成功后，Environments界面多了一个我们创建的命名为tensorflow的python3.5的环境，并自动预先安装一些基础的库。\n 图4. 安装成功后的python3.5环境  搞定python3.5的环境后，我们顺便在python3.5环境下安装常用的jupyter notebook和spyder。先选择Home界面，然后可以看到jupyter notebook和spyder下方均显示Install，当然就点击Install，就等着Anaconda Navigator帮我们搞定啦！安装成功后，它们下方会变成Launch（如图5所示）。\n 图5. 安装jupyter和spyder成功后的Home界面  如果要在Anaconda prompt界面启动python3.5环境，很简单，一行命令！\nconda activate tensorflow 这里的tensorflow其实是我们的python3.5环境的命名~\n 通过pip安装我们的主角TensorFlow ٩(๑\u0026gt; ₃ \u0026lt;)۶з 搞定一切基础的部分后，接下来就开始用pip安装我们TensorFlow的CPU版本了~我们先激活python3.5的环境并用pip安装tensorflow\nconda activate python35 pip install tensorflow 旋转跳跃~闭着眼~睁开眼后就搞定了你要的TensorFlow (●´▽｀●) 下面我们先测试一下，在激活python3.","tags":["TensorFlow"],"title":"小菜鸟的入门TensorFlow","type":"post"},{"authors":null,"categories":["Deep Learning"],"content":"\rFrom some on-line article, it is interesting that we can consider EM algorithm as some Chinese Kungfu manuals and it contains 9 levels of perspectives. With such metaphor, I can feel how this method powerful it is. Now, I want to share my view with you.\nIntroduction to the EM algorithm\rExpectation-Maximization (EM) algorithm is an important method to solve the maximum likelihood problem with latent variables in statistics. It is widely used in Machine Learning because it can simplify many difficult problems. One of the famous and classical application is gaussian mixture model.\nBasic probability theory\rLet \\(p(x|\\theta)\\) be the probability density function of random variable \\(x\\), where \\(\\theta\\) is the parameter of the density function, then we know that the basic probability property is \\[\rp(\\text{x};\\theta) \\geq 0, \\int_{-\\infty}^{+\\infty}p(\\text{x};\\theta) d\\text{x} = 1.\r\\]\nIf we take the expectation of x, we get \\[\r\\mathbb{E}[\\text{x}] = \\int\\text{x}p(\\text{x};\\theta) d\\text{x}\r\\] In the integral, we know that the \\(\\mathbb{E}[\\text{x}]\\) involves \\(\\theta\\) but not \\(\\text{x}\\).\nIf we generalize it to function and let \\(f(\\text{x})\\) be a function of \\(\\text{x}\\). Similarly, the expectation of \\(f(x)\\) is given by \\[\r\\mathbb{E}[f] = \\int f(\\text{x})p(\\text{x};\\theta) d\\text{x}\r\\]\nWith the similar result, we know that the \\(\\mathbb{E}[f]\\) involves \\(\\theta\\) but not \\(\\text{x}\\).\n\rMotivation of the EM algorithm\rAt the beginning, we denote that\n\r\\(\\text{X}\\): Set of all observed data (incomplete-data)\r\\(\\text{Z}\\): Set of all latent variables\r\\(\\theta\\): Set of all model parameters\r\\(\\{ \\text{X,Z} \\}\\): Each observation in \\(\\text{X}\\) is corresponding value of the latent variable \\(\\text{Z}\\) (complete-data)\r\rThen the log-likelihood function can be written as \\[\rL(\\text{X};\\theta) = \\ln p(\\text{X};\\theta) = \\ln \\{ \\sum_{\\text{Z}}p(\\text{X}, \\text{Z};\\theta) \\}\r\\]\nIt is too hard to straightly solve the problem with \\(\\ln\\) and \\(\\sum\\). The likelihood function for the complete data set simply takes the form \\(\\ln p(\\text{X,Z}|\\theta)\\), and we shall suppose that maximization of this complete-data log-likelihood function is straightforward.\nIn practice, we are not given the latent variable \\(\\text{Z}\\) but we know the posterior distribution \\(p(\\text{Z}|\\text{X};\\theta)\\).\n\rThe Level 1 of the EM algorithm\rIn the level 1, we just need to know the basic knowledge of EM algorithm.\nIn the \\(\\mathbb{E}\\)-step, we use the current parameter values \\(\\theta_{\\text{old}}\\) to find the posterior distribution of the latent variables given by \\(p(\\text{Z}|\\text{X};\\theta_{\\text{old}})\\).\nThen, we would use this posterior distribution to find the expectation of the complete-data log-likelihood evaluated for some general parameter value \\(\\theta\\). This expectation, denoted \\(\\mathcal{Q}(\\theta,\\theta_{\\text{old}})\\), is given by \\[\r\\mathcal{Q}(\\theta,\\theta_{\\text{old}}) = \\mathbb{E}_{\\text{Z}|\\text{X};\\theta_{\\text{old}}}[\\ln p(\\text{X,Z};\\theta)] = \\sum_{\\text{Z}} \\ln p(\\text{X,Z};\\theta)p(\\text{Z}|\\text{X};\\theta_{\\text{old}})\r\\]\nIn the \\(\\mathbb{M}\\) (Maximization) step, we determine the revised parameter estimate \\(\\theta_{\\text{new}}\\) by maximizing the function \\[\r\\theta_{\\text{new}} = \\arg \\max_\\limits{\\theta} \\mathcal{Q}(\\theta,\\theta_{\\text{old}})\r\\]\nSince the complete-data log-likelihood involves unobversed data \\(\\text{Z}\\), we use Expectation to eliminate the uncertainty, and the function \\(\\mathbb{E}_{\\text{Z}|\\text{X},\\theta_{\\text{old}}}[\\ln p(\\text{X,Z}|\\theta)]\\) does not involve \\(\\text{Z}\\) but involve \\((\\theta,\\theta_{\\text{old}})\\).\nWe can summarize the procedure as:\n\rWhile \\(\\theta_{\\text{new}} - \\theta_{old} \u0026gt; \\epsilon\\)\n\\(\\quad\\) Expectation-Step on log likelihood function: \\[\r\\mathcal{Q}(\\theta,\\theta_{\\text{old}}) = \\mathbb{E}_{\\text{Z}|\\text{X};\\theta_{\\text{old}}}[\\ln p(\\text{X,Z};\\theta)]= \\sum_{\\text{Z}} \\ln p(\\text{X,Z};\\theta)p(\\text{Z}|\\text{X};\\theta_{\\text{old}})\r\\] \\(\\quad\\) Maximization-Step on \\(\\mathcal{Q}(\\theta,\\theta_{\\text{old}})\\): \\[\r\\theta_{\\text{new}} = \\arg \\max_\\limits{\\theta} \\mathcal{Q}(\\theta,\\theta_{\\text{old}})\r\\]\n\r\rThe Level 2 of the EM algorithm\rAfter writting down the pseudocode of EM algorithm, we want to know the reason that we can use expectation to approximate the maximum log-likelihood by repeating Expectation and Maximization. In other words, we want to prove that \\[\r\\arg \\max_\\theta \\mathbb{E}_{\\text{Z}|\\text{X};\\theta_{\\text{old}}}[\\ln p(\\text{X};\\theta)] \\approx \\arg \\max_\\theta \\ln p(\\text{X};\\theta)\r\\] where the joint distribution \\(p(\\text{X}, \\text{Z};\\theta)\\) is governed by a set of parameters \\(\\theta\\).\nNext we introduce a distribution \\(q(\\text{Z})\\) defined over the latent variables.\nSince \\[\rp(\\text{X},\\text{Z};\\theta) = p(\\text{Z}|\\text{X};\\theta)p(\\text{X};\\theta)\r\\] and \\[\r\\sum_\\text{Z}q(\\text{Z}) = 1\r\\] we can get decomposition by \\[\r\\begin{align}\r\\ln p(\\text{X};\\theta) =\u0026amp;\\ \\ln \\frac{p(\\text{X},\\text{Z};\\theta)}{p(\\text{Z}|\\text{X};\\theta)}\\\\\r=\u0026amp;\\ \\ln p(\\text{X},\\text{Z};\\theta) - \\ln p(\\text{Z}|\\text{X};\\theta) + \\ln q(\\text{Z}) - \\ln q(\\text{Z})\\\\\r=\u0026amp;\\ \\ln \\frac{p(\\text{X},\\text{Z};\\theta)}{q(\\text{Z})} - \\ln \\frac{p(\\text{Z}|\\text{X};\\theta)}{q(\\text{Z})}\\\\\r=\u0026amp; \\sum_\\text{Z}q(\\text{Z}) \\{ \\ln \\frac{p(\\text{X},\\text{Z};\\theta)}{q(\\text{Z})} - \\ln \\frac{p(\\text{Z}|\\text{X};\\theta)}{q(\\text{Z})} \\}\\\\\r=\u0026amp;\\ \\sum_\\text{Z}q(\\text{Z}) \\ln \\frac{p(\\text{X},\\text{Z};\\theta)}{q(\\text{Z})} - \\sum_\\text{Z}q(\\text{Z}) \\ln \\frac{p(\\text{Z}|\\text{X};\\theta)}{q(\\text{Z})}\\\\\r=\u0026amp;\\ \\mathcal{L}(q,\\theta) + KL(q||p)\r\\end{align}\r\\] where \\[\r\\begin{align}\r\\mathcal{L}(q,\\theta) = \\sum_\\text{Z}q(\\text{Z}) \\ln \\frac{p(\\text{X},\\text{Z};\\theta)}{q(\\text{Z})}\\\\\rKL(q||p) = - \\sum_\\text{Z}q(\\text{Z}) \\ln \\frac{p(\\text{Z}|\\text{X};\\theta)}{q(\\text{Z})}\r\\end{align}\r\\]\nWe call the \\(KL(q||p)\\) as the Kullback-Leibler divergence (KL divergence, also known as relative entropy).\nRecall that Jensen’s inequality holds for convex function \\(f(x)\\). \\[\r\\mathbb{E}[f(x)] \\geq f(\\mathbb{E}[x])\r\\]\nApplying Jensen’s inequality in KL divergence, we have \\[\r\\begin{align}\rKL(q||p) =\u0026amp; - \\sum_\\text{Z}q(\\text{Z}) \\ln \\frac{p(\\text{Z}|\\text{X};\\theta)}{q(\\text{Z})} = -\\mathbb{E}_q[\\ln\\{\\frac{p(\\text{Z}|\\text{X};\\theta)}{q(\\text{Z})}\\} ]\\\\\r\\geq\u0026amp; -\\ln \\mathbb{\\text{E}_q}[\\frac{p(\\text{Z}|\\text{X};\\theta)}{q(\\text{Z})}] = -\\ln \\sum_\\text{Z} q(\\text{Z}) \\frac{p(\\text{Z}|\\text{X};\\theta)}{q(\\text{Z})}\\\\\r=\u0026amp; -\\ln \\sum_\\text{Z} p(\\text{Z}|\\text{X};\\theta) = -\\ln 1\\\\ =\u0026amp;\\ 0\r\\end{align}\r\\]\nIf we let \\(q(\\text{Z}) = p(\\text{Z}|\\text{X};\\theta)\\), and \\(p(\\text{X},\\text{Z};\\theta) = p(\\text{Z}|\\text{X};\\theta)p(\\text{X};\\theta)\\), then \\[\r\\begin{align}\r\\mathcal{L}(q,\\theta) =\u0026amp; \\sum_\\text{Z}q(\\text{Z}) \\ln \\frac{p(\\text{X},\\text{Z};\\theta)}{q(\\text{Z})} = \\mathbb{E}_q[\\ln \\frac{p(\\text{X},\\text{Z};\\theta)}{q(\\text{Z})}]\\\\\r=\u0026amp;\\ \\mathbb{E}_{\\text{Z}|\\text{X};\\theta}[\\ln p(X;\\theta)]\r\\end{align}\r\\]\nThus, we can get the result that \\(\\ln p(\\text{X};\\theta) \\geq \\mathcal{L}(q,\\theta) = \\mathbb{E}_{\\text{Z}|\\text{X};\\theta}[\\ln p(X;\\theta)]\\), so we can say that \\(\\mathcal{L}(q,\\theta) = \\mathbb{E}_{\\text{Z}|\\text{X};\\theta}[\\ln p(X;\\theta)]\\) is the low bound of \\(\\ln p(\\text{X};\\theta)\\).\n\rFigure1. The EM algorithm involves alternatel computing a lower bound on the log likelihood for the current parameter values and then maximizing this bound to obtain the new parameter values.\r\rActually, EM algorithm is one of the special case of Minorize-Maximization (MM) algorithm, and \\(\\mathcal{L}(q,\\theta)\\) can be considered as the surrogate function in MM algorithm.\n\r\r","date":1545609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545609600,"objectID":"8ffb9edea8bf95f2a21b2f21e43bb7d1","permalink":"/post/expectation-maximization-algorithm/","publishdate":"2018-12-24T00:00:00Z","relpermalink":"/post/expectation-maximization-algorithm/","section":"post","summary":"From some on-line article, it is interesting that we can consider EM algorithm as some Chinese Kungfu manuals and it contains 9 levels of perspectives. With such metaphor, I can feel how this method powerful it is. Now, I want to share my view with you.\nIntroduction to the EM algorithm\rExpectation-Maximization (EM) algorithm is an important method to solve the maximum likelihood problem with latent variables in statistics. It is widely used in Machine Learning because it can simplify many difficult problems.","tags":["Algorithm"],"title":"Expectation-Maximization Algorithm","type":"post"},{"authors":null,"categories":["Machine Learning"],"content":"\rCoordinate Descent Framework\rAt the begining of this section, we start to discuss three different types of function.\nGiven convex, differentiable function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\), we know \\(f(x+\\delta \\cdot e_i)\\geq f(x)\\) for all \\(\\delta\\) because \\(\\nabla f(x) = (\\frac{\\partial f}{\\partial x_1}(x),\\dots,\\frac{\\partial f}{\\partial x_n}(x)) = 0\\). Here, \\(e_i = (0,\\dots,1,\\dots,0) \\in \\mathbb{R}^n\\), the \\(i\\)th standard basis vactor.\r\r\rFigure1. Convex and differential function \\(f\\)\r\rGiven convex but not differentiable function \\(f\\), we can not found a global minimizer.\r\r\rFigure2. Convex but not differential function \\(f\\)\r\rGiven convex \\(g\\) and each convex but not differentiable \\(h_i\\), so we get \\(f(x)=g(x)+\\sum_{i=1}^n h_i(x_i)\\). In this function, the non-smooth part is called as separable.\nFor any \\(y\\), we get\r\\[\\begin{align}\rf(y) - f(x) \\geq\u0026amp; \\nabla g(x)^T (y-x) + \\sum_{i=1}^n [h_i(y_i)-h_i(x_i)]\\\\\r=\u0026amp; \\sum\\limits_{i=1}^n [\\nabla_ig(x)(y_i-x_i)+h_i(y_i)-h_i(x_i)] \\geq 0\r\\end{align}\\]\rThus, we can get global minimizer.\r\r\rFigure3. Convex, not differential but separable function \\(f\\)\r\rIf we get a function with the formula like \\(f(x) = g(x) + \\sum_{i=1}^n h_i(x_i)\\), where the \\(g\\) is convex and differentiable function, each \\(h_i\\) is convex functions, then we can use coordinate descent to find global minimizer. The procedure is following: start with some initial guess \\(x^{(0)}\\), and repeat\r\\[\\begin{align}\rx_1^{(k)} \\in\u0026amp; \\mathop{\\arg\\min}_{x_1} f(x_1,x_2^{(k-1)},x_3^{(k-1)},\\dots,x_n^{(k-1)})\\\\\rx_2^{(k)} \\in\u0026amp; \\mathop{\\arg\\min}_{x_2} f(x_1^{(k)},x_2,x_3^{(k-1)},\\dots,x_n^{(k-1)})\\\\\rx_3^{(k)} \\in\u0026amp; \\mathop{\\arg\\min}_{x_3} f(x_1^{(k)},x_2^{(k)},x_3,\\dots,x_n^{(k-1)})\\\\\r\\cdots\u0026amp; \\\\\rx_n^{(k)} \\in\u0026amp; \\mathop{\\arg\\min}_{x_n} f(x_1^{(k)},x_2^{(k)},x_3^{(k)},\\dots,x_n)\\\\\r\\end{align}\\]\rfor \\(k=1,2,3,\\dots,K\\)\nNote: after we solve for \\(x_i^{(k)}\\), we use its new value from then on.\n\rCoordinate descent for linear regression with convex penalties\r\r","date":1545264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545264000,"objectID":"a7b929a37f776a7ca16cc0156eb8ea6e","permalink":"/post/coordinate-descent-algorithm/","publishdate":"2018-12-20T00:00:00Z","relpermalink":"/post/coordinate-descent-algorithm/","section":"post","summary":"Coordinate Descent Framework\rAt the begining of this section, we start to discuss three different types of function.\nGiven convex, differentiable function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\), we know \\(f(x+\\delta \\cdot e_i)\\geq f(x)\\) for all \\(\\delta\\) because \\(\\nabla f(x) = (\\frac{\\partial f}{\\partial x_1}(x),\\dots,\\frac{\\partial f}{\\partial x_n}(x)) = 0\\). Here, \\(e_i = (0,\\dots,1,\\dots,0) \\in \\mathbb{R}^n\\), the \\(i\\)th standard basis vactor.\r\r\rFigure1. Convex and differential function \\(f\\)\r\rGiven convex but not differentiable function \\(f\\), we can not found a global minimizer.","tags":["R","Algorithm"],"title":"Coordinate Descent Algorithm","type":"post"}]