[{"authors":null,"categories":["Deep Learning"],"content":" 神经元及神经网络基础结构  图1. 神经元的组成（源自维基百科）  神经元这个图大多数理科生在高中生物课本都学过~神经网络则由许许多多的神经元所组成，通常一个神经元具有多个树突，主要用来接收消息；轴突只有一条，相当于我们定义的一个计算过程；而轴突尾部的许许多多轴突末梢，将传递信息给其他神经元。\n 图2. 神经网络基础结构  通常这里的非线性函数会用上各式各样的激活函数，比如Sigmoid函数，tanh函数和ReLu函数。\nSigmoid函数\n\\[f(z) = \\frac{1}{1+e^{-z}}\\] tanh函数\n\\[f(z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}\\] ReLu函数\n\\[f(z) = \\max(0,z)\\]\n 神经网络基础认知 我们把许多神经元组合起来就可以得到一个神经网络，由于有输入的数据和我们想得到的输出数据，便会有“输入层”（Input layer）和“输出层”（Output layer）；中间的神经元则组成了“隐藏层”（Hidden layer）。在下面图3中，输入层有3个神经元，隐藏层有4个神经元，输出层有2个神经元。在实际情况中，输入层和输出层通常是固定的，而隐藏层的层数和节点数则可以自由调节。  图3. 神经网络基础层级结构  我们假设一个全连接的网络结构，其中隐藏层只有一层。另外，假设输入层和隐藏层之间的边的权值构成的矩阵为 \\[ \\left [ \\begin{matrix} w_{11} \u0026amp; w_{12} \u0026amp; w_{13} \\\\ w_{21} \u0026amp; w_{22} \u0026amp; w_{23} \\\\ w_{31} \u0026amp; w_{32} \u0026amp; w_{33} \\end{matrix} \\right ] \\] 其中，第一列的\\(w_{11}, w_{21}, w_{31}\\)代表的是输入层的点\\(x_1\\)分别连接隐藏层的三个节点的边的权值；第二列的\\(w_{12}, w_{22}, w_{32}\\)代表的是输入层的点\\(x_2\\)分别连接隐藏层的三个节点的边的权值；第三列的\\(w_{13}, w_{23}, w_{33}\\)代表的是输入层的点\\(x_3\\)分别连接隐藏层的三个节点的边的权值。\n图中的“+1”点代表我们添加了一个值b，称其为偏置项。那么，隐藏层的节点可以由下计算得到： \\[ \\begin{align} a_1 = w_{11}\\times x_1 + w_{12}\\times x_2 + w_{13}\\times x_3 + b_1\\\\ a_2 = w_{21}\\times x_1 + w_{22}\\times x_2 + w_{23}\\times x_3 + b_2\\\\ a_3 = w_{31}\\times x_1 + w_{32}\\times x_2 + w_{33}\\times x_3 + b_3 \\end{align} \\tag{1} \\] 由于线性计算的表现能力比较差，所以考虑用非线性函数进行计算，即使用激活函数\\(f(\\cdot)\\)（前面已提及）。（1）式可以变换为（2）式： \\[ \\begin{align} a_1 = f(w_{11}\\times x_1 + w_{12}\\times x_2 + w_{13}\\times x_3 + b_1)\\\\ a_2 = f(w_{21}\\times x_1 + w_{22}\\times x_2 + w_{23}\\times x_3 + b_2)\\\\ a_3 = f(w_{31}\\times x_1 + w_{32}\\times x_2 + w_{33}\\times x_3 + b_3) \\end{align} \\tag{2} \\] 将（2）式改写为矩阵运算形式（3）式： \\[ \\begin{align} \\boldsymbol{a} = f \\begin{pmatrix} \\begin{pmatrix} w_{11},w_{12},w_{13}\\\\w_{21},w_{22},w_{23}\\\\w_{31},w_{32},w_{33} \\end{pmatrix} \\begin{pmatrix} x_1\\\\x_2\\\\x_3 \\end{pmatrix} + \\begin{pmatrix} b_1\\\\b_2\\\\b_3 \\end{pmatrix} \\end{pmatrix} = f(\\boldsymbol{W}x+\\boldsymbol{B}) \\end{align} \\tag{3} \\]\n 图4. 简单全连接网络中层之间的计算方式  当我们增加隐藏层的层数，便可以构成更复杂的网络，即深度神经网络。  图5. 深度神经网络示意图   损失函数（Loss Function） 训练数据通常是一系列“输入-输出”数据对组成的集合，我们希望输入一个数据，尽可能与配对的输出数据相同。那么网络的输出结果和实际的真实结果差多少，我们需要一定数学形式进行量化，所以引入了损失函数（Loss Function）。常见的损失函数有以下几种：\n0-1损失函数\n如果预测值和真实值一样，则损失值为0；若不等，则为1；公式表达为： \\[ L(y,f(x)) = \\begin{cases} 1, \u0026amp; y = f(x)\\\\ 0, \u0026amp; y \\neq f(x) \\end{cases} \\]\n绝对值损失函数（1-范数形式）\n通过预测值和真实值之差的绝对值进行衡量，公式表达为： \\[ L(y,f(x)) = |y-f(x)| \\]\n均方误差损失函数（2-范数形式）\n通过计算预测值和真实值之差的平方再求均值，可得到均方误差，公式表达为： \\[ L(y,f(x)) = \\frac{1}{n}\\sum_{i=1}^n(y_i-f(x_i))^2 \\]\n 优化算法 梯度下降法（Gradient Descent Method） 传统的梯度下降法是通过计算损失函数的一阶导数作为方向进行下降计算，计算方法可表示为： \\[ W_{ij} = W_{ij} - \\alpha\\frac{\\partial}{\\partial W_{ij}}L(w,b)\\\\ b_i = b_i - \\alpha\\frac{\\partial}{\\partial b_i}L(w,b) \\] 其中\\(W_{ij}\\)和\\(b_i\\)是需要优化的参数，\\(L(w,b)\\)是损失函数，\\(\\alpha\\)在深度学习中通常称为学习率（learning rate），在机器学习或最优化计算领域中我们通常称为步长（stepsize）。\n传统的梯度下降法需要计算\\(n\\)个梯度，即样本数量的梯度个数，在数据越来越大的时代，这会大大降低我们需要的计算速度，因此也产生了随机梯度下降（Stochastic gradient descent）这一类的方法。随机梯度下降通常随机选取某个样本并计算其相应导数，作为所有样本相同的导数进行计算，这种方法在实践上有不错的效果。当然，我们可以随机选取一小批样本，样本数量记为batch size，将batch size个样本的导数进行累加后求均值作为所有样本相同的导数，再进一步计算；这种方法我们称为小批量随机梯度下降法（mini-batch stochastic gradient descent）。\n虽然梯度下降直接快速，但是也有一定的不足，由于我们需要选取stepsize，若stepsize太大，那可能无法达到优化问题的最优点；若stepsize太小，则收敛速度太慢，大大降低了模型训练速度。同时，不变的stepsize可能会使结果无法收敛到全局最优解，并可能停在局部最小值（局部最优解），当然很容易陷入到“鞍点”。\n 图6. “鞍点”示意图   Momentum优化器（Momentum Optimizer） Momentum优化器也可称为基于动量的优化算法，其中参数的更新会根据梯度的变化而变化：动量再梯度连续指向同一方向上时会增加，而在梯度方向变化时会减小；这样可以更快地收敛并减少震荡。公式表示为： \\[ v_t^{W} = \\gamma \\times v_{t-1}^{W} + \\alpha \\times \\frac{\\partial}{\\partial W_{ij}}L(w,b)\\\\ W_{ij} = W_{ij} - v_t^{W}\\\\ v_t^{b} = \\gamma \\times v_{t-1}^{b} + \\alpha \\times \\frac{\\partial}{\\partial b_i}L(w,b)\\\\ b_i = b_i - v_t^{b} \\] 其中，\\(\\gamma\\)是动量更新值，通常取0.9。这样，基于Momentum的随机梯度下降可以更快地收敛，并减少陷入局部最优点的概率。\n Adagrad优化器（Adaptive Gradient Optimizer） Momentum优化器虽然加速了参数的更新并加速收敛，但存在缺点是没有对不同的参数进行区别对待。Adagrad优化器则基于这样的梯度优化思想：根据参数自适应地更新学习率（也为步长stepsize），对于不频繁更新的参数做较大更新，而对于频繁更新的参数做较小的更新。\nAdagrad对于每个参数\\(\\theta_{t,i}\\)，在每个时间点\\(t\\)使用不同的学习率。首先我们先考虑Adagrad的单参数情况，为了公式形式的整洁，我们记各个时间点\\(t\\)的参数\\(\\theta_{t,i}\\)下的目标函数梯度为\\(g_{t,i}\\)： \\[g_{t,i} = \\frac{\\partial}{\\partial \\theta_{t,i}}L(\\theta_{t,i})\\] 在Adagrad的更新规则中，我们会根据每个时间点\\(t\\)对每个参数\\(\\theta_{t+1,i}\\)基于上次已经计算过的梯度\\(\\theta_{t,i}\\)来修改步长： \\[\\theta_{t+1,i} = \\theta_{t,i} - \\frac{\\alpha}{\\sqrt{G_{t,ii}+\\epsilon}}\\times g_{t,i}\\] 其中，\\(G_{t,ii}\\in R^{d\\times d}\\)，\\(G_{t,ii}\\)是一个对角矩阵，其对角元素\\(t\\)时刻参数\\(\\theta_{t,i}\\)的梯度平方和，\\(\\epsilon\\)是一个光滑项，防止分母为0，通常取1e-8级别的数。另外，\\(\\alpha\\)使用默认值0.01。Adagrad有个缺点就是其分母实际上累积了梯度的平方，会使得步长（学习率）越来越小。\n Adadelta优化器 Adadelta是对Adagrad的改进，通过用过去计算的梯度平方的均值代替单纯的累加梯度平方，可以避免一味地降低步长。\n\\(t\\)时刻的梯度平方均值表示为： \\[ E[g^2]_{t,i} = \\gamma\\times E[g^2]_{t-1,i} + (1-\\gamma)\\times g_{t,i}^2 \\] 其中，\\(\\gamma\\)和前面提到的Momentum优化器中的\\(\\gamma\\)类似，通常取0.9。将累积梯度平方更改为梯度平方均值，可得到： \\[\\theta_{t+1,i} = \\theta_{t,i} - \\frac{\\alpha}{\\sqrt{E_t+\\epsilon}}\\times g_{t,i}\\] 另外，我们还想要变换分子的\\(\\gamma\\)，将\\(\\gamma\\)改为\\(\\sqrt{E[\\Delta\\theta^2]_t+\\epsilon}\\)，便得到Adadelta的计算形式，以上内容可以总结为： \\[ E[g^2]_{t,i} = \\gamma\\times E[g^2]_{t-1,i} + (1-\\gamma)\\times g_{t,i}^2\\\\ E[\\Delta\\theta^2]_{t,i} = \\gamma\\times E[\\Delta\\theta^2]_{t-1,i} + (1-\\gamma)\\times \\Delta\\theta_{t,i}^2\\\\ \\theta_{t+1,i} = \\theta_{t,i} - \\frac{\\sqrt{E[\\Delta\\theta^2]_{t-1,i}+\\epsilon}}{\\sqrt{E[g^2]_{t,i}+\\epsilon}}\\times g_{t,i} \\] 显然，我们不再需要提前设定步长了。\n Adam优化器（Adaptive Moment Estimation Optimizer, Adam Optimizer） Adam也是个人名，圣经中说他是世上的第一个人类也是第一个男人，接着和夏娃结为夫妻，过上了幸福的生活…跑远了！回正题！其实Adam的全称中文是自适应矩估计，它不仅像Adadelta一样存储过去梯度平方\\(v_t\\)的平均值之外，还保留了像Momentum一样的保留了过去梯度\\(m_t\\)，其计算公式为： \\[ m_t = \\beta_1m_{t-1} + (1-\\beta_1)g_t\\\\ v_t = \\beta_2v_{t-1} + (1-\\beta_2)g_t^2 \\] 由于\\(m_t\\)和\\(v_t\\)在计算上会存在偏差，所以进行了偏差校正： \\[ \\hat{m_t} = \\frac{m_t}{1-\\beta_1^t}\\\\ \\hat{v_t} = \\frac{v_t}{1-\\beta_2^t} \\] Adam的更新规则： \\[\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{v_t}+\\epsilon}}\\times \\hat{m_t}\\] 其中，\\(\\beta_1\\)通常取0.9，\\(\\beta_2\\)通常取0.999，\\(\\epsilon\\)通常取1e-8。大多实验表明，Adam比其他自适应学习算法表现更优。\n 算法表现效果  图7. 不同优化器的随机梯度下降法在鞍点处的不同表现  注：动图来源于Sebastian Ruder的文章，由Alec Radford制作。\n 题外话（跳过这段吧~）\n在整理学习优化算法的时候，发现一件有趣的事。我边看着《Tensorflow入门与实战》的第四章边学习优化算法，同时网上边找找资料帮助理解。然而有趣的是我找到了一位业界大神Sebastian Ruder的主页，并看到了他在2016年1月6日写下了An overview of gradient descent optimization algorithms。看着看着我发现手中拿的书竟然是电脑屏幕上显示的文章的中文版，我便好奇地寻找手中这本实战书的出版时间 —— 2018年2月17日。怪哉怪哉~再翻翻书，发现并无任何引用。算了，回归主题！（Reference选了日期比较前的S.R.大佬的文章作为引用）    反向传播算法（Backpropagation） 反向传播算法是目前用来训练人工神经网络（Artificial Neural Network，ANN）的最常用且最有效的算法。首先我们先定义变量：\n \\(v_i^{(l)}\\)：第\\(l\\)层的第\\(i\\)个节点的输入值，\\(v_i^{(l)} = \\sum_{j=0}^n w_{ij}^{(l)}a_j^{(l-1)} + b_i^{(l)}\\)； \\(a_i^{(l)}\\)：第\\(l\\)层的第\\(i\\)个节点的输出值，\\(a_i^{(l)} = f(v_i^{(l)})\\)，其中\\(f(\\cdot)\\)是激活函数； \\(w_{ij}^{(l)}\\)：第\\(l-1\\)层的第\\(j\\)个节点到第\\(l\\)层的第\\(i\\)个节点的权值； \\(b_i^{(l)}\\)：第\\(l\\)层计算第\\(i\\)个节点的输入值时的偏置项的值； \\(K\\)：神经网络的总层数； \\(f(\\cdot)\\)：激活函数，例如sigmoid函数，tanh函数或者ReLu函数； \\(L(w,b)\\)：整体损失函数，常用的损失函数为\\(\\frac{1}{2n}\\sum_{i=1}^n(y_i-f(x_i))^2\\)，其中n是样本的个数。  反向传播计算过程的细节如下所示：\n 参数初始化\n随机初始化网络中的各层的参数\\(w_{ij}^{(l)}\\)和\\(b_i^{(l)}\\)，且满足\\(N(0,\\ 0.01)\\)分布的随机数；\n 前向传播\n  以图3中隐藏层的第一个节点（从上往下数第一个）为例，对于这个节点而言，其输入信号为： \\[v_1^{(2)} = a_1^{(1)}\\times w_{11}^{(2)} + a_2^{(1)}\\times w_{12}^{(2)} + a_3^{(1)}\\times w_{13}^{(2)} + b_1^{(2)}\\]\n同理，我们可以得到该层的其他节点的计算： \\[ v_2^{(2)} = a_1^{(1)}\\times w_{21}^{(2)} + a_2^{(1)}\\times w_{22}^{(2)} + a_3^{(1)}\\times w_{23}^{(2)} + b_2^{(2)}\\\\ v_3^{(2)} = a_1^{(1)}\\times w_{31}^{(2)} + a_2^{(1)}\\times w_{32}^{(2)} + a_3^{(1)}\\times w_{33}^{(2)} + b_3^{(2)}\\\\ v_4^{(2)} = a_1^{(1)}\\times w_{41}^{(2)} + a_2^{(1)}\\times w_{42}^{(2)} + a_3^{(1)}\\times w_{43}^{(2)} + b_4^{(2)}\\\\ \\]\n若用矩阵形式进行表达： \\[V^{(2)} = A^{(1)}\\times W^{(2)} + B^{(2)}\\] 其中， \\[ V^{(2)} = (v_1^{(2)}, v_2^{(2)}, v_3^{(2)}, v_4^{(2)})\\\\ A^{(1)} = (a_1^{(1)}, a_2^{(1)}, a_3^{(1)})\\\\ W^{(2)} = \\begin{pmatrix} w_{11}^{(2)} \u0026amp; w_{21}^{(2)} \u0026amp; w_{31}^{(2)} \u0026amp; w_{41}^{(2)}\\\\ w_{12}^{(2)} \u0026amp; w_{22}^{(2)} \u0026amp; w_{32}^{(2)} \u0026amp; w_{42}^{(2)}\\\\ w_{13}^{(2)} \u0026amp; w_{23}^{(2)} \u0026amp; w_{33}^{(2)} \u0026amp; w_{43}^{(2)} \\end{pmatrix}\\\\ B^{(2)} = (b_1^{(2)}, b_2^{(2)}, b_3^{(2)}, b_4^{(2)}) \\]\n再经过激活函数（非线性函数）变换后得到： \\[A^{(2)} = f(V^{(2)})\\]\n同理，经由 \\[ V^{(3)} = A^{(2)}\\times W^{(3)} + B^{(3)}\\\\ A^{(3)} = f(V^{(3)}) \\] 可以得到最终输出。\n 反向传播\n首先对于最后一层节点的偏导数，其实我们很容易得到，我们定义神经网络总共有\\(K\\)层，对于最后一层即第\\(K\\)层（输出层），根据偏导数的定义：  \\[ \\begin{align} \\delta_i^{(K)} =\u0026amp;\\ \\frac{\\partial}{\\partial v_i^{(K)}}L(w,b)\\\\ =\u0026amp;\\ \\frac{\\partial L(w,b)}{\\partial a_i^{(K)}}\\times \\frac{\\partial a_i^{(K)}}{\\partial v_i^{(K)}}\\\\ =\u0026amp;\\ \\frac{\\partial L(w,b)}{\\partial a_i^{(K)}}\\times f\u0026#39;(v_i^{(K)}) \\end{align} \\tag{4} \\] 明显的是，\\(a_i^{(K)}\\)是最后一层（即输出层）的输出值，\\(f\u0026#39;(v_i^{(K)})\\)则是激活函数对\\(v_i^{(K)}\\)的导数。\n对（4）式进一步推导可以得到： \\[ \\begin{align} \\delta_i^{(K)} =\u0026amp;\\ \\frac{\\partial}{\\partial a_i^{(K)}}\\Big[\\frac{1}{2n_K}\\sum_{j=1}^{n_K}\\Big(y_j-a_j^{(K)}\\Big)^2\\Big]\\times f\u0026#39;(v_i^{(K)})\\\\ =\u0026amp;\\ -\\frac{1}{n_k}(y_i-a_i^{(K)})\\times f\u0026#39;(v_i^{(K)}) \\end{align} \\] 其中，\\(y_i\\)是样本对应的正确值，\\(n_K\\)是第K层节点个数。\n因此，可得到最后一层（第K层）的计算公式： \\[ \\delta_i^{(K)} = -\\frac{1}{n_k}(y_i-a_i^{(K)})\\times f\u0026#39;(v_i^{(K)}) \\tag{5} \\]\n那么对于第\\(K-1\\)层的偏导数，可以根据第\\(K\\)层的计算出来： \\[ \\begin{align} \\delta_i^{(K-1)} =\u0026amp;\\ \\frac{\\partial}{\\partial v_i^{(K-1)}}L(w,b)\\\\ =\u0026amp;\\ \\frac{\\partial}{\\partial v_i^{(K-1)}}\\Big[\\frac{1}{2n_K}\\sum_{j=1}^{n_K}\\Big(y_j-a_j^{(K)}\\Big)^2\\Big]\\\\ =\u0026amp;\\ \\frac{1}{2n_K}\\Big[\\frac{\\partial}{\\partial v_i^{(K-1)}} \\sum_{j=1}^{n_K}\\Big(y_j-f(v_j^{(K)})\\Big)^2\\Big] \\end{align} \\tag{6} \\]\n利用连续函数的求导和求和顺序可互换，（6）式可以推得： \\[ \\begin{align} \\delta_i^{(K-1)} =\u0026amp;\\ -\\frac{1}{n_K} \\sum_{j=1}^{n_K} \\Big[(y_j-f(v_j^{(K)}))\\times \\frac{\\partial}{\\partial v_i^{(K-1)}}f(v_j^{(K)})\\Big]\\\\ =\u0026amp;\\ -\\frac{1}{n_K} \\sum_{j=1}^{n_K} \\Big[(y_j-f(v_j^{(K)}))\\times \\frac{\\partial f(v_j^{(K)})}{\\partial v_i^{(K)}}\\times \\frac{\\partial v_i^{(K)}}{\\partial v_i^{(K-1)}} \\Big]\\\\ =\u0026amp;\\ \\sum_{j=1}^{n_K} \\Big[-\\frac{1}{n_K} (y_j-f(v_j^{(K)}))\\times f\u0026#39;(v_j^{(K)})\\times \\frac{\\partial v_i^{(K)}}{\\partial v_i^{(K-1)}} \\Big] \\end{align} \\]\n联合（5）式，由（6）式可以得到： \\[ \\begin{align} \\delta_i^{(K-1)} =\u0026amp;\\ \\sum_{j=1}^{n_K} \\Big[ \\delta_i^{(K)}\\times \\frac{\\partial v_i^{(K)}}{\\partial v_i^{(K-1)}} \\Big]\\\\ =\u0026amp;\\ \\sum_{j=1}^{n_K}\\Bigg[ \\delta_i^{(K)}\\times \\frac{\\partial }{\\partial v_i^{(K-1)}}\\Big[ \\sum_{m=0}^{n_{K-1}}a_m^{(K-1)}\\times w_{jm}^{(K)}+b_j^{(K)} \\Big]\\Bigg]\\\\ =\u0026amp;\\ \\sum_{j=1}^{n_K}\\Bigg[ \\delta_i^{(K)}\\times \\frac{\\partial }{\\partial v_i^{(K-1)}}\\Big[ \\sum_{m=0}^{n_{K-1}}f(v_m^{(K-1)})\\times w_{jm}^{(K)}+b_j^{(K)} \\Big]\\Bigg]\\\\ =\u0026amp;\\ \\sum_{j=1}^{n_K}\\Bigg[ \\delta_i^{(K)}\\times f\u0026#39;(v_i^{(K-1)})\\times w_{ji}^{(K)} \\Bigg]\\\\ =\u0026amp;\\ \\Bigg[\\sum_{j=1}^{n_K}\\Big[ \\delta_i^{(K)}\\times w_{ji}^{(K)} \\Big]\\Bigg] \\times f\u0026#39;(v_i^{(K-1)}) \\end{align} \\]\n因此，可得到第K-1层的计算公式： \\[ \\delta_i^{(K-1)} = \\Bigg[\\sum_{j=1}^{n_K}\\Big[ \\delta_i^{(K)}\\times w_{ji}^{(K)} \\Big]\\Bigg] \\times f\u0026#39;(v_i^{(K-1)}) \\tag{7} \\]\n同理，用\\(K-2\\)替换\\(K-1\\)，用\\(K-1\\)替换\\(K\\)，则可计算第\\(K-2\\)层的偏导数。 \\[ \\delta_i^{(K-2)} = \\Bigg[\\sum_{j=1}^{n_{K-1}}\\Big[ \\delta_i^{(K-1)}\\times w_{ji}^{(K-1)} \\Big]\\Bigg] \\times f\u0026#39;(v_i^{(K-2)}) \\tag{7} \\]\n同样的，可以根据（7）式计算得到网络中所有节点的偏导数。\n回归我们的参数迭代公式： \\[ \\begin{align} w_{ij}^{(l)} =\u0026amp;\\ w_{ij}^{(l)} - \\alpha\\times \\frac{\\partial}{\\partial w_{ij}^{(l)}}L(w,b)\\\\ b_i^{(l)} =\u0026amp;\\ b_i^{(l)} - \\alpha\\times \\frac{\\partial}{\\partial b_i^{(l)}}L(w,b) \\end{align} \\tag{8} \\]\n对于后面的偏导数部分，我们可以加以处理，对于参数\\(w_{ij}^{(l)}\\)部分： \\[ \\begin{align} \\frac{\\partial L(w,b)}{\\partial w_{ij}^{(l)}} =\u0026amp;\\ \\frac{\\partial L(w,b)}{\\partial v_i^{(l)}}\\times \\frac{\\partial v_i^{(l)}}{\\partial w_{ij}^{(l)}}\\\\ =\u0026amp;\\ \\delta_i^{(l)}\\times \\frac{\\partial v_i^{(l)}}{\\partial w_{ij}^{(l)}}\\\\ =\u0026amp;\\ \\delta_i^{(l)}\\times \\frac{\\partial }{\\partial w_{ij}^{(l)}}\\Big[\\sum_{j=0}^{n_{l-1}}a_j^{(l-1)}\\times w_{ij}^{(l)}+b_i^{(l)}\\Big]\\\\ =\u0026amp;\\ \\delta_i^{(l)}\\times a_j^{(l-1)} \\end{align} \\]\n对于参数\\(b_i^{(l)}\\)部分： \\[ \\begin{align} \\frac{\\partial L(w,b)}{\\partial b_i^{(l)}} =\u0026amp;\\ \\frac{\\partial L(w,b)}{\\partial v_i^{(l)}}\\times \\frac{\\partial v_i^{(l)}}{\\partial b_i^{(l)}}\\\\ =\u0026amp;\\ \\delta_i^{(l)}\\times \\frac{\\partial}{\\partial b_i^{(l)}}\\Big[\\sum_{j=0}^{n_{l-1}}a_j^{(l-1)}\\times w_{ij}^{(l)}+b_i^{(l)}\\Big]\\\\ =\u0026amp;\\ \\delta_i^{(l)} \\end{align} \\]\n因此，由（8）式可以推得 \\[ \\begin{align} w_{ij}^{(l)} =\u0026amp;\\ w_{ij}^{(l)} - \\alpha\\times \\delta_i^{(l)}\\times a_j^{(l-1)}\\\\ b_i^{(l)} =\u0026amp;\\ b_i^{(l)} - \\alpha\\times \\delta_i^{(l)} \\end{align} \\tag{8} \\]\nOver！反向传播算法到此结束！\n Reference Sebastian Ruder. An overview of gradient descent optimization algorithms\n ","date":1546646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546646400,"objectID":"a02ffc69823c43b02495a4e0f043803c","permalink":"/post/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/","publishdate":"2019-01-05T00:00:00Z","relpermalink":"/post/%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/","section":"post","summary":"神经元及神经网络基础结构  图1. 神经元的组成（源自维基百科）  神经元这个图大多数理科生在高中生物课本都学过~神经网络则由许许多多的神经元所组成，通常一个神经元具有多个树突，主要用来接收消息；轴突只有一条，相当于我们定义的一个计算过程；而轴突尾部的许许多多轴突末梢，将传递信息给其他神经元。\n 图2. 神经网络基础结构  通常这里的非线性函数会用上各式各样的激活函数，比如Sigmoid函数，tanh函数和ReLu函数。\nSigmoid函数\n\\[f(z) = \\frac{1}{1+e^{-z}}\\] tanh函数\n\\[f(z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}\\] ReLu函数\n\\[f(z) = \\max(0,z)\\]\n 神经网络基础认知 我们把许多神经元组合起来就可以得到一个神经网络，由于有输入的数据和我们想得到的输出数据，便会有“输入层”（Input layer）和“输出层”（Output layer）；中间的神经元则组成了“隐藏层”（Hidden layer）。在下面图3中，输入层有3个神经元，隐藏层有4个神经元，输出层有2个神经元。在实际情况中，输入层和输出层通常是固定的，而隐藏层的层数和节点数则可以自由调节。  图3. 神经网络基础层级结构  我们假设一个全连接的网络结构，其中隐藏层只有一层。另外，假设输入层和隐藏层之间的边的权值构成的矩阵为 \\[ \\left [ \\begin{matrix} w_{11} \u0026amp; w_{12} \u0026amp; w_{13} \\\\ w_{21} \u0026amp; w_{22} \u0026amp; w_{23} \\\\ w_{31} \u0026amp; w_{32} \u0026amp; w_{33} \\end{matrix} \\right ] \\] 其中，第一列的\\(w_{11}, w_{21}, w_{31}\\)代表的是输入层的点\\(x_1\\)分别连接隐藏层的三个节点的边的权值；第二列的\\(w_{12}, w_{22}, w_{32}\\)代表的是输入层的点\\(x_2\\)分别连接隐藏层的三个节点的边的权值；第三列的\\(w_{13}, w_{23}, w_{33}\\)代表的是输入层的点\\(x_3\\)分别连接隐藏层的三个节点的边的权值。\n图中的“+1”点代表我们添加了一个值b，称其为偏置项。那么，隐藏层的节点可以由下计算得到： \\[ \\begin{align} a_1 = w_{11}\\times x_1 + w_{12}\\times x_2 + w_{13}\\times x_3 + b_1\\\\ a_2 = w_{21}\\times x_1 + w_{22}\\times x_2 + w_{23}\\times x_3 + b_2\\\\ a_3 = w_{31}\\times x_1 + w_{32}\\times x_2 + w_{33}\\times x_3 + b_3 \\end{align} \\tag{1} \\] 由于线性计算的表现能力比较差，所以考虑用非线性函数进行计算，即使用激活函数\\(f(\\cdot)\\)（前面已提及）。（1）式可以变换为（2）式： \\[ \\begin{align} a_1 = f(w_{11}\\times x_1 + w_{12}\\times x_2 + w_{13}\\times x_3 + b_1)\\\\ a_2 = f(w_{21}\\times x_1 + w_{22}\\times x_2 + w_{23}\\times x_3 + b_2)\\\\ a_3 = f(w_{31}\\times x_1 + w_{32}\\times x_2 + w_{33}\\times x_3 + b_3) \\end{align} \\tag{2} \\] 将（2）式改写为矩阵运算形式（3）式： \\[ \\begin{align} \\boldsymbol{a} = f \\begin{pmatrix} \\begin{pmatrix} w_{11},w_{12},w_{13}\\\\w_{21},w_{22},w_{23}\\\\w_{31},w_{32},w_{33} \\end{pmatrix} \\begin{pmatrix} x_1\\\\x_2\\\\x_3 \\end{pmatrix} + \\begin{pmatrix} b_1\\\\b_2\\\\b_3 \\end{pmatrix} \\end{pmatrix} = f(\\boldsymbol{W}x+\\boldsymbol{B}) \\end{align} \\tag{3} \\]","tags":["Deep Learning"],"title":"深度神经网络基础","type":"post"},{"authors":null,"categories":["Rmarkdown"],"content":" 缘于某人用Rmarkdown搞不出中文内容的pdf而引发一场激战之下，TT只能忍气吞声继续走上帮人帮到底的道路，于是网上搜出一大堆关于Rmarkdown生成中文pdf的麻烦事。无奈，众里寻它千百度，最终发现解决问题的YAML模板及相关的解决方案，怕在接下来的日子可能遭受同样的折磨，并以扩充Blog文章为前提，书写此文。\n首先，让我们先在RStudio菜单栏选择Tools并点击Global Options。选择Sweaver并按图勾选，最后点OK~  Figure1. 可爱的Global Options窗口  然后.Rmd文件中的YAML模板如下设置：\n--- title: \u0026quot;我是一个Test文档的标题\u0026quot; author: \u0026quot;我是一个Test文档的作者名称\u0026quot; date: \u0026quot;我是一个Test文档的写作日期\u0026quot; CJKmainfont: Microsoft YaHei output: pdf_document: includes: header-includes: - \\usepackage{xeCJK} keep_tex: yes latex_engine: xelatex --- 注：介个模板用上了大微软的雅黑字体，如若想修改，那请继续摸索摸索。（T.T累了不想改了~）\n搞定！Over！愿你的探索之路不与我一样艰辛((٩(//̀Д/́/)۶))\n课外补充：\n 关于Rmarkdown to pdf的美好世界\n如果你也被这样的问题所困扰，那么你会发现R界的大佬谢益辉搞了个包叫rticles，直接提供template给你写中文文档。然而，无奈Tex世界的混乱，还是遇到奇奇怪怪的乱七八糟的问题，但是大佬说大家用TinyTex吧，那将提供Rmarkdown to pdf的一片美好世界。  ","date":1546387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546387200,"objectID":"787c464bca8e3faac12f75c940ece8da","permalink":"/post/%E5%85%B3%E4%BA%8Ermarkdown%E7%94%9F%E6%88%90%E4%B8%AD%E6%96%87%E5%86%85%E5%AE%B9pdf%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B/","publishdate":"2019-01-02T00:00:00Z","relpermalink":"/post/%E5%85%B3%E4%BA%8Ermarkdown%E7%94%9F%E6%88%90%E4%B8%AD%E6%96%87%E5%86%85%E5%AE%B9pdf%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B/","section":"post","summary":" 缘于某人用Rmarkdown搞不出中文内容的pdf而引发一场激战之下，TT只能忍气吞声继续走上帮人帮到底的道路，于是网上搜出一大堆关于Rmarkdown生成中文pdf的麻烦事。无奈，众里寻它千百度，最终发现解决问题的YAML模板及相关的解决方案，怕在接下来的日子可能遭受同样的折磨，并以扩充Blog文章为前提，书写此文。\n首先，让我们先在RStudio菜单栏选择Tools并点击Global Options。选择Sweaver并按图勾选，最后点OK~  Figure1. 可爱的Global Options窗口  然后.Rmd文件中的YAML模板如下设置：\n--- title: \u0026quot;我是一个Test文档的标题\u0026quot; author: \u0026quot;我是一个Test文档的作者名称\u0026quot; date: \u0026quot;我是一个Test文档的写作日期\u0026quot; CJKmainfont: Microsoft YaHei output: pdf_document: includes: header-includes: - \\usepackage{xeCJK} keep_tex: yes latex_engine: xelatex --- 注：介个模板用上了大微软的雅黑字体，如若想修改，那请继续摸索摸索。（T.T累了不想改了~）\n搞定！Over！愿你的探索之路不与我一样艰辛((٩(//̀Д/́/)۶))\n课外补充：\n 关于Rmarkdown to pdf的美好世界\n如果你也被这样的问题所困扰，那么你会发现R界的大佬谢益辉搞了个包叫rticles，直接提供template给你写中文文档。然而，无奈Tex世界的混乱，还是遇到奇奇怪怪的乱七八糟的问题，但是大佬说大家用TinyTex吧，那将提供Rmarkdown to pdf的一片美好世界。  ","tags":["Rmarkdown"],"title":"关于Rmarkdown生成中文内容pdf的那些事","type":"post"},{"authors":null,"categories":["Deep Learning"],"content":"\rIntroduction\rWe assume the observed variable \\(x\\) is a random sample from an unknown underlying process, whose true distribution \\(p^*(x)\\) is unknown. We attempt to approximate this underlying process with a chosen model \\(p_{\\theta}(x)\\), with parameters \\(\\theta\\): \\[x\\sim p_{\\theta}(x)\\] We always talk about learning like Deep Learning, and actually the learning is the process of searching for a value of the parameters \\(\\theta\\) in model \\(p_{\\theta}(x)\\), which can approximate the true distribution of the data, denoted by \\(p^*(x)\\). In other words, \\[p_{\\theta}(x)\\approx p^*(x)\\] Latent variables are variables that are part of the model, but which we don’t observe, and are therefore not part of the dataset. We typically use \\(z\\) to denote such latent variables.\nThe marginal distribution over the observed variables \\(p_{\\theta}(x)\\), is given by: \\[\rp_{\\theta}(x) = \\int p_{\\theta}(x,z) dz = \\int p_{\\theta}(z) p_{\\theta}(x|z) dz\r\\] We use the term deep latent variable model (DLVM) to denote a latent variable model \\(p_{\\theta}(x,z)\\) whose distributions are parameterized by neural networks.\nExample DLVM for multivariate Bernoulli data\rA simple example DLVM for binary data \\(x\\), with a spherical Gaussian latent space, and a factorized Bernoulli obervation model \\[\rp(z) = \\mathcal{N}(0,\\text{I})\\\\\r\\text{p} = \\text{DecoderNeuralNet}_{\\theta}(z)\\\\\r\\begin{align}\r\\log p(x|z) =\u0026amp; \\sum_{j=1}^J \\log p(x_j|z) = \\sum_{j=1}^J \\text{Bernoulli}(x_j,p_j)\\\\\r=\u0026amp; \\sum_{j=1}^Jx_j \\log p_j + (1-x_j)\\log (1-p_j)\r\\end{align}\r\\] where \\(0\\leq p_j\\leq 1\\).\nTherefore, we easily get \\(p(x,z) = p(x|z)\\times p(z)\\) by the term we described above.\n\rSome problem\rNote that \\(p_{\\theta}(x,z)\\) is efficient to compute. Since the intractability of \\(p_{\\theta}(x)\\) (\\(p_{\\theta}(x) = \\int p_{\\theta}(x,z) dz\\)), the posterior distribution \\(p_{\\theta}(z|x)\\) is also intractable, because their densities are related through the basic identity: \\[p_{\\theta}(z|x) = \\frac{p_{\\theta}(x,z)}{p_{\\theta}(x)}\\]\nHow can we perform efficient approximate posterior inference and efficient approximate maximum likelihood estimation in deep latent variable models, in the presence of large datasets?\n\rSimilar method like DLVM\rWe introduce a parametric inference model \\(q_{\\phi}(z|x)\\) (also called as encoder)in this part and we try to optimize the variational parameters \\(\\phi\\) such that: \\[q_{\\phi}(z|x) \\approx p_{\\theta}(z|x)\\]\nSimilar to DLVM, the distribution of \\(q_{\\phi}(z|x)\\) also can be parameterized using deep neural networks. In this case, the variational parameters \\(\\phi\\) include the weights and biases of the neural network. For example: \\[\r(\\mu,\\sigma) = \\text{EncoderNeuralNet}_{\\phi}(x)\\\\\rq_{\\phi}(z|x) = \\mathcal{N}(\\mu,\\text{diag}(\\sigma^2))\r\\]\n\r\rEvidence lower bound (ELBO) and KL divergence\rThe optimization objective of the variational autoencoder is the evidence lower bound, abbreviated as ELBO. An alternative term for this objective is variational lower bound. We can obtain the lower bound by: \\[\r\\begin{align}\r\\log p_{\\theta}(x) =\u0026amp;\\ \\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x)] = \\mathbb{E}_{q_{\\phi}(z|x)} \\Big[\\log\\Big[ \\frac{p_{\\theta}(x,z)}{p_{\\theta}(z|x)}\\Big]\\Big]\\\\\r=\u0026amp;\\ \\mathbb{E}_{q_{\\phi}(z|x)}\\Big[\\log\\Big[\\frac{p_{\\theta}(x,z)}{q_{\\phi}(z|x)}\\frac{q_{\\phi}(z|x)}{p_{\\theta}(z|x)}\\Big]\\Big]\\\\\r=\u0026amp;\\ \\mathbb{E}_{q_{\\phi}(z|x)}\\Big[\\log\\Big[\\frac{p_{\\theta}(x,z)}{q_{\\phi}(z|x)}\\Big]\\Big] + \\mathbb{E}_{q_{\\phi}(z|x)}\\Big[\\log\\Big[\\frac{q_{\\phi}(z|x)}{p_{\\theta}(z|x)}\\Big]\\Big]\\\\\r=\u0026amp;\\ \\mathcal{L}_{\\theta,\\phi}(x) + KL[q_{\\phi}(z|x)||p_{\\theta}(z|x)]\\\\\r\\geq\u0026amp;\\ \\mathcal{L}_{\\theta,\\phi}(x)\r\\end{align}\r\\]\nKL divergence\rWe want to find a good probability distribution \\(q_{\\phi}(z|x)\\) (‘good’ means the efficient computation) to approximate the true posterior probability \\(p_{\\theta}(z|x)\\), where the \\(z\\) is the latent variable. KL divergence can measure the distance well between these two distribution. For the discrete probability situation, the KL divergence can be written as \\[KL(q||p) = \\sum q(x)\\log \\frac{q(x)}{p(x)}\\]\nExample of 1-dimension Guassian distribution\rSupposed that we have two random variables \\(x_1, x_2\\) w.r.t the guassian distribution \\(\\mathcal{N}(\\mu_1,\\sigma_1^2),\\mathcal{N}(\\mu_2,\\sigma_2^2)\\) respectively.\nRecall that the density function of guassian distribution \\[\r\\mathcal{N}(\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\r\\] Then \\[\r\\begin{align}\rKL(p_1,p_2) =\u0026amp;\\ \\int p_1(x)\\log \\frac{p_1(x)}{p_2(x)}dx\\\\\r=\u0026amp;\\ \\int p_1(x)(\\log p_1(x) - \\log p_2(x))dx\\\\\r=\u0026amp;\\ \\int p_1(x)(\\log \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}}e^{-\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}} - \\log \\frac{1}{\\sqrt{2\\pi\\sigma_2^2}}e^{-\\frac{(x-\\mu_2)^2}{2\\sigma_2^2}})dx\\\\\r=\u0026amp;\\ \\int p_1(x)(-\\log \\sqrt{2\\pi \\sigma_1^2} - \\frac{(x-\\mu_1)^2}{2\\sigma_1^2} + \\log \\sqrt{2\\pi \\sigma_2^2} + \\frac{(x-\\mu_2)^2}{2\\sigma_2^2})dx\\\\\r=\u0026amp;\\ \\int p_1(x)(-\\frac{1}{2}\\log2\\pi-\\log\\sigma_1+\\frac{1}{2}\\log2\\pi+\\log\\sigma_2 - (\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}-\\frac{(x-\\mu_2)^2}{2\\sigma_2^2}))dx\\\\\r=\u0026amp;\\ \\int p_1(x)(\\log\\frac{\\sigma_2}{\\sigma_1} - (\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}-\\frac{(x-\\mu_2)^2}{2\\sigma_2^2}))dx\\\\\r=\u0026amp;\\ \\int p_1(x)(\\log\\frac{\\sigma_2}{\\sigma_1})dx + \\int p_1(x)(\\frac{(x-\\mu_2)^2}{2\\sigma_2^2})dx - \\int p_1(x)(\\frac{(x-\\mu_1)^2}{2\\sigma_1^2})dx\\\\\r=\u0026amp;\\ \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{1}{2\\sigma_2^2}\\int p_1(x)(x-\\mu_2)^2dx - \\frac{1}{2\\sigma_1^2}\\int p_1(x)(x-\\mu_1)^2dx\r\\end{align}\r\\] Since \\(\\sigma^2 = \\int p_1(x)(x-\\mu_1)^2dx\\), then \\[\r\\begin{align}\rKL(p_1,p_2) =\u0026amp;\\ \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{1}{2\\sigma_2^2}\\int p_1(x)(x-\\mu_2)^2dx - \\frac{1}{2}\\\\\r=\u0026amp;\\ \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{1}{2\\sigma_2^2}\\int p_1(x)(x - \\mu_1 + \\mu_1 - \\mu_2)^2dx - \\frac{1}{2}\\\\\r=\u0026amp;\\ \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{1}{2\\sigma_2^2}[\\int p_1(x)(x-\\mu_1)^2dx+\\int p_1(x)(\\mu_1-\\mu_2)^2dx+2\\int p_1(x)(x-\\mu_1)(\\mu_1-\\mu_2)dx] - \\frac{1}{2}\\\\\r\\end{align}\r\\] We know that \\(\\mu_1 = \\int x p_1(x)dx\\), so \\(2\\int p_1(x)(x-\\mu_1)(\\mu_1-\\mu_2)dx = 2(\\mu_1-\\mu_2)[\\int xp_1(x)dx - \\mu_1] = 0\\), thus \\[\r\\begin{align}\rKL(p_1,p_2) =\u0026amp;\\ \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{1}{2\\sigma_2^2}[\\int p_1(x)(x-\\mu_1)^2dx + (\\mu_1-\\mu_2)^2] - \\frac{1}{2}\\\\\r=\u0026amp;\\ \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2+(\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\\\\\r\\end{align}\r\\] If we suppose that the \\(\\mathcal{N}(\\mu_2,\\sigma_2^2)\\) is standard guassian distribution, \\(\\mu_2 = 0, \\sigma_2^2 = 1\\), so \\[\r\\begin{align}\rKL =\u0026amp;\\ \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2+(\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\\\\\r=\u0026amp;\\ \\log1 - \\log\\sigma_1 + \\frac{\\sigma_1^2+(\\mu_1 - 0)^2}{2} - \\frac{1}{2}\\\\\r=\u0026amp;\\ -\\log\\sigma_1 + \\frac{\\sigma_1^2+\\mu_1^2}{2} - \\frac{1}{2}\\\\\r\\end{align}\r\\] We expect that the KL can be as small as possible, we calculate its derivative, then we get \\[\r\\frac{\\partial KL}{\\partial \\sigma_1} = -\\frac{1}{\\sigma_1} + \\sigma_1\\\\\r\\frac{\\partial KL}{\\partial \\mu_1} = \\mu_1\r\\] We let them equal to zero, then we get \\[\r-\\frac{1}{\\sigma_1} + \\sigma_1 = 0 \\Rightarrow \\sigma_1 = 1\\\\\r\\mu_1 = 0\r\\] which means that the KL becomes the minimum when \\(x_2 \\sim \\mathcal{N}(0,1)\\)\n\rMinimization of KL divergence\rIf we want to use the ELBO to approximate the log-likelihood, then we need to minimize the \\(D_{KL}[q_{\\phi}(z|x)||p_{\\theta}(z|x)]\\).\nFrom \\[\r\\begin{align}\rKL[q_{\\phi}(z|x)||p_{\\theta}(z|x)] =\u0026amp;\\ \\int q_{\\phi}(z|x) \\log \\frac{q_{\\phi}(z|x)}{p_{\\theta}(z|x)} dz\\\\\r=\u0026amp;\\ \\int q_{\\phi}(z|x) [\\log q_{\\phi}(z|x) - \\log p_{\\theta}(z|x)]dz\r\\end{align}\r\\] and Bayesian formula \\[\rp_{\\theta}(z|x) = \\frac{p_{\\theta}(x|z)*p_{\\theta}(z)}{p_{\\theta}(x)}\r\\] We can get \\[\r\\begin{align}\rKL[q_{\\phi}(z|x)||p_{\\theta}(z|x)] =\u0026amp;\\ \\int q_{\\phi}(z|x) [\\log q_{\\phi}(z|x) - \\log \\frac{p_{\\theta}(x|z)*p_{\\theta}(z)}{p_{\\theta}(x)}]dz\\\\\r=\u0026amp;\\ \\int q_{\\phi}(z|x) [\\log q_{\\phi}(z|x) -\\log p_{\\theta}(x|z) - \\log p_{\\theta}(z) + \\log p_{\\theta}(x)]dz\\\\\r=\u0026amp;\\ \\int q_{\\phi}(z|x) [\\log q_{\\phi}(z|x) -\\log p_{\\theta}(x|z) - \\log p_{\\theta}(z)]dz + \\log p_{\\theta}(x)\\\\\r=\u0026amp;\\ KL[q_{\\phi}(z|x)||p_{\\theta}(z)] - \\int q_{\\phi}(z|x) \\log p_{\\theta}(x|z)dz + \\log p_{\\theta}(x)\r\\end{align}\r\\]\nWhen the data \\(x\\) are provided, then last term in the right side \\(\\log p_{\\theta}(x)\\) becomes constant, and we wish the \\(D_{KL}[q_{\\phi}(z|x)||p_{\\theta}(z|x)]\\) can be as small as possible.\nThus, the optimization problem becomes\n\r\\(\\min\\limits_x D_{KL}[q_{\\phi}(z|x)||p_{\\theta}(z)]\\)\n\r\\(\\max\\limits_x \\int q_{\\phi}(z|x) \\log p_{\\theta}(x|z)dz\\)\n\r\rIt also can be written as \\[\\min_x KL[q_{\\phi}(z|x)||p_{\\theta}(z)] - \\mathbb{E}_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)]\\]\nAlthough we can obtain our new optimization problem, the problem actually is difficult to solve, and thus we would like to straightly optimize the ELBO.\n\r\r\rVariational Auto-Encoder\rConnection with EM\rFor standard EM algorithms, the posterior is often known, \\(q_{\\phi}(z|x) = q(z|x) = p_{\\theta}(z|x)\\), then the KL term becomes zero, so \\[\r\\begin{align}\r\\log p_{\\theta}(x) = \\mathcal{L}_{\\theta}(x) =\u0026amp;\\ \\mathbb{E}_{q(z|x)}\\Big[\\log\\Big[\\frac{p_{\\theta}(x,z)}{q(z|x)}\\Big]\\Big]\\\\\r=\u0026amp;\\ \\mathbb{E}_{q(z|x)}[\\log p_{\\theta}(x,z)] - \\mathbb{E}_{q(z|x)}[\\log q(z|x)]\r\\end{align}\r\\]\nThe above step is indeed the E-step in the standard EM algorithm. The M-step would be \\[\\theta_{\\text{new}} = \\arg \\max_\\theta L_{\\theta}(x)\\]\n\rStochastic gradient-based optimization of the ELBO\rFrom the Evidence lower bound (ELBO) part, we obtain the inequality fomula as \\(\\log p_{\\theta}(x) \\geq \\mathcal{L}_{\\theta,\\phi}(x)\\). Recall that EM algorithm is one of the special case of Minorize-Maximization (MM) algorithm, and \\(\\mathcal{L}_{\\theta,\\phi}(x)\\) can be considered as the surrogate function in MM algorithm, so we would get the maximum of log-likelihood by maximizing the lower bound.\n\rFigure1. The EM algorithm involves alternatel computing a lower bound on the log likelihood for the current parameter values and then maximizing this bound to obtain the new parameter values.\r\rGiven a dataset with i.i.d. data, the ELBO objective is the sum (or average) of individual-datapoint ELBO’s: \\[\r\\mathcal{L}_{\\theta,\\phi}(\\mathcal{D})=\\sum_{x\\in\\mathcal{D}}\\mathcal{L}_{\\theta,\\phi}(x)\r\\]\nApparantly, the individual-datapoint ELBO and its gradient \\(\\nabla_{\\theta,\\phi}\\mathcal{L}_{\\theta,\\phi}(x)\\) is intractable in general.\nThe SGVB estimator and Auto-Encoding VB (AEVB) algorithm\rReparamterization trick\nLet \\(z\\) be a continuous random variable and \\(z\\sim q_{\\phi}(z|x)\\) be some conditional distribution. It is often possible to express the random variable \\(z\\) as a deterministic variable \\(z=g_{\\phi}(\\epsilon,x)\\), where \\(\\epsilon\\) is an auxiliary variable with independent marginal \\(p(\\epsilon)\\).\nWe suppose that the recognition model \\(q_{\\phi}(z|x)\\) can be written as some differentiable transformation of another randome variable \\(\\epsilon\\), \\(g_{\\phi}(\\epsilon,x)\\), and we can form a simple Monte Carlo estimator \\(\\tilde{\\mathcal{L}}_{\\theta,\\phi}(x)\\) of the individual-datapoint ELBO: \\[\r\\epsilon \\sim p(\\epsilon)\r\\]\nso we can get our generic Stochastic Gradient Variational Bayes (SGVB) estimator from the lower bound \\[\r\\tilde{\\mathcal{L}}_{\\theta,\\phi}^{A}(x^{(i)}) = \\frac{1}{L}\\sum_{l=1}^L[\\log p_{\\theta}(x^{(i)},z^{(i,l)}) - \\log q_{\\phi}(z^{(i,l)}|x^{(i)})]\r\\] where \\(z^{(i,l)} = g_{\\phi}(\\epsilon^{(i,l)},x^{(i)}),\\quad \\epsilon^{(i,l)} \\sim p(\\epsilon)\\).\nWe try to decompose the \\(\\mathcal{L}_{\\theta,\\phi}(x)\\), and we get \\[\r\\begin{align}\r\\mathcal{L}_{\\theta,\\phi}(x^{(i)}) =\u0026amp;\\ \\mathbb{E}_{q_{\\phi}(z|x^{(i)})}\\Big[\\log\\frac{p_{\\theta}(x^{(i)},z)}{q_{\\phi}(z|x^{(i)})}\\Big]\\\\\r=\u0026amp;\\ \\mathbb{E}_{q_{\\phi}(z|x^{(i)})}\\Big[\\log\\frac{p_{\\theta}(x^{(i)}|z)p_{\\theta}(z)}{q_{\\phi}(z|x^{(i)})}\\Big]\\\\\r=\u0026amp;\\ \\mathbb{E}_{q_{\\phi}(z|x^{(i)})}\\Big[\\log p_{\\theta}(x^{(i)}|z)-\\log\\frac{q_{\\phi}(z|x^{(i)})}{p_{\\theta}(z)}\\Big]\\\\\r=\u0026amp;\\ \\mathbb{E}_{q_{\\phi}(z|x^{(i)})}[\\log p_{\\theta}(x^{(i)}|z)]-KL[q_{\\phi}(z|x^{(i)})||p_{\\theta}(z)]\r\\end{align}\r\\] The final equality showed the same object result (In Minimization of KL divergence section).\nWith this equality, we also can obtain another estimator \\[\r\\tilde{\\mathcal{L}}_{\\theta,\\phi}^{B}(x^{(i)}) = \\mathbb{E}_{q_{\\phi}(z|x^{(i)})}[\\log p_{\\theta}(x^{(i)}|z)]-KL[q_{\\phi}(z|x^{(i)})||p_{\\theta}(z)]\\\\\r=\\frac{1}{L}\\sum_{l=1}^L\\log p_{\\theta}(x^{(i)}|z^{(i,l)})-KL[q_{\\phi}(z|x^{(i)})||p_{\\theta}(z)]\r\\] where \\(z^{(i,l)} = g_{\\phi}(\\epsilon^{(i,l)},x^{(i)}),\\quad \\epsilon^{(i,l)} \\sim p(\\epsilon)\\). Given multiple datapoints from a dataset \\(\\text{X}\\) with \\(N\\) datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches: \\[\r\\mathcal{L}_{\\theta,\\phi}(\\text{X})\\simeq\\tilde{\\mathcal{L}}_{\\theta,\\phi}^{M}(\\text{X}^M)=\\frac{N}{M}\\sum_{i=1}^M\\tilde{\\mathcal{L}}_{\\theta,\\phi}(x^{(i)})\r\\] where the minibatch \\(\\text{X}^M=\\{x^{(i)}\\}_{i=1}^M\\) is randomly drawn sample of \\(M\\) datapoints from the full dataset \\(\\text{X}\\) with \\(N\\) datapoints. In the paper Auto-Encoding Variational Bayes, author set \\(M = 100, L = 1\\) in their experiments.\n\r\r\rVariational Auto-Encoder with specific case\rWe know that we can not perform the algorithm that we describe above, because we don’t know the distributions of \\(\\epsilon, p_{\\theta}(x|z), q_{\\phi}(z|x), p_{\\theta}(z)\\) and \\(g_{\\phi}(\\epsilon,x)\\). In reality, like author described in the paper, we firstly let the prior over the latent variables be the centered isotropic multivariate Guassian \\(p_{\\theta}(z) = \\mathcal{N}(0,\\text{I})\\).\nVariational approxiamte posterior \\(q_{\\phi}(z|x^{(i)})\\)\rLet the variational approxiamte posterior be a multivariate Guassian with a diagonal covariance structure: \\[\rq_{\\phi}(z|x^{(i)}) = \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})\r\\] where \\(\\mu^{(i)},\\sigma^{(i)}\\) denote the variational mean and s.d. evaluated by datapoint \\(i\\).\nThen a valid reparameterization is \\(z=\\mu+\\sigma\\epsilon\\), where \\(\\epsilon\\) is an auxiliary noise variable \\(\\epsilon\\sim \\mathcal{N}(0,\\text{I})\\).\nLet \\(J\\) be the dimensionality of \\(z\\) and \\(\\mu^{(i)}_j, \\sigma^{(i)}_j\\) denote the \\(j\\)-th element. Recall that \\[\r\\mathbb{E}[z] = \\int z p(z) dz\\\\\r\\mathbb{E}[z^2] = \\int z^2 p(z) dz\\\\\r\\text{Var}[z] = \\mathbb{E}[z^2] - \\mathbb{E}^2[z]\r\\] Then, \\[\r\\begin{align}\r\\int q_{\\phi}(z|x^{(i)})\\log p_{\\theta}(z)dz =\u0026amp;\\ \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})\\log \\mathcal{N}(0,\\text{I})dz\\\\\r=\u0026amp;\\ \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})(\\log \\frac{1}{\\sqrt{2\\pi}})dz - \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I}) \\frac{z^2}{2}dz\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi)\\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})dz - \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I}) \\frac{z^2}{2}dz\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\int z^2 \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})dz\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{j=1}^J\\mathbb{E}_{ q_{\\phi}(z_j|x^{(i)})}[z_j^2]\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{j=1}^J\\Big[\\mathbb{E}_{ q_{\\phi}(z_j|x^{(i)})}^2[z_j]+\\text{Var}_{ q_{\\phi}(z_j|x^{(i)})}[z_j]\\Big]\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{j=1}^J(\\mu_j^2+\\sigma_j^2)\r\\end{align}\r\\] and \\[\r\\begin{align}\r\\int q_{\\phi}(z|x^{(i)})\\log q_{\\phi}(z|x^{(i)})dz =\u0026amp;\\ \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})\\log \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})dz\\\\\r=\u0026amp;\\ \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})\\log \\Big[\\frac{1}{\\sqrt{2\\pi\\sigma^{(i)^2}}}\\exp(\\frac{-(z-\\mu^{(i)})^2}{2\\sigma^{(i)^2}})\\Big]dz\\\\\r=\u0026amp;\\ \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})\\Big[-\\frac{1}{2}\\log 2\\pi - \\frac{1}{2}\\log \\sigma^{(i)^2} - \\frac{(z-\\mu^{(i)})^2}{2\\sigma^{(i)^2}}\\Big]dz\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})\\Big[\\frac{\\log \\sigma^{(i)^2}}{2} - \\frac{(z-\\mu^{(i)})^2}{2\\sigma^{(i)^2}}\\Big]dz\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\int \\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I}) \\Big[\\frac{1}{2}\\log \\sigma^{(i)^2} - \\frac{z^2-2\\mu^{(i)}z+\\mu^{(i)^2}}{2\\sigma^{(i)^2}}\\Big]dz\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{J=1}^J\\log \\sigma_j^{(i)^2} + \\int \\frac{1}{2\\sigma^{(i)^2}}\\mathcal{N}(\\mu^{(i)},\\sigma^{(i)^2}\\text{I})(z^2-2\\mu^{(i)}z+\\mu^{(i)^2})dz\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{j=1}^J\\log \\sigma_j^{(i)^2} + \\frac{1}{2}\\sum_{j=1}^J\\frac{\\mu^{(i)^2}+\\sigma^{(i)^2}-2\\mu^{(i)^2}+\\mu^{(i)^2}}{\\sigma^{(i)^2}}\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{j=1}^J(1+\\log \\sigma_j^{(i)^2})\r\\end{align}\r\\] Therefore, \\[\r\\begin{align}\r-KL[q_{\\phi}(z|x^{(i)})||p_{\\theta}(z)] =\u0026amp; - \\int q_{\\phi}(z|x^{(i)})\\log \\frac{q_{\\phi}(z|x^{(i)})}{p_{\\theta}(z)}dz\\\\\r=\u0026amp;\\ - \\int q_{\\phi}(z|x^{(i)})[\\log q_{\\phi}(z|x^{(i)}) - \\log p_{\\theta}(z)]dz\\\\\r=\u0026amp;\\ \\int q_{\\phi}(z|x^{(i)})[\\log p_{\\theta}(z) - \\log q_{\\phi}(z|x^{(i)})]dz\\\\\r=\u0026amp;\\ -\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{j=1}^J(\\mu_j^2+\\sigma_j^2) - \\Big[-\\frac{J}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{j=1}^J(1+\\log \\sigma_j^{(i)^2})\\Big]\\\\\r=\u0026amp;\\ \\frac{1}{2}\\sum_{j=1}^J(1+\\log \\sigma_j^{(i)^2}-\\mu_j^2-\\sigma_j^2)\r\\end{align}\r\\]\n\rTrue posterior \\(p_{\\theta}(x|z)\\)\rWe supposed that the true posterior \\(p_{\\theta}(x|z)\\) be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution parameters are computed from \\(z\\) with a MLP (Multi-Layer Perceptron).\nBernoulli MLP as decoder\nIf the data are binary data, then we would choose \\[\r\\log p_{\\theta}(x|z) = \\sum_{j=1}^Dx_j \\log y_j + (1-x_j)\\log (1-y_j)\r\\] where \\(y = f_\\sigma(W_2\\tanh(W_1z+b_1)+b_2)\\), \\(f_\\sigma(\\cdot)\\) is the elementwise sigmoid activation function and \\(\\theta=\\{W_1,W_2,b_1,b_2\\}\\) are the weights and biases of the MLP.\nGaussian MLP as decoder\nLet decoder be a mutivariate Guassian with a diagonal covariance structure: \\[\r\\log p_{\\theta}(x|z) = \\log \\mathcal{N}(\\mu,\\sigma^2\\text{I})\r\\] where \\(\\mu = W_4h+b_4,\\ \\log\\sigma^2 = W_5h+b_5,\\ h = \\tanh(W_3Z+b_3)\\) and \\(\\{W_3,W_4,W_5,b_3,b_4,b_5\\}\\) are the weights and biases of the MLP and part of \\(\\theta\\).\nAnalysis in case of binary data\nRecall the second estimator we describe above \\[\r\\begin{align}\r\\mathcal{L}_{\\theta,\\phi}(\\text{X})\\simeq\u0026amp;\\ \\tilde{\\mathcal{L}}_{\\theta,\\phi}^{B}(x^{(i)})\\\\\r=\u0026amp;\\ \\frac{1}{L}\\sum_{l=1}^L\\log p_{\\theta}(x^{(i)}|z^{(i,l)}) - KL[q_{\\phi}(z|x^{(i)})||p_{\\theta}(z)]\\\\\r=\u0026amp;\\ \\frac{1}{L}\\sum_{l=1}^L\\log p_{\\theta}(x^{(i)}|z^{(i,l)}) + \\frac{1}{2}\\sum_{j=1}^J(1+\\log \\sigma_j^{(i)^2}-\\mu_j^2-\\sigma_j^2)\r\\end{align}\r\\] where \\(z^{(i,l)} = \\mu^{(i)} + \\sigma^{(i)}\\epsilon^{(l)}, \\epsilon^{l}\\sim p(\\epsilon)\\) and \\(\\log p_{\\theta}(x|z) = \\sum_{j=1}^Dx_j \\log y_j + (1-x_j)\\log (1-y_j)\\).\n\r\r\rUsing Variational Auto-Encoder in python\rImport packages\rimport numpy as np\rimport matplotlib.pyplot as plt\rimport tensorflow as tf\rfrom tensorflow.examples.tutorials.mnist import input_data\r\rFunction for visualizing batch images\rdef VisConcatImg(batch_images, title):\rbatch_size = np.shape(batch_images)[0]\rsqrt_size = int(batch_size ** 0.5)\rbatch_images = batch_images.reshape(batch_size, 28, 28)\rrow_concatenated = [np.concatenate(batch_images[i*sqrt_size : (i+1)*sqrt_size], axis=1) for i in range(sqrt_size)]\rconcatenated = np.concatenate(row_concatenated, axis=0)\rplt.imshow(concatenated, cmap=\u0026#39;gray\u0026#39;)\rplt.title(title)\rplt.axis(\u0026#39;off\u0026#39;)\rplt.show()\r\rMNIST Dataset\rThe MNIST includes 60000 training samples and 10000 testing samples. Each sample is a 784-dimensional vector (28??28), with pixel values in [0, 1], which can be assumed as multivariate Bernoulli variables.\n# Downloading MNIST dataset\rmnist = input_data.read_data_sets(\u0026#39;./mnist\u0026#39;, one_hot=False)\r# VAE for MNIST\rclass VAE(object):\rdef __init__(self, x_size=28*28, hidden1_size=100, hidden2_size=400, hidden3_size=100, hidden4_size=400, z_size=20, learning_rate=1e-4):\rself.x_size = x_size\rself.hidden1_size = hidden1_size\rself.hidden2_size = hidden2_size\rself.hidden3_size = hidden3_size\rself.hidden4_size = hidden4_size\rself.z_size = z_size\rself.learning_rate = learning_rate\rself.x = tf.placeholder(tf.float32, [None, x_size])\rself.epsilon = tf.placeholder(tf.float32, [None, z_size]) # sample from N(0,1) for every step\rwith tf.variable_scope(\u0026#39;encoder\u0026#39;):\rself.encoder()\rwith tf.variable_scope(\u0026#39;decoder\u0026#39;):\rself.decoder()\rwith tf.variable_scope(\u0026#39;loss\u0026#39;):\rself.compute_loss()\rwith tf.variable_scope(\u0026#39;train\u0026#39;):\rself.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.total_loss)\rdef encoder(self):\rself.hidden1 = tf.layers.dense(self.x, units=self.hidden1_size, activation=tf.nn.relu)\rself.hidden2 = tf.layers.dense(self.hidden1, units=self.hidden2_size, activation=tf.nn.relu)\rself.mu = tf.layers.dense(self.hidden2, units=self.z_size)\rself.sigma = tf.layers.dense(self.hidden2, units=self.z_size, activation=tf.exp)\rself.z = tf.add(self.mu, tf.multiply(self.epsilon, self.sigma))\rdef decoder(self):\rself.hidden3 = tf.layers.dense(self.z, units=self.hidden3_size, activation=tf.nn.relu)\rself.hidden4 = tf.layers.dense(self.hidden3, units=self.hidden4_size, activation=tf.nn.relu)\rself.y = tf.layers.dense(self.hidden4, units=self.x_size, activation=tf.nn.sigmoid)\r# adding 1e-8 before taking the logarithm to avoid numerical instability.\rdef compute_loss(self):\rself.recons_loss = tf.reduce_mean(tf.reduce_sum(-(self.x * tf.log(self.y + 1e-8) + (1 - self.x) * tf.log(1 - self.y + 1e-8)), 1))\rself.KL_loss = tf.reduce_mean(-0.5 * tf.reduce_sum(1 + 2 * tf.log(self.sigma + 1e-8) - tf.square(self.mu) - tf.square(self.sigma), 1))\rself.total_loss = self.recons_loss + self.KL_loss\r# Training VAE\rmodel = VAE()\rBATCH_SIZE = 100\rEPOCHS = 50\rSTEPS = int(60000 / BATCH_SIZE)\rsess = tf.Session()\rsess.run(tf.global_variables_initializer())\rfor e in range(EPOCHS):\rfor i in range(STEPS):\rtrain_data, _ = mnist.train.next_batch(batch_size=BATCH_SIZE)\rep = np.random.multivariate_normal(np.zeros(model.z_size), np.eye(model.z_size), size=BATCH_SIZE)\rsess.run(model.train_op, feed_dict={model.x: train_data, model.epsilon: ep})\rREloss, KLloss, Tloss = sess.run([model.recons_loss, model.KL_loss, model.total_loss], feed_dict={model.x: train_data, model.epsilon: ep})\rprint(\u0026#39;Epoch: \u0026#39;, e, \u0026#39;| reconstruction loss: \u0026#39;, REloss, \u0026#39;| KL loss:\u0026#39;, KLloss, \u0026#39;| total loss: \u0026#39;, Tloss)\r# Visualizing results\rtest_data, _ = mnist.test.next_batch(batch_size=50)\rVisConcatImg(test_data, \u0026#39;raw images\u0026#39;)\rep = np.random.multivariate_normal(np.zeros(model.z_size), np.eye(model.z_size), size=50)\rlatent, recons_x = sess.run([model.mu, model.y], feed_dict={model.x: test_data, model.epsilon: ep})\rVisConcatImg(recons_x, \u0026#39;reconstructed images\u0026#39;)\rrandoms = np.random.multivariate_normal(np.zeros(model.z_size), np.eye(model.z_size), size=50)\rgenerated_x = sess.run(model.y, feed_dict={model.z: randoms})\rVisConcatImg(generated_x, \u0026#39;generated images\u0026#39;)\rsess.close()\r\rResult\r\rFigure3. Raw Images\r\r\rFigure4. Reconstructed Images\r\r\rFigure5. Generated Images\r\r\r\rReference\rD. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2014. 5, 1\rYang Can. VAE_demo in python. 2018,12,31\r\r\r","date":1546214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546214400,"objectID":"364d7cb43b7ce5560d8689197bd08d27","permalink":"/post/variational-auto-encoder/","publishdate":"2018-12-31T00:00:00Z","relpermalink":"/post/variational-auto-encoder/","section":"post","summary":"Introduction\rWe assume the observed variable \\(x\\) is a random sample from an unknown underlying process, whose true distribution \\(p^*(x)\\) is unknown. We attempt to approximate this underlying process with a chosen model \\(p_{\\theta}(x)\\), with parameters \\(\\theta\\): \\[x\\sim p_{\\theta}(x)\\] We always talk about learning like Deep Learning, and actually the learning is the process of searching for a value of the parameters \\(\\theta\\) in model \\(p_{\\theta}(x)\\), which can approximate the true distribution of the data, denoted by \\(p^*(x)\\).","tags":["TensorFlow","VAE"],"title":"Variational Auto-Encoder","type":"post"},{"authors":null,"categories":["Deep Learning"],"content":" 迈出TensorFlow世界的第一步 安装TensorFlow 鉴于python3更高级（传闻python2最终会被淘汰，所以希望选择能陪伴更久的工具:)），本文中会以python3为基准，屏幕前的读者你！也建议你跟我用上python3！另外，如果你刚起步使用python的话，那建议你直接下载 Anaconda，快捷高效，下载引导详见图1。Anaconda会帮助我们省去下载很多库的时间，把时间留给TensorFlow吧！\n 图1. 下载Anaconda  设置水土不服的Anaconda 首先，在安装Anaconda后，先打开Anaconda Prompt（一般在你的应用目录可以找到），打开后是一个跟CMD一样黑乎乎的界面 ╮(๑•́ ₃•̀๑)╭ 打开后呢~通过conda --version看看是否成功安装了Anaconda。一般会得到版本信息，如下所示\nconda 4.5.12 那么，Anaconda安装成功！\n下一步我们设置下Anaconda的仓库(Repository)镜像，因为默认连接的是境外镜像地址，会超慢（我记得我当时只有10kb的网速T_T），我们把镜像地址改为境内的清华大学开源软件镜像站，所以通过下面指令就可以提高你的下载速度(´•灬•‘)。\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --set show_channel_urls yes  创建python3.5环境 首先，由于目前TensoFlow官方Only支持python3.5版本，而且在文章写的这个时候，Anaconda官方最新版本中的python是3.7版本，所以我们需要创建一个python3.5的新环境。\n首先在系统菜单栏找到并点击Anaconda Navigator，然后选择Enviroments（如图2所示），然后点击Create创建新环境：\n 图2. Environments界面  我们命名为tensorflow，并选择python3.5的版本：\n 图3. 创建新环境窗口  安装成功后，Environments界面多了一个我们创建的命名为tensorflow的python3.5的环境，并自动预先安装一些基础的库。\n 图4. 安装成功后的python3.5环境  搞定python3.5的环境后，我们顺便在python3.5环境下安装常用的jupyter notebook和spyder。先选择Home界面，然后可以看到jupyter notebook和spyder下方均显示Install，当然就点击Install，就等着Anaconda Navigator帮我们搞定啦！安装成功后，它们下方会变成Launch（如图5所示）。\n 图5. 安装jupyter和spyder成功后的Home界面  如果要在Anaconda prompt界面启动python3.5环境，很简单，一行命令！\nconda activate tensorflow 这里的tensorflow其实是我们的python3.5环境的命名~\n 通过pip安装我们的主角TensorFlow ٩(๑\u0026gt; ₃ \u0026lt;)۶з 搞定一切基础的部分后，接下来就开始用pip安装我们TensorFlow的CPU版本了~我们先激活python3.5的环境并用pip安装tensorflow\nconda activate python35 pip install tensorflow 旋转跳跃~闭着眼~睁开眼后就搞定了你要的TensorFlow (●´▽｀●) 下面我们先测试一下，在激活python3.5之后，输入python运行python，然后输入下面的命令。\nimport tensorflow as tf hello = tf.constant(\u0026#39;Hello, TensorFlow!\u0026#39;) sess = tf.Session() print(sess.run(hello)) a = tf.constant(1) b = tf.constant(2) c = sess.run(a+b) print(\u0026quot;1 + 2 = %d\u0026quot; % c) 如果输出了\n‘Hello, TensorFlow!’\n1 + 2 = 3\n那么恭喜你，TensorFlow安装成功！！\nAttention!\n CPU有个利器(๑•̀_•́๑)\n当我使用sess = tf.Session()的时候，对话框告诉我：\n2018-12-26 15:03:57.708274: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n然后一脸懵，杰是个啥？\n感恩必应搜索，感恩维基百科！\n 高级矢量扩展（AVX）是英特尔在2008年3月提出的英特尔和AMD微处理器的x86指令集体系结构的扩展，英特尔首先通过Sandy Bridge处理器在2011年第一季度推出，随后由AMD推出Bulldozer处理器在2011年第三季度.AVX提供了新功能，新指令和新编码方案。 特别是，AVX引入了融合乘法累加（FMA）操作，加速了线性代数计算，即点积，矩阵乘法，卷积等。几乎所有机器学习训练都涉及大量这些操作，因此将会支持AVX和FMA的CPU（最高达300％）更快。\n 而对话框想告诉我，我的CPU支持AVX，让我赶紧用上它！\nimport tensorflow as tf import os os.environ[\u0026#39;TF_CPP_MIN_LOG_LEVEL\u0026#39;] = \u0026#39;2\u0026#39; sess = tf.Session() 这样就不会出现警告了~\n 目前觉得不重要的part(๑•̀_•́๑)\n查看Windows系统下机器的GPU信息\n首先打开“运行”对话框并在“运行”对话框中输入“dxdiag”（如图6），此时会打开“DirextX诊断工具”窗口，再通过选择“显示”标签便可查到机器的GPU信息（如图7）。\n   图6. 输入dxdiag命令   图7. 查看机器GPU信息  简单提及安装GPU版本TensorFlow\n如果你觉得运行的速度不满足你的需求，那么你可以选择用上GPU版本的TensorFlow，那将帮助你火箭般的速度运行！下面简单带过如何安装TensorFlow的GPU版本~由于具体操作复杂，暂且跳过~\npip install tensorflow-gpu    ","date":1545782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545782400,"objectID":"0194b36e492a9de3af5ffa26d2f117a7","permalink":"/post/%E5%B0%8F%E8%8F%9C%E9%B8%9F%E7%9A%84%E5%85%A5%E9%97%A8tensorflow/","publishdate":"2018-12-26T00:00:00Z","relpermalink":"/post/%E5%B0%8F%E8%8F%9C%E9%B8%9F%E7%9A%84%E5%85%A5%E9%97%A8tensorflow/","section":"post","summary":"迈出TensorFlow世界的第一步 安装TensorFlow 鉴于python3更高级（传闻python2最终会被淘汰，所以希望选择能陪伴更久的工具:)），本文中会以python3为基准，屏幕前的读者你！也建议你跟我用上python3！另外，如果你刚起步使用python的话，那建议你直接下载 Anaconda，快捷高效，下载引导详见图1。Anaconda会帮助我们省去下载很多库的时间，把时间留给TensorFlow吧！\n 图1. 下载Anaconda  设置水土不服的Anaconda 首先，在安装Anaconda后，先打开Anaconda Prompt（一般在你的应用目录可以找到），打开后是一个跟CMD一样黑乎乎的界面 ╮(๑•́ ₃•̀๑)╭ 打开后呢~通过conda --version看看是否成功安装了Anaconda。一般会得到版本信息，如下所示\nconda 4.5.12 那么，Anaconda安装成功！\n下一步我们设置下Anaconda的仓库(Repository)镜像，因为默认连接的是境外镜像地址，会超慢（我记得我当时只有10kb的网速T_T），我们把镜像地址改为境内的清华大学开源软件镜像站，所以通过下面指令就可以提高你的下载速度(´•灬•‘)。\nconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --set show_channel_urls yes  创建python3.5环境 首先，由于目前TensoFlow官方Only支持python3.5版本，而且在文章写的这个时候，Anaconda官方最新版本中的python是3.7版本，所以我们需要创建一个python3.5的新环境。\n首先在系统菜单栏找到并点击Anaconda Navigator，然后选择Enviroments（如图2所示），然后点击Create创建新环境：\n 图2. Environments界面  我们命名为tensorflow，并选择python3.5的版本：\n 图3. 创建新环境窗口  安装成功后，Environments界面多了一个我们创建的命名为tensorflow的python3.5的环境，并自动预先安装一些基础的库。\n 图4. 安装成功后的python3.5环境  搞定python3.5的环境后，我们顺便在python3.5环境下安装常用的jupyter notebook和spyder。先选择Home界面，然后可以看到jupyter notebook和spyder下方均显示Install，当然就点击Install，就等着Anaconda Navigator帮我们搞定啦！安装成功后，它们下方会变成Launch（如图5所示）。\n 图5. 安装jupyter和spyder成功后的Home界面  如果要在Anaconda prompt界面启动python3.5环境，很简单，一行命令！\nconda activate tensorflow 这里的tensorflow其实是我们的python3.5环境的命名~\n 通过pip安装我们的主角TensorFlow ٩(๑\u0026gt; ₃ \u0026lt;)۶з 搞定一切基础的部分后，接下来就开始用pip安装我们TensorFlow的CPU版本了~我们先激活python3.5的环境并用pip安装tensorflow\nconda activate python35 pip install tensorflow 旋转跳跃~闭着眼~睁开眼后就搞定了你要的TensorFlow (●´▽｀●) 下面我们先测试一下，在激活python3.","tags":["TensorFlow"],"title":"小菜鸟的入门TensorFlow","type":"post"},{"authors":null,"categories":["Deep Learning"],"content":"\rFrom some on-line article, it is interesting that we can consider EM algorithm as some Chinese Kungfu manuals and it contains 9 levels of perspectives. With such metaphor, I can feel how this method powerful it is. Now, I want to share my view with you.\nIntroduction to the EM algorithm\rExpectation-Maximization (EM) algorithm is an important method to solve the maximum likelihood problem with latent variables in statistics. It is widely used in Machine Learning because it can simplify many difficult problems. One of the famous and classical application is gaussian mixture model.\nBasic probability theory\rLet \\(p(x|\\theta)\\) be the probability density function of random variable \\(x\\), where \\(\\theta\\) is the parameter of the density function, then we know that the basic probability property is \\[\rp(\\text{x};\\theta) \\geq 0, \\int_{-\\infty}^{+\\infty}p(\\text{x};\\theta) d\\text{x} = 1.\r\\]\nIf we take the expectation of x, we get \\[\r\\mathbb{E}[\\text{x}] = \\int\\text{x}p(\\text{x};\\theta) d\\text{x}\r\\] In the integral, we know that the \\(\\mathbb{E}[\\text{x}]\\) involves \\(\\theta\\) but not \\(\\text{x}\\).\nIf we generalize it to function and let \\(f(\\text{x})\\) be a function of \\(\\text{x}\\). Similarly, the expectation of \\(f(x)\\) is given by \\[\r\\mathbb{E}[f] = \\int f(\\text{x})p(\\text{x};\\theta) d\\text{x}\r\\]\nWith the similar result, we know that the \\(\\mathbb{E}[f]\\) involves \\(\\theta\\) but not \\(\\text{x}\\).\n\rMotivation of the EM algorithm\rAt the beginning, we denote that\n\r\\(\\text{X}\\): Set of all observed data (incomplete-data)\r\\(\\text{Z}\\): Set of all latent variables\r\\(\\theta\\): Set of all model parameters\r\\(\\{ \\text{X,Z} \\}\\): Each observation in \\(\\text{X}\\) is corresponding value of the latent variable \\(\\text{Z}\\) (complete-data)\r\rThen the log-likelihood function can be written as \\[\rL(\\text{X};\\theta) = \\ln p(\\text{X};\\theta) = \\ln \\{ \\sum_{\\text{Z}}p(\\text{X}, \\text{Z};\\theta) \\}\r\\]\nIt is too hard to straightly solve the problem with \\(\\ln\\) and \\(\\sum\\). The likelihood function for the complete data set simply takes the form \\(\\ln p(\\text{X,Z}|\\theta)\\), and we shall suppose that maximization of this complete-data log-likelihood function is straightforward.\nIn practice, we are not given the latent variable \\(\\text{Z}\\) but we know the posterior distribution \\(p(\\text{Z}|\\text{X};\\theta)\\).\n\rThe Level 1 of the EM algorithm\rIn the level 1, we just need to know the basic knowledge of EM algorithm.\nIn the \\(\\mathbb{E}\\)-step, we use the current parameter values \\(\\theta_{\\text{old}}\\) to find the posterior distribution of the latent variables given by \\(p(\\text{Z}|\\text{X};\\theta_{\\text{old}})\\).\nThen, we would use this posterior distribution to find the expectation of the complete-data log-likelihood evaluated for some general parameter value \\(\\theta\\). This expectation, denoted \\(\\mathcal{Q}(\\theta,\\theta_{\\text{old}})\\), is given by \\[\r\\mathcal{Q}(\\theta,\\theta_{\\text{old}}) = \\mathbb{E}_{\\text{Z}|\\text{X};\\theta_{\\text{old}}}[\\ln p(\\text{X,Z};\\theta)] = \\sum_{\\text{Z}} \\ln p(\\text{X,Z};\\theta)p(\\text{Z}|\\text{X};\\theta_{\\text{old}})\r\\]\nIn the \\(\\mathbb{M}\\) (Maximization) step, we determine the revised parameter estimate \\(\\theta_{\\text{new}}\\) by maximizing the function \\[\r\\theta_{\\text{new}} = \\arg \\max_\\limits{\\theta} \\mathcal{Q}(\\theta,\\theta_{\\text{old}})\r\\]\nSince the complete-data log-likelihood involves unobversed data \\(\\text{Z}\\), we use Expectation to eliminate the uncertainty, and the function \\(\\mathbb{E}_{\\text{Z}|\\text{X},\\theta_{\\text{old}}}[\\ln p(\\text{X,Z}|\\theta)]\\) does not involve \\(\\text{Z}\\) but involve \\((\\theta,\\theta_{\\text{old}})\\).\nWe can summarize the procedure as:\n\rWhile \\(\\theta_{\\text{new}} - \\theta_{old} \u0026gt; \\epsilon\\)\n\\(\\quad\\) Expectation-Step on log likelihood function: \\[\r\\mathcal{Q}(\\theta,\\theta_{\\text{old}}) = \\mathbb{E}_{\\text{Z}|\\text{X};\\theta_{\\text{old}}}[\\ln p(\\text{X,Z};\\theta)]= \\sum_{\\text{Z}} \\ln p(\\text{X,Z};\\theta)p(\\text{Z}|\\text{X};\\theta_{\\text{old}})\r\\] \\(\\quad\\) Maximization-Step on \\(\\mathcal{Q}(\\theta,\\theta_{\\text{old}})\\): \\[\r\\theta_{\\text{new}} = \\arg \\max_\\limits{\\theta} \\mathcal{Q}(\\theta,\\theta_{\\text{old}})\r\\]\n\r\rThe Level 2 of the EM algorithm\rAfter writting down the pseudocode of EM algorithm, we want to know the reason that we can use expectation to approximate the maximum log-likelihood by repeating Expectation and Maximization. In other words, we want to prove that \\[\r\\arg \\max_\\theta \\mathbb{E}_{\\text{Z}|\\text{X};\\theta_{\\text{old}}}[\\ln p(\\text{X};\\theta)] \\approx \\arg \\max_\\theta \\ln p(\\text{X};\\theta)\r\\] where the joint distribution \\(p(\\text{X}, \\text{Z};\\theta)\\) is governed by a set of parameters \\(\\theta\\).\nNext we introduce a distribution \\(q(\\text{Z})\\) defined over the latent variables.\nSince \\[\rp(\\text{X},\\text{Z};\\theta) = p(\\text{Z}|\\text{X};\\theta)p(\\text{X};\\theta)\r\\] and \\[\r\\sum_\\text{Z}q(\\text{Z}) = 1\r\\] we can get decomposition by \\[\r\\begin{align}\r\\ln p(\\text{X};\\theta) =\u0026amp;\\ \\ln \\frac{p(\\text{X},\\text{Z};\\theta)}{p(\\text{Z}|\\text{X};\\theta)}\\\\\r=\u0026amp;\\ \\ln p(\\text{X},\\text{Z};\\theta) - \\ln p(\\text{Z}|\\text{X};\\theta) + \\ln q(\\text{Z}) - \\ln q(\\text{Z})\\\\\r=\u0026amp;\\ \\ln \\frac{p(\\text{X},\\text{Z};\\theta)}{q(\\text{Z})} - \\ln \\frac{p(\\text{Z}|\\text{X};\\theta)}{q(\\text{Z})}\\\\\r=\u0026amp; \\sum_\\text{Z}q(\\text{Z}) \\{ \\ln \\frac{p(\\text{X},\\text{Z};\\theta)}{q(\\text{Z})} - \\ln \\frac{p(\\text{Z}|\\text{X};\\theta)}{q(\\text{Z})} \\}\\\\\r=\u0026amp;\\ \\sum_\\text{Z}q(\\text{Z}) \\ln \\frac{p(\\text{X},\\text{Z};\\theta)}{q(\\text{Z})} - \\sum_\\text{Z}q(\\text{Z}) \\ln \\frac{p(\\text{Z}|\\text{X};\\theta)}{q(\\text{Z})}\\\\\r=\u0026amp;\\ \\mathcal{L}(q,\\theta) + KL(q||p)\r\\end{align}\r\\] where \\[\r\\begin{align}\r\\mathcal{L}(q,\\theta) = \\sum_\\text{Z}q(\\text{Z}) \\ln \\frac{p(\\text{X},\\text{Z};\\theta)}{q(\\text{Z})}\\\\\rKL(q||p) = - \\sum_\\text{Z}q(\\text{Z}) \\ln \\frac{p(\\text{Z}|\\text{X};\\theta)}{q(\\text{Z})}\r\\end{align}\r\\]\nWe call the \\(KL(q||p)\\) as the Kullback-Leibler divergence (KL divergence, also known as relative entropy).\nRecall that Jensen’s inequality holds for convex function \\(f(x)\\). \\[\r\\mathbb{E}[f(x)] \\geq f(\\mathbb{E}[x])\r\\]\nApplying Jensen’s inequality in KL divergence, we have \\[\r\\begin{align}\rKL(q||p) =\u0026amp; - \\sum_\\text{Z}q(\\text{Z}) \\ln \\frac{p(\\text{Z}|\\text{X};\\theta)}{q(\\text{Z})} = -\\mathbb{E}_q[\\ln\\{\\frac{p(\\text{Z}|\\text{X};\\theta)}{q(\\text{Z})}\\} ]\\\\\r\\geq\u0026amp; -\\ln \\mathbb{\\text{E}_q}[\\frac{p(\\text{Z}|\\text{X};\\theta)}{q(\\text{Z})}] = -\\ln \\sum_\\text{Z} q(\\text{Z}) \\frac{p(\\text{Z}|\\text{X};\\theta)}{q(\\text{Z})}\\\\\r=\u0026amp; -\\ln \\sum_\\text{Z} p(\\text{Z}|\\text{X};\\theta) = -\\ln 1\\\\ =\u0026amp;\\ 0\r\\end{align}\r\\]\nIf we let \\(q(\\text{Z}) = p(\\text{Z}|\\text{X};\\theta)\\), and \\(p(\\text{X},\\text{Z};\\theta) = p(\\text{Z}|\\text{X};\\theta)p(\\text{X};\\theta)\\), then \\[\r\\begin{align}\r\\mathcal{L}(q,\\theta) =\u0026amp; \\sum_\\text{Z}q(\\text{Z}) \\ln \\frac{p(\\text{X},\\text{Z};\\theta)}{q(\\text{Z})} = \\mathbb{E}_q[\\ln \\frac{p(\\text{X},\\text{Z};\\theta)}{q(\\text{Z})}]\\\\\r=\u0026amp;\\ \\mathbb{E}_{\\text{Z}|\\text{X};\\theta}[\\ln p(X;\\theta)]\r\\end{align}\r\\]\nThus, we can get the result that \\(\\ln p(\\text{X};\\theta) \\geq \\mathcal{L}(q,\\theta) = \\mathbb{E}_{\\text{Z}|\\text{X};\\theta}[\\ln p(X;\\theta)]\\), so we can say that \\(\\mathcal{L}(q,\\theta) = \\mathbb{E}_{\\text{Z}|\\text{X};\\theta}[\\ln p(X;\\theta)]\\) is the low bound of \\(\\ln p(\\text{X};\\theta)\\).\n\rFigure1. The EM algorithm involves alternatel computing a lower bound on the log likelihood for the current parameter values and then maximizing this bound to obtain the new parameter values.\r\rActually, EM algorithm is one of the special case of Minorize-Maximization (MM) algorithm, and \\(\\mathcal{L}(q,\\theta)\\) can be considered as the surrogate function in MM algorithm.\n\r\r","date":1545609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545609600,"objectID":"8ffb9edea8bf95f2a21b2f21e43bb7d1","permalink":"/post/expectation-maximization-algorithm/","publishdate":"2018-12-24T00:00:00Z","relpermalink":"/post/expectation-maximization-algorithm/","section":"post","summary":"From some on-line article, it is interesting that we can consider EM algorithm as some Chinese Kungfu manuals and it contains 9 levels of perspectives. With such metaphor, I can feel how this method powerful it is. Now, I want to share my view with you.\nIntroduction to the EM algorithm\rExpectation-Maximization (EM) algorithm is an important method to solve the maximum likelihood problem with latent variables in statistics. It is widely used in Machine Learning because it can simplify many difficult problems.","tags":["Algorithm"],"title":"Expectation-Maximization Algorithm","type":"post"},{"authors":null,"categories":["Machine Learning"],"content":"\rCoordinate Descent Framework\rAt the begining of this section, we start to discuss three different types of function.\nGiven convex, differentiable function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\), we know \\(f(x+\\delta \\cdot e_i)\\geq f(x)\\) for all \\(\\delta\\) because \\(\\nabla f(x) = (\\frac{\\partial f}{\\partial x_1}(x),\\dots,\\frac{\\partial f}{\\partial x_n}(x)) = 0\\). Here, \\(e_i = (0,\\dots,1,\\dots,0) \\in \\mathbb{R}^n\\), the \\(i\\)th standard basis vactor.\r\r\rFigure1. Convex and differential function \\(f\\)\r\rGiven convex but not differentiable function \\(f\\), we can not found a global minimizer.\r\r\rFigure2. Convex but not differential function \\(f\\)\r\rGiven convex \\(g\\) and each convex but not differentiable \\(h_i\\), so we get \\(f(x)=g(x)+\\sum_{i=1}^n h_i(x_i)\\). In this function, the non-smooth part is called as separable.\nFor any \\(y\\), we get\r\\[\\begin{align}\rf(y) - f(x) \\geq\u0026amp; \\nabla g(x)^T (y-x) + \\sum_{i=1}^n [h_i(y_i)-h_i(x_i)]\\\\\r=\u0026amp; \\sum\\limits_{i=1}^n [\\nabla_ig(x)(y_i-x_i)+h_i(y_i)-h_i(x_i)] \\geq 0\r\\end{align}\\]\rThus, we can get global minimizer.\r\r\rFigure3. Convex, not differential but separable function \\(f\\)\r\rIf we get a function with the formula like \\(f(x) = g(x) + \\sum_{i=1}^n h_i(x_i)\\), where the \\(g\\) is convex and differentiable function, each \\(h_i\\) is convex functions, then we can use coordinate descent to find global minimizer. The procedure is following: start with some initial guess \\(x^{(0)}\\), and repeat\r\\[\\begin{align}\rx_1^{(k)} \\in\u0026amp; \\mathop{\\arg\\min}_{x_1} f(x_1,x_2^{(k-1)},x_3^{(k-1)},\\dots,x_n^{(k-1)})\\\\\rx_2^{(k)} \\in\u0026amp; \\mathop{\\arg\\min}_{x_2} f(x_1^{(k)},x_2,x_3^{(k-1)},\\dots,x_n^{(k-1)})\\\\\rx_3^{(k)} \\in\u0026amp; \\mathop{\\arg\\min}_{x_3} f(x_1^{(k)},x_2^{(k)},x_3,\\dots,x_n^{(k-1)})\\\\\r\\cdots\u0026amp; \\\\\rx_n^{(k)} \\in\u0026amp; \\mathop{\\arg\\min}_{x_n} f(x_1^{(k)},x_2^{(k)},x_3^{(k)},\\dots,x_n)\\\\\r\\end{align}\\]\rfor \\(k=1,2,3,\\dots,K\\)\nNote: after we solve for \\(x_i^{(k)}\\), we use its new value from then on.\n\rCoordinate descent for linear regression with convex penalties\r\r","date":1545264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545264000,"objectID":"a7b929a37f776a7ca16cc0156eb8ea6e","permalink":"/post/coordinate-descent-algorithm/","publishdate":"2018-12-20T00:00:00Z","relpermalink":"/post/coordinate-descent-algorithm/","section":"post","summary":"Coordinate Descent Framework\rAt the begining of this section, we start to discuss three different types of function.\nGiven convex, differentiable function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\), we know \\(f(x+\\delta \\cdot e_i)\\geq f(x)\\) for all \\(\\delta\\) because \\(\\nabla f(x) = (\\frac{\\partial f}{\\partial x_1}(x),\\dots,\\frac{\\partial f}{\\partial x_n}(x)) = 0\\). Here, \\(e_i = (0,\\dots,1,\\dots,0) \\in \\mathbb{R}^n\\), the \\(i\\)th standard basis vactor.\r\r\rFigure1. Convex and differential function \\(f\\)\r\rGiven convex but not differentiable function \\(f\\), we can not found a global minimizer.","tags":["R","Algorithm"],"title":"Coordinate Descent Algorithm","type":"post"}]