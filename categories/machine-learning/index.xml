<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on BONBON BLOG</title>
    <link>/./categories/machine-learning.html</link>
    <description>Recent content in Machine Learning on BONBON BLOG</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Dec 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/./categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Expectation-Maximization Algorithm</title>
      <link>/./post/expectation-maximization.html</link>
      <pubDate>Mon, 24 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/./post/expectation-maximization.html</guid>
      <description>From some on-line article, it is interesting that we can consider EM algorithm as some Chinese Kungfu manuals and it contains 9 levels of perspectives. With such metaphor, I can feel how this method powerful it is. Now, I want to share my view with you.
Introduction to the EM algorithmExpectation-Maximization (EM) algorithm is an important method to solve the maximum likelihood problem with latent variables in statistics. It is widely used in Machine Learning because it can simplify many difficult problems.</description>
    </item>
    
    <item>
      <title>Coordinate Descent</title>
      <link>/./post/coordinate-descent.html</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/./post/coordinate-descent.html</guid>
      <description>Coordinate Descent FrameworkAt the begining of this section, we start to discuss three different types of function.
Given convex, differentiable function \(f: \mathbb{R}^n \to \mathbb{R}\), we know \(f(x+\delta \cdot e_i)\geq f(x)\) for all \(\delta\) because \(\nabla f(x) = (\frac{\partial f}{\partial x_1}(x),\dots,\frac{\partial f}{\partial x_n}(x)) = 0\). Here, \(e_i = (0,\dots,1,\dots,0) \in \mathbb{R}^n\), the \(i\)th standard basis vactor.Figure1. Convex and differential function \(f\)Given convex but not differentiable function \(f\), we can not found a global minimizer.</description>
    </item>
    
  </channel>
</rss>